깃허브
https://github.com/studylabdoc/text/blob/main/20240722.txt

구글드라이브
https://drive.google.com/drive/folders/1MIP3LQNU3oISKebgHWvUCgEVF4_2Xg95?usp=sharing

miro
https://miro.com/app/board/uXjVKwkVIgM=/?share_link_id=305482122361

google docs
https://docs.google.com/document/d/1vbzX5a2MGcT2SaKxQ3SR5XfKiK7ITvymUA98Fyz7eUI/edit?usp=sharing

nsr230@studylab.xyz


[랩 접속 방법]
메일 : 20240722
rol.redhat.com  < RHLS 

MY VIRTUAL TRAINING CLASSES
RH134 - Red Hat System Administration II
Class Starts: 월, 7월 22 2024, 10:00 오전 

Class Ends: 금, 7월 26 2024, 06:00 오후 
JOIN

리소스 -> pdf download
랩 > workstation
Web Applications>  Classroom Web Terminal > access

PDF는 MS 엣지로 오픈
10:00 - 18:00 
쉬는시간 : 10 / 15분 
점심시간 : 12:30 - 14:00

[10] < 10page 

[7]
목차

RH134 과정을 위한 사전 명령어
# ls
디렉토리의 컨켄츠 확인
# chmod
권한 변경
# pwd
현재 디렉토리 확인
# passwd
비밀번호 변경
# useradd
계정 생성
# rm
삭제
# cp
복사
# mv
파일 이동 / 이름 변경
# date
시간 확인 / 시간변경 > timedatectl
# touch
파일의 시간 변경 , 빈 파일 생성
# mkdir
디렉토리 생성
# rmdir
빈 디렉토리 삭제 -> rm -rf dir
# id
현재 계정의 정보 확인
UID / GID / groups / selinux data
# cat
파일 확인 / 파일 병합 > 표준 출력 
# tac
파일 출력 방향을 반대로 돌림
# ps
프로세스 확인 / ps / ps -ef / ps aux
# bash
사용자 쉘 명령 / 서브 쉘을 실행 최근에는 bash도 사용하지만 fish / zsh 도 많이 사용중
# su -
접속 된 계정이 아닌 새로운 계정으로 전환 / 최근에는 sudo 명령 권장
# echo
빈 한 줄을 리턴 / 환경변수 호출
# more
cat의 경우에는 라인이 길어도 모두 한번에 출력
페이지 단위로 한 페이지 씩 끊어서 화면에 출력
more와 man에서 사용되는 기능키
* spacebar : 매뉴얼 페이지에서 한 화면 단위로 넘어 갈 때 사용
Enter : 매뉴얼 페이지에서 한 라인씩 넘어 갈 때 사용
b : Back Screen, 한 화면 전 화면으로 넘어갈 때 사용
* /pattern : 특정한 패턴을 빨리 찾을 때 사용
* n : Next, 특정한 패턴을 찾은 후 다음 번째 똑같은 문자열을 찾을 때 사용
h : help 매뉴얼 페이지 안에서 사용 할 수 있는 명령어 소개
* q : quit, 빠져나옴
# man
도움말 
https://linux.die.net/man/
# clear
화면을 청소 / 청소 이후 위의 출력 결과를 알수가 없음
# ln
출력 결과의 앞에 번호를 붙임
# tree
파일시스템의 파일, 디렉토리의 구조화
# dnf
yum, dnf 패키지 관리 프로그램
# chown
소유자 변경 
# umask
기본 권한 변경, 확인
# which
명령어 절대경로 확인
# whereis
매뉴얼 확인
# alias
기본 명령이 아닌 쉽게 사용할 수 있는 약어 생성, 확인
# histroy
사용한 명령어를 확인
# stat
전체 시간 정보 + 권한등 여러가지를 확인
# file
파일의 형식 확인
# cat file1 file2 > file3
표준 출력의 리다이렉션
# find . -name core 2> /dev/null
표준 에러의 리다이렉션
# cat < file1
표준 입력의 리다이렉션
# ssh
원격 접속을 위한 보안 쉘
# scp
원격 파일 복사를 위한 보안 쉘 [사용을 현재 중지]
# sftp
보안 파일 전송 프로토콜
# script
화면에 출력되는 모든 텍스트를 캡처, 저장
# vi
텍스트 에디터
# systemctl
systemd를 위한 명령어
# exit
쉘을 빠져나가기
# export
환경 변수 지정
# ^c
현재 작업중인 내용을 인터럽트하고 강제 종료
# ^d
EOF / EXIT
EOF: 파일의 끝
EXIT: 쉘을 끝내는 단축키
# poweroff
시스템 종료
# reboot
재부팅
# shutdown
시스템 종료 / 유예시간을 부여 할 수 있다. 
# ping
ICMP 프로토콜을 이용하여 헬스 체크 / 조건문에서 사용하기 위해 작성
# ifconfig
NIC 정보 확인 > 최근에는 잘 사용되지 않음.
net-tools 패키지가 있어야 하지만 설치자체를 하지 않음.
ip a 명령으로 대체
# head
파일의 상단 10줄
# tail
파일의 하단 10줄
# who
현재 접속자 확인
# last
사용자들의 접속 기록
# lastb
사용자의 잘못된 접속 기록
# w
현재 접속자의 사용 명령 확인
# while true do ; action ; done
반복문
# df
디스크의 남은 용량 / 파일 시스템 확인
# du
디스크의 사용 용량
# sort
데이터 정렬
# uniq 
데이터의 값 중 유일한 것 하나만 추출
# tee
출력을 두 방향으로 복제
# screen
지금은 tmux 명령으로 변경
쉘을 이용하면 쉘이 꺼지게 되면 작업 중지
내부에서 백 그라운드 작업으로 돌기 때문에 쉘이 꺼져도 계속 작업이 가능.
# mail
s-nail 명령으로 변환
# wall 
현재 실행중인 모든 터미널에 동일한 내용으로 브로드캐스트 
# grep
컨텐츠 내에서 특정 패턴을 찾기 위해 사용
+ 특정 패턴을 걸고 디렉토리나 파일에서 해당 내용이 존재하는 파일을 찾는 용도
# find
inode 정보를 필터링 하여 원하는 파일 / 디렉토리를 찾는 용도
+ 검색된 파일과 디렉토리에 실행 명령을 내려 2차 작업이 가능 

124 / 134 / 199 -> RHCSA

시험을 대비하는 분들을 위해:
1장
연습가이드: 간단한 Bash 스크립트 작성
2장
연습가이드: 반복 실행 사용자 작업 예약
3장
연습가이드: 정확한 시간 유지 관리
4장
연습 가이드: 압축된 tar 아카이브 관리 
tar + gz , bzip2, xz 
5장
연습가이드: 튜닝 프로파일 조정
6장
연습 가이드: SELinux 문제 조사 및 해결
+ 11장 연습 가이드: SELinux 포트 레이블 지정 제어
+ 연습 가이드: 서버 방화벽 관리

7장 8장 all
block storage를 마운트 하고 fstab까지 구성이 가능한 상태 >
LVM 구성 
+ LVM 생성
+ LVM 확장
+ swap

9장
연습 가이드: 네트워크 연결 스토리지 자동 마운트

10장
연습 가이드: 루트 암호 재설정

13장
container build 
container image control
연습 가이드: 컨테이너를 시스템 서비스로 관리

[14]
강의실 환경에 대한 오리엔테이션
workstation : 작업을 위한 컴퓨터
servera,serverb : 실제 작업을 위한 서버 
나머지 서버들은 lab 환경을 위해 존재한다.

시스템의 계정들은 다음과 같다.
student / student
root / redhat

1장
[20]
스크립트 = 자동화
쉘 스크립트, perl, -> [ansible:RHCE:RH294]

스크립트 top-down 
리눅스에 존재하는 왠만한 파일들은 위에서 아래로 읽어 내려감
1 shell을 사용한다는 것을 명시
2 명령어 작성

# vi run.sh
리눅스에서는 파일명 끝의 확장자는 아무 의미 없음.

명령 인터프리터 지정
# < 주석 처리로 진행이 되지만 첫줄에서 #! -> 이 뒤에 오는 파일 명을 가지고 쉘을 실행
sample
#!/usr/bin/bash == #!/bin/bash

실제 사용할 명령 리스트 순서대로 진행

date
echo 'run.sh file'
sleep 2
echo 'run.sh exit'


$ vi run.sh
---------------------------
#!/usr/bin/bash
date
echo 'run.sh file'
sleep 2
echo 'run.sh exit'
---------------------------

i : 입력모드
입력
esc -> 명령행모드
:wq  -> 저장+ 빠져나가기

현재 쉘에서 실행하기
./run.sh
./ = 현재 디렉토리 
run.sh = 실제 사용할 파일
./= /home/student/
./run.sh = /home/student/run.sh
실행을 위해서는 x권한이 필요
[student@workstation ~]$ ls -l run.sh
-rw-r--r--. 1 student student 67 Jul 21 23:12 run.sh
[student@workstation ~]$ chmod u+x run.sh
[student@workstation ~]$ ls -l run.sh
-rwxr--r--. 1 student student 67 Jul 21 23:12 run.sh

명령어를 호출하는 순서
1 hash에서 검색 
  # hash , # hash -r 
2 alias 에서 확인
3 실제 경로를 확인
4 명령어 실행

# command ls
alias 무시후 실제 경로에서만 실행

[student@workstation ~]$ echo $PATH
/home/student/.local/bin:/home/student/bin:/sbin:/bin:/usr/sbin:/usr/bin:/usr/local/sbin:/usr/local/bin:/home/student/.venv/labs/bin
PATH 환경변수는 명령어를 입력 했을때 3번[실제 경로를 확인] 작업을 위한 디렉토리를 모아둔 곳

실제 경로를 확인
[student@workstation ~]$ ls
Desktop    Downloads  Music     Public  Templates
Documents  file1      Pictures  run.sh  Videos
[student@workstation ~]$ mkdir bin
[student@workstation ~]$ mv run.sh bin
[student@workstation ~]$ tree bin
bin
└── run.sh

0 directories, 1 file
run.sh 파일을 bin 디렉토리 아래에 복사.
해당 디렉토리는 /home/student/bin에 존재

0 directories, 1 file
run.sh 파일을 bin 디렉토리 아래에 복사.
해당 디렉토리는 /home/student/bin에 존재

인용 특수 문자 -> 메타캐릭터
shell에서 특수문자를 파일명에 추가 하지 마라.
!@#$%^&*()_ | ‘ “ ; : , . / \ ~ `  -> 쉘에서 하는 기능들이 있다.
명령어 입력 -> 
# ls -l $(which ls)
> 토큰 단위로 분할 [빈칸 ]
1  ls
2  -l
3  $(which ls)
쉘에서는 특수문자를 기능으로 사용을 한다.
파일을 생성 할때:
255 자.
빈칸 쓰지말고
특수문자 쓰지말고
영문자, 대문자, 숫자, -, _ , . 만 가지고 생성
자주 사용되는 메타캐릭터:
# : 주석 처리
\:
1 바로 뒤에 오는 한가지 문자를 해석하지 않음
[student@workstation ~]$ echo \$HOME
$HOME
2 명령어 라인의 끝에 \를 붙이게 되면 PS2 변수를 반환 하면서 라인을 끝내지 않고 아래줄에서 이어서 작성
‘ : ‘text’따옴표 안의 문자열을 해석하지 않음
“: “text”따옴표 안의 문자열을 해석하지 않음 [하지만 해석하는 문자도 있다. \ ` $]
` : `CMD` 역따옴표 안의 명령어를 해석하여 명령어의 결과로 보여준다.
[student@workstation ~]$ echo `date`
Mon Jul 22 01:16:14 AM EDT 2024

[student@workstation ~]$ echo "=====`date`======"
=====Mon Jul 22 01:16:43 AM EDT 2024======
최근에는 `가 아니라 $(CMD)
[student@workstation ~]$ echo $(date)
Mon Jul 22 01:17:29 AM EDT 2024
[student@workstation ~]$ echo "=====$(date)======"
=====Mon Jul 22 01:17:39 AM EDT 2024======

둘의 차이:
`cmd` : 단순하게 명령어를 실행하는 용도
$(cmd) : 임시 환경 변수를 만들어서 작업해야 하는 경우에 많이 사용. 

[student@workstation ~]$  var=$(hostname -s); echo $var 
workstation

쉘에서 임시 변수 지정
text=value
var=$(hostname -s)
; : 명령어의 격리
[student@workstation ~]$ echo ls
ls
[student@workstation ~]$ echo ; ls 

bin      Documents  file1  Pictures  Templates
Desktop  Downloads  Music  Public    Videos
;은 단순한 명령어에서 사용 하시기 바랍니다. 
그러면 단순하지 않은 명령은? = 민감한 명령 삭제. 이동. ..

[root@servera ~]# mkdir /test1
[root@servera ~]# mkdir /test2
[root@servera ~]# touch /test1/file1
[root@servera ~]# mkdir /test
[root@servera ~]# cd /test
[root@servera test]# pwd
/test

/test 라는 곳에서 /test1 아래 있는 모든 파일을 삭제
[root@servera test]# cd /test1 ; rm -rf *   << 요렇게 작업하려고 생각을 함
[root@servera test]# pwd
/test
[root@servera test]# touch file1
[root@servera test]# ls
file1
현재 디렉토리에 파일을 생성
[root@servera test]# cd /testq ; rm -rf *
-bash: cd: /testq: No such file or directory
[root@servera test]# ls
[root@servera test]# 
/test 밑의 파일이 삭제가 되었다.
위험: 이런 경우에는 ; 을 사용하면 안됨
그렇다면 무엇을 사용해야 할까?
[root@servera test]# touch file1
[root@servera test]# pwd
/test
[root@servera test]# ls
file1
&& : 앞의 명령어가 성공의 값일때만 뒤의 명령어를 실행
명령어의 리턴값이 0인 경우에만 뒤의 명령어를 실행
성공의 값 : 
[root@servera test]# ls
file1
[root@servera test]# echo $?
0
0 = 성공 정상
1-255 = 실패 에러

[root@servera test]# cd /testq &&  rm -rf *
-bash: cd: /testq: No such file or directory
[root@servera test]# ls
file1

|| : 명령어의 리턴값이 0이 아닌경우에만 뒤의 명령어를 실행
[root@servera test1]# cd /testq ||  echo 'two command'-bash: cd: /testq: No such file or directory
two command

둘을 합쳐보면 다음과 같이 이용
# CMD && CMD1 || CMD2

쉘 스크립트에서 출력 제공
[user@host ~]$ cat ~/bin/hello 

#!/usr/bin/bash   < 지시어
echo "Hello, world" < echo 명령을 통해 Hello, world를 실행, 출력

 [user@host ~]$ hello 
Hello, world 

[user@host ~]$ cat ~/bin/hello
#!/usr/bin/bash 
echo "Hello, world" 
echo "ERROR: Houston, we have a problem." >&2 

표준 입력 : STDIN: 따로 지정되지 않는 이상 키보드로 입력
표준 출력 : STDOUT: 따로 지정되지 않는 이상 모니터[터미널]로 출력
표준 에러 : STDERR: 따로 지정되지 않는 이상 모니터[터미널]로 출력

리다이렉션 : 파일이나 장치를 통해서 입력, 출력을 할 수 있다.
입력 리다이렉션: cat < file1 
[root@servera test]# echo 'RHEL9 rh134'
[root@servera test]# cat < file1
RHEL9 rh134
< : 표준 입력을 대신하여 파일에서 입력을 받는 용도
출력 리다이렉션: 
[root@servera test]# echo 'RHEL9 rh134'
RHEL9 rh134
[root@servera test]# echo 'RHEL9 rh134' > file1
[root@servera test]# cat file1
RHEL9 rh134
[root@servera test]# echo 'RHEL9 rh134' >> file1 
[root@servera test]# cat file1
RHEL9 rh134
RHEL9 rh134
[root@servera test]# echo 'RHCSA' > file1 
[root@servera test]# cat file1
RHCSA
> : 덮어쓰기, 표준 출력 리다이렉션
>> : 이어쓰기 

에러 리다이렉션:
리눅스는 에러와 출력을 따로 구분한다.
[root@servera test]# echo 111 > /dev/null 
[root@servera test]# cd /testq > /dev/null 
-bash: cd: /testq: No such file or directory

[root@servera test]# cd /testq 2> /dev/null 
[user@host ~]$ cat ~/bin/hello
#!/usr/bin/bash 
echo "Hello, world" 
echo "ERROR: Houston, we have a problem." >&2
 >&2 : 강제로 출력을 표준 에러로 변경

# CMD > file1 2>&1 :  표준 출력과 표준 에러를 file1 하나에 모두 담을때 

[24]
연습 가이드 사용법

연습 가이드 자체는 같이 하지는 않습니다.
시험 관련된 가이드 

[27]
스크립트에서 사용되는 반복문 / 조건문
반복문
for
while 
조건문
if

전체 시스템에서 부팅이 다 되었는가?
계정을 1-1000번까지 생성
openssl - 암호화 된 암호  생성 
useradd – password 
chage -d 0 


for문
for VARIABLE in LIST; do 
COMMAND VARIABLE
 done

for [변수] in [반복할 문자| 파일| 명령어로 숫자를 반복]; do 
실제 실행할 명령어 
sleep 1 < 반복문 이지만 한번씩 끊어서 작업 하기 위해 sleep으로 한 타임씩 끊어주기
done

변수:
보통 단일 문자로 많이 사용  특정한 문자열로 작성하는 것이 이후에 에러를 줄일수 있다.
[반복할 문자| 파일| 명령어로 숫자를 반복]
반복할 문자: 1 2 3 4 , a b c  d  반복문에서 사용할 문자를 수동으로 작성
파일 : `cat list`
명령어로 숫자를 반복: 
seq 1 100 
seq -w 1 100
001부터 시작




[root@servera ~]# for NUMBER in 1 2 3 4 ; do 
 > echo "hostname: lab.server${NUMBER}.example.com"
> done

변수로는 NUMBER 문자열 사용
반복된 숫자는 1- 4까지 사용
${NUMBER} 변수 지정시 중괄호 이용

반복문이 아닌 글로빙 기능을 사용하기
# touch file{1..10}
file1번에서 file10번까지 생성
bash shell 4.0 이상에서 제공 되는 기능

Bash 스크립트 종료 코드
쉘을 종료 할 때 에러 코드를 강제 삽입
스크립트가 하나로 이루어진게 아니라 이후 조건문을 위해 두개 이상의 스크립트가 결합되는 경우

https://blog.naver.com/mmarine/70094622832 


if?
조건식
if [조건] ; then
 조건이 맞는 경우의 실행
fi

if [조건] ; then
 조건이 맞는 경우의 실행
else 
 조건이 맞지않는 경우의 실행
fi

기본적으로 사용하면 좋은 for if 
while : 참인 경우 무한 반복

[root@servera test]# while true
> do
> echo "===========$(date)=========="
> sleep 1
> done

[root@servera test]# while true; do echo "===========$(date)=========="; sleep 1; done

1 while 
2 for
3 if

지금은 ansible 을 이용하게 되면 위의 조건식들을 더 쉽게 이용 할 수 있음

[35]

현재 레드햇 시험은 크게 두가지 형식으로 가능
오프라인 [특이한 경우에만 오픈] X
1 학동역 에티버스 , 패스트레인  : 키오스크 장비로 시험을 보는 방법
모든 장비가 갖춰져 있음. 몸만 가시면 바로 시험 진행 가능
2 remote exam : 온라인으로 시험을 진행
https://youtu.be/LX3VMIAuPzg?si=EB8FLuRGfzESX7gO
시험에 관련된 사항을 직접 준비.
RHEL DVD 라이브 부팅
네트워크
접속..
조용한 방 +컴퓨터
카메라  2개 
마이크 1개

시험 이후에 보통 30분 안에 결과가 날아옵니다.
PASS / NOPASS
210/300














[root@servera test]# cat list 
cat
dog
concatenate
dogma
category
educated
boondoggle
vindication
chilidog
위와 같은 파일을 생성

[root@servera test]# cat list  | grep cat 
cat
concatenate
category
educated
vindication
cat의 문자열 패턴을 검색

grep 명령이 각 라인별로 검사
문자열의 시작부분 그리고 끝 부분을 대상으로 검색 할 수 있음.

문자열의 시작 부분
[root@servera test]# cat list  | grep ^cat 
cat
category

문자열의 끝부분
[root@servera test]# cat list  | grep cat$
cat

[root@servera test]# cat /etc/ssh/sshd_config | grep port
# If you want to change the port on a SELinux system, you have to tell
# semanage port -a -t ssh_port_t -p tcp #PORTNUMBER
# WARNING: 'UsePAM no' is not supported in Fedora and may cause several
[root@servera test]# cat /etc/ssh/sshd_config | grep -i port
# If you want to change the port on a SELinux system, you have to tell
# semanage port -a -t ssh_port_t -p tcp #PORTNUMBER
#Port 22
# WARNING: 'UsePAM no' is not supported in Fedora and may cause several
#GatewayPorts no

grep를 이용하여 다음과 같이 나옴
/etc/passwd 파일에서 root 라는 단어가 들어있는 라인을 찾아 /root/lines.txt 파일로 생성
1.  순서대로 작성 할것
2. 빈칸이 없을것
3. 파일이 변화되면 안됨

grep을 이용한 특수문자 찾기
/etc 밑에서 다음 파일을 확인해 봅시다.
/etc/passwd
/etc/group
/etc/shadow
/etc/gshadow

ls -l /etc/passwd
ls -l /etc/group
ls -l /etc/shadow
ls -l /etc/gshadow

ls -l /etc/passwd-ls 
ls -l /etc/group-
ls -l /etc/shadow-
ls -l /etc/gshadow-

명령어의 체계
# CMD OP ARG
첫번째 오는건 무조건 명령어
두번째는 옵션
-a 짧은 옵션
–all 긴 옵션
a GNU 형식의 옵션
인자 : 파일이나 디렉토리를 받아 위치를 조정

[root@servera ~]# ls -l  /etc  | grep [-]$
-rw-r--r--.  1 root root      675 May 18  2022 group-
----------.  1 root root      540 May 18  2022 gshadow-
-rw-r--r--.  1 root root     1512 May 18  2022 passwd-
----------.  1 root root      942 May 18  2022 shadow-
-rw-r--r--.  1 root root       21 May 18  2022 subgid-
-rw-r--r--.  1 root root       21 May 18  2022 subuid-


 group-
그룹을 잘못 설정하게 되면 업데이트 되면서 기존의 모든 그룹설정이  업데이트되어 없어진다.
cat /etc/group- > /etc/group

[46] 랩 

vi bash-lab
echo ‘#!/usr/bin/bash’ >> bash-lab

[54] 

at
crontab  << * 
anacron
timer

at: 1회성 작업을 위한 스케쥴러
른 예는 방화벽 구성 작업 중인 시스템 관리자가 새 방화벽 구성이 작동했기 때문에 방화벽 설정을 10분 이내에 이전의 작업 상태로 재설정하기 위해 안전 작업 을 대기열에 추가하는 경우  << 현재 firewalld를 사용하면 이 구성은 굳이 필요없다.
방화벽 구성
at을 이용해서 10분뒤에 원복하면 된다.
현재 firewalld를 이용하는 경우에는 timer 옵션으로 원상태로 복구 가능하기 때문에 약간 틀린 예시: CMD –timeout 600 

실제 예시: 
다른 시스템이 재부팅 : 재부팅 이후에 체크, 재부팅 이후 접속
 
명령어 : at
데몬 [서비스] : atd 
[root@servera ~]# systemctl status atd
 atd.service - Deferred execution scheduler
     Loaded: loaded (/usr/lib/systemd/system/atd.service; enabled; vendor preset: enabled)
     Active: active (running) since Sun 2024-07-21 21:47:36 EDT; 23h ago
       Docs: man:atd(8)
   Main PID: 803 (atd)
      Tasks: 1 (limit: 10799)
     Memory: 312.0K
        CPU: 13ms
     CGroup: /system.slice/atd.service
             └─803 /usr/sbin/atd -f

Jul 21 21:47:36 servera.lab.example.com systemd[1]: Started Deferred execution scheduler.





서비스 파일 설명
[root@servera ~]# systemctl status atd
atd.service - Deferred execution scheduler : 데몬[서비스]명 확인
     Loaded: loaded (/usr/lib/systemd/system/atd.service; enabled; vendor preset: enabled)
     서비스 파일에 대해서 읽은 상태 |실제 파일 위치 | 활성화 정보 
     Active: active (running) since Sun 2024-07-21 21:47:36 EDT; 23h ago
      현재 상태 
       Docs: man:atd(8)
       사용 가능한 도움말
   Main PID: 803 (atd)
       현재 사용중인 PID 번호 확인
      Tasks: 1 (limit: 10799)
     Memory: 312.0K
        CPU: 13ms
     CGroup: /system.slice/atd.service
             └─803 /usr/sbin/atd -f

Jul 21 21:47:36 servera.lab.example.com systemd[1]: Started Deferred execution scheduler.
로그 중에 atd 와 관련있는 로그들을 추출하여 status 출력시 최신 로그를 같이 보여준다.

작업:
1 현재 시간을 확인
[root@servera log]# date   < 시간을 확인
Mon Jul 22 09:20:41 PM EDT 2024
[root@servera log]# at 09:25   < at 명령을 실행 TIMESPEC 지정
warning: commands will be executed using /bin/sh
at> logger 'test at'
at> <EOT>
job 1 at Tue Jul 23 09:25:00 2024


[root@servera log]# at 09:25  < file1   : 작업을 수동으로 입력하지 않고 파일에서 작업을 불러온다.

[root@servera log]# atq
1       Tue Jul 23 09:25:00 2024 a root
현재 지정된 작업들을 확인

[root@servera log]# at -c 1

마지막 부분에서 실제 사용한 명령을 확인 할 수 있다.

[root@servera log]# atrm 1 
at의 작업을 삭제 하는 용도

[59]
crontab

[root@servera ~]# cat /etc/crontab 
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
변수처리된 부분은 top-down 형식으로 진행되기 때문에 변수가 지정된 이후 모든 작업에 적용된다.

# For details see man 4 crontabs

# Example of job definition:
# .---------------- minute (0 - 59)
# |  .------------- hour (0 - 23)
# |  |  .---------- day of month (1 - 31)
# |  |  |  .------- month (1 - 12) OR jan,feb,mar,apr ...
# |  |  |  |  .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat
# |  |  |  |  |
# *  *  *  *  * user-name  command to be executed

[1] [2] [3] [4] [5] [user-name] [command to be executed]

각 자리가 표시 하는것 
[1]: 분
[2]: 시
[3]: 일
[4]: 월
[5]: 주

각 자리가 표시 하는것 + 숫자의 범위
[1]: 분 0-59
[2]: 시 0-23
[3]: 일 1-31
[4]: 월 1-12
[5]: 주 0-6  0==7  1-5 월-금 6 토 일0,7 



각 자리가 표시 하는것 + 숫자의 범위 + 표시 되는 형식에서 알아야 하는것
주의 사항을 알아보자

[1]: 분 0-59
0분을 사용하지 말것
모든 시간의 0분에는 시스템에서 사용되는 스케쥴러들이 많이 돌아간다.
[2]: 시 0-23
0시는 제외
하루가 시작되는 시간에 모든 초기화 작업, 정보 수집이 새로 시작되는데 이때 크론을 걸게 되면 부하가 심해짐
[3]: 일 1-31
1 제외
30일,28일,27
[4]: 월 1-12
[5]: 주 0-6  0==7  1-5 월-금 6 토 일0,7 
사용하는 사람들마다 0과 7을 섞어서 사용함.
팀에서 포맷 맞춰서 사용

숫자를 이용할때 사용 가능한 형식
[1]: 분 0-59

* 모든 시간을 지정

10 : 10분이라는 시간에 지정

10,15 10분,15분이라는 시간에 지정

10-15 10,11,12,13,14,15 분이라는 시간에 지정

10-15,20 10,11,12,13,14,15,20 분이라는 시간에 지정

*/N  N분에 한번씩
*/2 2분에 한번씩
*/10 10분에 한번씩

*/N 단점:  크론탭은 1분에 한번씩 동작
*/10  10분에 한번씩 로그를 출력해야겠다.
*/10 * * * * PATH/script.log

XXXX  XX XX XX:10
XXXX  XX XX XX:20
XXXX  XX XX XX:30
XXXX  XX XX XX:40

reboot, restart : 부팅 이후 시간기준으로 10분간격
service start time XXXX  XX XX XX:16
XXXX  XX XX XX:16
XXXX  XX XX XX:26
XXXX  XX XX XX:36
XXXX  XX XX XX:46

[user-name] [command to be executed]

[user-name] 이전방식 : /etc/crontab 파일에서 사용하다보니 어떤 사용자로 실행 할 것인가를 지정
 [command to be executed] 실행 파일을 지정 실행 권한도 같이 필요.

과거에는 /etc/crontab 파일에 작성을 root 계정으로 하고 관리도 root가 진행
현재:  더이상 /etc/crontab 파일에 작성하지 않는다. 이 파일은 이제 참고용 파일로 남겨둠

각각 계정별로 작업을 한다.
1 root 계정으로 각 사용자 파일을 생성
[root@servera cron]# crontab -e -u student
2 실제 사용자 계정으로 파일을 생성
[root@servera cron]# su -  student
Last login: Mon Jul 22 02:15:38 EDT 2024 from 172.25.250.9 on pts/0
[student@servera ~]$ crontab -e 

# crontab 
-l : crontab 파일을 확인
-e : crontab 파일을 수정
-r:  crontab 파일을 삭제
-u [계정] 위 명령들을 현재 사용자가 아니라 특정 사용자로 지정하여 진행

[root@servera ~]# cd
[root@servera ~]# id
uid=0(root) gid=0(root) groups=0(root) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

[root@servera ~]# crontab -l
no crontab for root   < 현재는 크론 파일이 생성되지 않음.

[root@servera ~]# crontab -e

"/tmp/crontab.Y7L9LY" 0L, 0B   << crontab  파일의 경우는 임시파일로 생성 한뒤 실제 파일로 변환
i
*/2 * * * * logger 'test log'
esc
:wq

crontab: installing new crontab   < 정상 저장이 완료

[root@servera ~]# cd /var/spool/cron/
[root@servera cron]# pwd
/var/spool/cron
[root@servera cron]# ls
root
[root@servera cron]# file root
root: ASCII text
[root@servera cron]# cat root 
*/2 * * * * logger 'test log'
[root@servera cron]# crontab -l
*/2 * * * * logger 'test log'

[root@servera cron]# crontab -e -u student 
*/2 * * * * logger 'test log student'
[root@servera cron]# ls -l
total 8
-rw-------. 1 root root 30 Jul 22 22:17 root
-rw-------. 1 root root 38 Jul 22 22:19 student
생성된 파일의 경우에는 root 권한으로 생성 되지만 상관 없음.
실제 진행되는 프로세스는  student 파일명으로 동일한 계정에서 실행

[root@servera cron]# journalctl | grep 'test log'
Jul 22 22:18:01 servera.lab.example.com CROND[2662]: (root) CMD (logger 'test log')
Jul 22 22:18:01 servera.lab.example.com root[2662]: test log
Jul 22 22:18:01 servera.lab.example.com CROND[2660]: (root) CMDEND (logger 'test log')
Jul 22 22:20:01 servera.lab.example.com CROND[2685]: (root) CMD (logger 'test log')
Jul 22 22:20:01 servera.lab.example.com root[2685]: test log
Jul 22 22:20:01 servera.lab.example.com CROND[2680]: (root) CMDEND (logger 'test log')
Jul 22 22:20:01 servera.lab.example.com CROND[2696]: (student) CMD (logger 'test log student')
Jul 22 22:20:01 servera.lab.example.com student[2696]: test log student
Jul 22 22:20:01 servera.lab.example.com CROND[2681]: (student) CMDEND (logger 'test log student')
현재 crontab에 지정한 내용들이 정상 동작중이다.

[root@servera cron]# ls
root  student
[root@servera cron]# crontab -r 
[root@servera cron]# ls
student
[root@servera cron]# rm -rf student 
[root@servera cron]# crontab -u student -l
no crontab for student

crontab은 저장 하는 순간부터 진행

[root@servera cron]# systemctl status crond
● crond.service - Command Scheduler
     Loaded: loaded (/usr/lib/systemd/system/crond.service; enabled; vendor preset: enabled)
     Active: active (running) since Sun 2024-07-21 21:47:36 EDT; 24h ago
   Main PID: 804 (crond)
      Tasks: 1 (limit: 10799)
     Memory: 1.3M
        CPU: 1.163s
     CGroup: /system.slice/crond.service
             └─804 /usr/sbin/crond -n

Jul 22 20:01:01 servera.lab.example.com CROND[2408]: (root) CMD (run-parts /etc/cron.hourly)

프로세스의 진행 시간을 확인 하는 명령어
[root@servera cron]# cd 
[root@servera ~]# dd if=/dev/zero of=/root/bigfile bs=1M count=1024
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.09995 s, 511 MB/s
[root@servera ~]# ls
bigfile  lines.txt  list  list1
[root@servera ~]# file bigfile 
bigfile: data


[root@servera ~]# time dd if=/dev/zero of=/root/bigfile bs=1M count=1024
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.4091 s, 446 MB/s

real    0m2.474s   << * 
user    0m0.003s
sys     0m0.683s

# man crond
# man -s 5 crontab
# man -s 8 crontab

[62]
시험에서 crontab 이 출제 
2.3 구문

 [65]
반복 실행 시스템 작업 예약 

[root@servera ~]# cd /etc/cron.
cron.d/   : 반복 실행될 시스템 작업에 대한 예약
cron.hourly/ :시간에 동작
cron.weekly/ :  주에 한번 동작
cron.daily/ : 일에 한번 동작
cron.monthly/ :  월에 한번 동작
[root@servera ~]# cd /etc/cron.d
[root@servera cron.d]# ls
0hourly
[root@servera cron.d]# 
[root@servera cron.d]# cat 0hourly 
# Run the hourly jobs
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
01 * * * * root run-parts /etc/cron.hourly
run-parts : 특정 디렉토리에 있는 스크립트를 실행하는 데 사용하는 명령


anacron
crontab과의 차이점: 
crontab의 경우에는 사용자 관점
anacron의 경우에는 시스템 관점
crontab의 경우는 충돌이나 재부팅등 사용이 불가능한 상황에 놓여 시간을 놓치게 된다면 해당 작업을 무시. 그 다음 시간대에 작업을 재개
anacron의 경우는  충돌이나 재부팅등 사용이 불가능한 상황이 오게 되면 그 이후 재부팅, 충돌에 대한 해결이 된 다음 곧 바로 작업을 재개

[root@servera cron.hourly]# cat /etc/anacrontab 
# /etc/anacrontab: configuration file for anacron

# See anacron(8) and anacrontab(5) for details.

SHELL=/bin/sh
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
# the maximal random delay added to the base delay of the jobs
RANDOM_DELAY=45
# the jobs will be started during the following hours only
START_HOURS_RANGE=3-22

#period in days    | delay in minutes  |  job-identifier  |  command
 1                                 5                                  cron.daily              nice run-parts /etc/cron.daily
 7                                 25                               cron.weekly           nice run-parts /etc/cron.weekly
@monthly                 45                                cron.monthly            nice run-parts /etc/cron.monthly
[root@servera cron.hourly]# cat /etc/anacrontab  | tail -4 | column -t

timer

init > systemd

systemd에서 사용 가능한 timer 유닛
현재 anacron의 경우에는 timer 유닛으로 모두 변경
[root@servera cron.hourly]# systemctl -t timer
  UNIT                         LOAD   ACTIVE SUB     DESCRIPTION                           
  dnf-makecache.timer          loaded active waiting dnf makecache --timer
  logrotate.timer              loaded active waiting Daily rotation of log files
  mlocate-updatedb.timer       loaded active waiting Updates mlocate database every day
  systemd-tmpfiles-clean.timer loaded active waiting Daily Cleanup of Temporary Directories

timer의 장점:
crontab은 1분에 한번 작용
timer는 초단위 가능

/tmp
/var/tmp

[68] 실습

[84]
시스템 로그 아키텍처 설명

로그 : 시스템 감사 및 문제 해결에 사용 , 이벤트에 대한 기록
로그는 보통 텍스트 파일로 구성 되어있지만 모두가 그런것은 아님

[root@servera ~]# cd /var/log
[root@servera log]# ls
audit                  dnf.librepo.log  maillog   spooler
boot.log               dnf.log          messages  sssd
boot.log-20240722      dnf.rpm.log      private   tallylog
btmp                   firewalld        qemu-ga   tuned
chrony                 hawkey.log       README    wtmp
cloud-init.log         insights-client  rhsm
cloud-init-output.log  kdump.log        samba
cron                   lastlog          secure
[root@servera log]# file * | grep -v  empty | grep -v directory | grep -v ASCII | grep -v text | grep -v symbolic
lastlog:               data
wtmp:                  data
 
data 파일의 경우에는 텍스트가 아니기 때문에 바로 읽을수 없음
특정 명령을 이용해야만 가능한 경우들도 있다.
로그 파일이든 뭐든 사용자가 직접 생성한 파일이 아닌 경우에는 
file 명령을 통해서 형식을 확인하는 것이 좋다.
[root@servera log]# file lastlog
lastlog: data
[root@servera ~]# ls
bigfile  lines.txt  list  list1
[root@servera ~]# file bigfile 
bigfile: data

[root@servera ~]# journalctl  현재부팅 이후부터 모든 로그를 가지고 있음
[root@servera ~]# journalctl -f 최근의 로그만 확인


systemd-journald  재부팅 후 유지되지 않는 파일 시스템에 저장 /run 밑에서 사용을 하기 때문에 그렇다.

syslog / rsyslog
사용되는 용어들도 알아야 하고 나오는 메시지도 봐야 하고 ..

syslog 기능 개요
메세지 종류  설  명
authpriv    개인 인증을 요하는 프로그램 유형이 발생한 메세지(EX: su, telnet, ssh)
cron        crontab(crond), at(atd) 프로그램이 발생한 메세지(EX: crontab, at)
daemon      시스템 데몬이 생성한 메세지. (EX: telnetd, ftpd) 
kern        커널이 발생한 메세지
lpr         프린트 유형의 프로그램이 발생한 메세지
mail        메일시스템이 발생한 메세지.
mark        syslogd 데몬에 의해 생성된 일정한 시간간격의 반복적인 메세지
news        유즈넷 뉴스 프로그램 유형이 발생한 메세지
security(auth)  인증 프로그램이 생성한 메세지(EX: login)
syslog     syslog 프로그램이 생성한 메세지(EX: syslogd)
user       사용자 프로그램에 의해 생성된 메세지(EX: top, system-config-*)
uucp       UUCP(Unix to Unix Copy) 시스템이 발생한 메세지
local0-7   예약된 메세지 종류, 여분으로 남겨둔 유형(EX: local0, local1, local2, ...)
*          모든 메세지(단, mark 메세지 종류는 제외)

 메세지 난이도(Level)
메세지 레벨        설  명
0 emerg (panic)   패닉상태, 모든 사용자에게 전달되어야 할 위험한 상황의 메세지
1 alert           즉각적인 조치를 위해야 상황의 메세지
2 crit            시스템에 문제가 생기는 단계의 메세지
3 error (err)     에러가 발생하는 경우의 메세지
4 warning (warn)  주의를 요하는 경고 메세지
5 notice          특별한 주의를 요하는 에러는 아닌 메세지
6 info            통계, 기본정보 메세지
7 debug           프로그램을 디버깅할 때 생성되는 메세지

실제 로그를 수집 하기 위한 정책에 대한 파일
메시지 기록 위치
# cat /etc/rsyslog.conf

Facility : 메세지의 종류
Level : 메세지의 난이도(위험 수준)
Action : 메세지 기록 위치


/etc/rsyslog.conf의 구성
authpriv.* /var/log/secure
Facility.Level      Action

메세지 기록 위치      설  명
/var/log/messages     기록을 남길 특정한 로그 파일 선택(예: /var/log/file.log)
만약에 없는 파일에 기록을 하는 경우 파일이 없어 에러가 날 수 있음. 미리 touch로 빈 파일을 생성
/dev/console          기록을 남길 콘솔에 로그 기록 남김(예: /dev/console)
user01,root, *        특정한 사용자나 모든 사용자 선택(예: user01, *)
@hostA, @172.16.8.254 다른 호스트 선택(예: @172.16.8.254)-> 보통 로그 서버 선택

예시파일
# cat /etc/syslog.conf
# Log all kernel messages to the console.
# Logging much else clutters up the screen.
#kern.*                                                 /dev/console
커널에 관련한 모든 메세지는 콘솔창에 출력한다.

# Log anything (except mail) of level info or higher.
# Don't log private authentication messages!
*.info;mail.none;news.none;authpriv.none;cron.none      /var/log/messages
메일(mail),뉴스(news),개인인증(authpriv),크론(cron) 메세지를 제외한 대부분의 메세지를 /var/log/messages 파일에 기록한다.

# The authpriv file has restricted access.
authpriv.*                                              /var/log/secure
개인적인 인증에 관련한 부분은 /var/log/secure 파일에 기록한다.(su, sshd)

# Log all the mail messages in one place.
mail.*                                                  -/var/log/maillog
메일에 관련한 모든 메세지는 /var/log/maillog 파일에 기록한다.
 -/var/log/maillog에서 -의 기능
기록 후 파일 동기화를 생략하기 위해 -를 접두사로 붙여 지정 가능
시스템이 쓰기를 시도 할 때 충돌 할 가능성이 있다. 성능에 따라서 사용 할 때 가 있음.

# Log cron stuff
cron.*                                                  /var/log/cron
크론(cron) 메세지 모두를 /var/log/cron 파일에 기록한다.

# Everybody gets emergency messages
*.emerg                                                 *
모든 경우 위험한 메세지(패닉 상태)는 모든 사용자에게 뿌린다.

# Save news errors of level crit and higher in a special file.
uucp,news.crit                                          /var/log/spooler
uucp,news에 대한 critical 이상 메세지는 /var/log/spooler 파일에 기록한다.

# Save boot messages also to boot.log
local7.*                                                /var/log/boot.log
미리 예약된 local7 종류에 대한 모든 메세지(부팅시의 메세지)를 /var/log/boot.log 파일에 기록한다.

#
# INN
#
news.=crit                                        /var/log/news/news.crit
news.=err                                         /var/log/news/news.err
news.notice                                       /var/log/news/news.notice

각 로그파일은 var/log 디렉토리에 존재한다.

로그파일                      설   명
(T) /var/log/messages      메일(mail), 뉴스(news), 인증(authpriv), 크론(cron) 메세지를 제외한 시스템의 전반적인 메세지를 기록하는 로그파일이다. 리눅스에서는 가장 많은 메세지를 담고 있는 파일이다. 
(TUI) # tail -f /var/log/messages 
(T) /var/log/secure        개인인증을 기록하는 로그파일이다.(EX: su, telnet, ssh)
(T) /var/log/dmesg         시스템이 부팅할때 메세지를 기록하는 로그 파일이다. dmesg 명령어가 이 파일을 참조하여 정보를 출력한다. # dmesg 
(D) /var/log/lastlog       사용자의 가장 마지막에 로그인한 시간을 기록하는 로그 파일이다. 바이너리 파일이므로 안의 내용을 cat 명령어를 통해 확인할 수 없다. lastlog 명령어가 이 파일을 참조하여 정보를 출력한다. # lastlog 
(T) /var/log/cron          crontab, at 명령어를 통해 실행했던 작업을 기록하는 로그 파일이다.
- crond -> crontab CMD
- atd   -> at CMD
(T) /var/log/maillog       postfix 메일 관련 작업을 기록하는 로그 파일이다.
(T) /var/log/xferlog       FTP 서버에서 업로드/다운로드을 기록하는 로그 파일이다.
(D) /var/log/wtmp          사용자의 로그인/로그아웃 시간을 기록하는 로그파일이다. 데이터 파일이므로 안의 내용을 cat 명령어를 통해 확인할 수 없다. last 명령어가 이 파일을 참조하여 정보를 출력한다. # last   (# last oracle)
(D) /var/run/utmp          현재 로그인한 사용자 정보를 기록하는 로그 파일이다. 데이터 파일이므로 안의 내용을 cat 명령어를 통해 확인 할 수 없다. who 명령어가 이 파일을 참조하여 정보를 출력한다. # who

logrotate
로그파일은 시간이 지남에따라 계속 계속 커진다.
1~2M 일때는 사실 상관 없음. 하지만 수많은 로그 데이터가 쌓이게 되면 파일을 오픈 하는 것도 힘들다.

32bit system : 코드 파일 ? 2.4G
open: 4시간 
하나의 파일이 너무 커지는 것을 방지하기 위해 logrotate를 사용한다.
logrotate를 통해 순환 파일로 제공
이전 방식: /var/log/messages.0 .1 .2 .3 
 /var/log/messages


현재 방식: var/log/messages-20220320

로그 데이터
Mar 20 20:11:48 localhost sshd[1433]: Failed password for student from 172.25.0.10 port 59344 ssh2

Mar 20 20:11:48 : 시간 timestamp
localhost  host 
sshd[1433]: id 사용자나 데몬이름 + PID
Failed password for student from 172.25.0.10 port 59344 ssh2 : 해당 로그의 상세한 내용
lastb

보통은logrotate를 통해 이동한 로그 파일들은 삭제 되기전에 tar 명령어를 통해 특정 디렉토리로 이동, 일정시간 저장 한 뒤에 ISO 이미지로 생성
cp 복사 > 시간 현재 시간 업데이트
tar -> 시간+ 권+ 소 + + 기존 파일 그대로 가지고 이동
컨테이너로 가면서 tar 명령어가 더 많이 사용

로그 이벤트 모니터링 방법
# tail -f /path/to/file : 일반 파일로 저장된 내용을 확인 하기 위해 사용
[root@servera log]# journalctl -f   < systemd-journal에서 나오는 데이터를 바로 확인

[104]
저널 보존

저널의 경우에는 부팅시 - 종료시까지 사용하고 기본 값은 삭제
/etc/systemd/ journald.conf 옵션을 조정하여 영구 보관 할 수 있도록 만들어야 함
#Storage=auto
auto :  /var/log/journal 디렉토리가 존재 한다면 systemd가 영구 보관용 데이터를 사용하고 없다면 /run/log 에서 처리
persistent : /var/log/journal 생성 부팅-종료시까지의 모든 데이터를 보관

필요하다면 저널 영구 저장 장치에 대한 작업을 실시한다.
ansible을 사용하다보니까 초기 구축시에 해당 작업을 완료.

NTP : network time protocol
많은 네트워크 프로그램들은 시간 동기화가 필요하게 된다. 네트워크 상에서 서로 연동하는 시스템들 간에 시간이 일치 하지 않게 된다면 다수의 시스템 장애처리, 성능카운트, 분석, 로깅등 수많은 문제점을 갖게된다.

시간 서버의 발전
이전에는 rdate 사용
현재는 ntp

[root@servera ~]# timedatectl
               Local time: Tue 2024-07-23 03:42:49 EDT   현재 시스템 시간
           Universal time: Tue 2024-07-23 07:42:49 UTC UTC의 시간 
                 RTC time: Tue 2024-07-23 07:42:49 하드웨어 시간
                Time zone: America/New_York (EDT, -0400) 현재 서버의 타임존 
System clock synchronized: yes  시간 동기화가 되어 있는지 확인
              NTP service: active  NTP 서비스가 실행중인지 
          RTC in local TZ: no

# date 월일시분
[root@servera ~]# date 06121300 
Wed Jun 12 01:00:00 PM EDT 2024

[root@servera ~]# timedatectl set-time 13:00:00
Failed to set time: Automatic time synchronization is enabled

[root@servera ~]# timedatectl set-time 13:00:00
Failed to set time: Automatic time synchronization is enabled
[root@servera ~]# timedatectl set-ntp 
false  true   
[root@servera ~]# timedatectl set-ntp false 
[root@servera ~]# timedatectl set-time 13:00:00

[root@servera ~]# timedatectl set-time '2012-10-30 13:00:00'
[root@servera ~]# timedatectl 
               Local time: Tue 2012-10-30 13:00:04 EDT
           Universal time: Tue 2012-10-30 17:00:04 UTC
                 RTC time: Tue 2012-10-30 17:00:04
                Time zone: America/New_York (EDT, -0400)
System clock synchronized: no
              NTP service: inactive
          RTC in local TZ: no

위의 내용은 timedatectl 명령을 이용하여 수동으로 시간을 변경
보통은 이렇게 하지 않음 NTP 서버가 동작하고 해당 서버를 바라보게 하기 때문에 이렇게 사용하지는 않는다.
[root@servera ~]# dnf -y remove chrony
chrony 삭제 후 진행

[root@servera ~]# ls -l $(which yum)
lrwxrwxrwx. 1 root root 5 Mar 21  2022 /usr/bin/yum -> dnf-3
[root@servera ~]# ls -l $(which dnf)
lrwxrwxrwx. 1 root root 5 Mar 21  2022 /usr/bin/dnf -> dnf-3

[root@servera ~]# dnf list
[root@servera ~]# dnf repolist
[root@servera ~]# dnf clean all

[root@servera ~]# dnf install telnetRed Hat Enterprise Linux 9.0 BaseOS (dvd)   779 kB/s | 2.7 kB     00:00    
Red Hat Enterprise Linux 9.0 AppStream (dvd 1.1 MB/s | 2.8 kB     00:00    
Dependencies resolved.
============================================================================
 Package Arch    Version          Repository                           Size
============================================================================
Installing:
 telnet  x86_64  1:0.17-85.el9    rhel-9.0-for-x86_64-appstream-rpms   66 k

Transaction Summary
============================================================================
Install  1 Package

Total download size: 66 k
Installed size: 121 k
Is this ok [y/N]: y
Downloading Packages:
telnet-0.17-85.el9.x86_64.rpm               398 kB/s |  66 kB     00:00    
----------------------------------------------------------------------------
Total                                       394 kB/s |  66 kB     00:00     
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                    1/1 
  Installing       : telnet-1:0.17-85.el9.x86_64                        1/1 
  Running scriptlet: telnet-1:0.17-85.el9.x86_64                        1/1 
  Verifying        : telnet-1:0.17-85.el9.x86_64                        1/1 

Installed:
  telnet-1:0.17-85.el9.x86_64                                               

Complete!
[root@servera ~]# 
[root@servera ~]# dnf -y install ftp 
Red Hat Enterprise Linux 9.0 BaseOS (dvd)   628 kB/s | 2.7 kB     00:00    
Red Hat Enterprise Linux 9.0 AppStream (dvd 1.3 MB/s | 2.8 kB     00:00    
Dependencies resolved.
============================================================================
 Package
       Arch     Version          Repository                            Size
============================================================================
Installing:
 ftp   x86_64   0.17-89.el9      rhel-9.0-for-x86_64-appstream-rpms    64 k

Transaction Summary
============================================================================
Install  1 Package

Total download size: 64 k
Installed size: 112 k
Downloading Packages:
ftp-0.17-89.el9.x86_64.rpm                  1.1 MB/s |  64 kB     00:00    
----------------------------------------------------------------------------
Total                                       1.1 MB/s |  64 kB     00:00     
Running transaction check
Transaction check succeeded.
Running transaction test
Transaction test succeeded.
Running transaction
  Preparing        :                                                    1/1 
  Installing       : ftp-0.17-89.el9.x86_64                             1/1 
  Running scriptlet: ftp-0.17-89.el9.x86_64                             1/1 
  Verifying        : ftp-0.17-89.el9.x86_64                             1/1 

Installed:
  ftp-0.17-89.el9.x86_64                                                    

Complete!
[root@servera ~]# 

install  : 설치
remove  : 삭제

ntp를 구성하여 클라이언트로서 동작 시키기
1 패키지 설치
# dnf -y install chrony
[root@servera ~]# rpm -ql chrony
chrony 패키지에서 추출된 파일 목록 확인
[root@servera ~]# rpm -qlc chrony
설정 파일만 모아보기
[root@servera ~]# rpm -qld chrony
문서파일만 모아보기
/etc/chrony.conf : chrony에서의 설정 파일
해당 파일에서 클라이언트 구성은 1번 섹션이다.
server / pool
server : 단독 서버 
pool : 다수의 서버로 구성되어 있다.
server 2.rhel.pool.ntp.org iburst
server : 서버를 지정
2.rhel.pool.ntp.org : 서버로 지정할 호스트/ IP 
iburst : 타임 서버들은 초기 서비스에 걸리는 동기화 시간이 존재한다. 서비스를 시작하고 외부, 내부망에서 네트워크를 확인한 뒤 패킷을 보내 동기화를 시작
패킷을 전송하고 다음 패킷까지 걸리는 시간은 16초가 걸린다. 이 시간을 줄이기 위해서 한번에 8개를 동시에 전송하라는 옵션이다.
[root@servera ~]# vi  /etc/chrony.conf
#pool 2.rhel.pool.ntp.org iburst  <  기존 라인 주석 처리
server classroom.example.com iburst 
:wq

여기부터 따라하면 됩니다.
[root@servera ~]# timedatectl set-ntp false 
[root@servera ~]# timedatectl set-time '2012-10-30 13:00:00'
[root@servera ~]# systemctl daemon-reload   < systemd에 변경된 파일이 있으면 업데이트
[root@servera ~]# systemctl enable --now chronyd 
Created symlink /etc/systemd/system/multi-user.target.wants/chronyd.service → /usr/lib/systemd/system/chronyd.service.
사용한 명령:  systemctl enable --now chronyd 
1 systemctl start chronyd 
2 systemctl enable chronyd 
[root@servera ~]# systemctl is-active chronyd
active
[root@servera ~]# systemctl is-enabled chronyd
enabled

[root@servera ~]# systemctl status chronyd
● chronyd.service - NTP client/server
     Loaded: loaded (/usr/lib/systemd/system/chronyd.service; enabled; vendor preset: enabled)
     Active: active (running) since Tue 2012-10-30 13:00:38 EDT; 11 years 8 months ago
       Docs: man:chronyd(8)
             man:chrony.conf(5)
    Process: 27544 ExecStart=/usr/sbin/chronyd $OPTIONS (code=exited, status=0/SUCCESS)
   Main PID: 27546 (chronyd)
      Tasks: 1 (limit: 10799)
     Memory: 804.0K
        CPU: 21ms
     CGroup: /system.slice/chronyd.service
             └─27546 /usr/sbin/chronyd -F 2

[root@servera ~]# timedatectl 
               Local time: Tue 2024-07-23 04:24:32 EDT
           Universal time: Tue 2024-07-23 08:24:32 UTC
                 RTC time: Tue 2024-07-23 08:24:32
                Time zone: America/New_York (EDT, -0400)
System clock synchronized: yes   <<< 싱크가 완료
              NTP service: active    <<< 활성화
          RTC in local TZ: no

다른 터미널을 열어서 root 권한으로 아래의 명령을 실행한다.
[root@servera ~]# while true; do date 06191300 ; sleep 1; done

[root@servera ~]# chronyc sources
MS Name/IP address         Stratum Poll Reach LastRx Last sample               
===============================================================================
^? 172.25.254.254                0   6   377     -     +0ns[   +0ns] +/-    0ns

[root@servera ~]# chronyc sources -v

  .-- Source mode  '^' = server, '=' = peer, '#' = local clock.
 / .- Source state '*' = current best, '+' = combined, '-' = not combined,
| /             'x' = may be in error, '~' = too variable, '?' = unusable.
||                                                 .- xxxx [ yyyy ] +/- zzzz
||      Reachability register (octal) -.           |  xxxx = adjusted offset,
||      Log2(Polling interval) --.      |          |  yyyy = measured offset,
||                                \     |          |  zzzz = estimated error.
||                                 |    |           \
MS Name/IP address         Stratum Poll Reach LastRx Last sample               
===============================================================================
^? 172.25.254.254                0   6   377     -     +0ns[   +0ns] +/-    0ns


[root@servera ~]# watch -n 1 'chronyc sources -v'
시간 동기화를 모니터링
[root@servera ~]# timedatectl set-ntp false ; date 06191300 ; timedatectl set-ntp true 



  .-- Source mode  '^' = server, '=' = peer, '#' = local clock.
 / .- Source state '*' = current best, '+' = combined, '-' = not combined,
| /             'x' = may be in error, '~' = too variable, '?' = unusable.
||                                                 .- xxxx [ yyyy ] +/- zzzz
||      Reachability register (octal) -.           |  xxxx = adjusted offset,
||      Log2(Polling interval) --.      |          |  yyyy = measured offset,
||                                \     |          |  zzzz = estimated error.
||                                 |    |           \
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^* 172.25.254.254                2   6    17    32  -1154ns[-3900ns] +/- 3832us
동기화가 완료 되면 Reach 값이 377로 유지된다.

다시한번 이렇게 구성
[root@servera ~]# timedatectl set-ntp false ; date 06191300

[root@servera ~]# timedatectl set-ntp true  
[root@servera ~]# chronyc -a makestep 
200 OK
200 OK chronyc -a makestep 이 명령어는 NTP 서버와 최대한 근접하게 시간을 수정하여 동기화 되는 시간을 줄여줄수 있다.

타임존을 변경해야 하는 경우에는 아래와 같이 사용한다.
[root@servera ~]# timedatectl set-timezone Asia/Seoul 
[root@servera ~]# timedatectl
               Local time: Tue 2024-07-23 17:35:30 KST
           Universal time: Tue 2024-07-23 08:35:30 UTC
                 RTC time: Tue 2024-07-23 08:35:30
                Time zone: Asia/Seoul (KST, +0900)
System clock synchronized: yes
              NTP service: active
          RTC in local TZ: no


MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^* 172.25.254.254                2   6   377    49  +5620ns[  +17us] +/- 4601us

Reach 이 377이면 동기화가 완료 되었다 라고 보면 된다.

시험에서는 설정 파일을 구성하고 ntp 서비스에서 대해 enabled만 구성되면 끝
패키지 설치 / 설정 파일 변경 / enabled 확인

연습문제는 내일 오전에 하고
 
storage 부터 7장 나가도록 하겠습니다.

https://meet.google.com/hqd-uhrn-tpb

114 / 118 
시험용 NTP * 
journalctl 
[217]

장치 인식부터 마운트까지의 디스크 관리
현재 과정에서는 장치인식은 따로 하지 않는다. 되어 있다고 가정

원판형 디스크 
장치 인식 [글자만]
—-----------------------------------
파티션 작업
파일시스템 포맷
마운트
오버마운트 주의사항
fstab 파일 작성
재부팅 -> 마운트 확인

디스크

IDE X
S-ATA
SCSI
SAS
SSD
Nvme
PCI - connect
인터페이스는 위와 같이 분류

https://babytiger.netlify.app/posts/hdd/

흔히 사용하는 S-ATA 디스크를 분해하였을때는 다음과 같은 모양으로 나타나게 된다.
스핀들(Spindle) : 플래터를 돌러주는 축
스핀들모터 - 스핀들과 완벽하게 물려있어 스핀들을 회전시켜 스핀들과 함께 플래터를 회전 시키는 역할을 한다.
플래터(Platters) : 원판(플랫터), 데이터가 저장되는 공간
헤드(Heads) : 데이터를 읽어들이는 부분
헤드 암(Head actuator arm) : 데이터를 읽어 들이는 팔




윈도우에서 사용되는 파티션 이름[디스크]
C:\ D:\..

리눅스에서 사용되는 파티션 이름
-  디스크이름 
SCSI DISK 
/dev/sda /dev/sdb /dev/sdc …. 

IDE DISK ( 이전에 사용하던 디스크 방식) 
/dev/hda /dev/hdb /dev/hdc

Nvme DISK 
/dev/nvme0 /dev/nvme1 /dev/nvme2
    
가상디스크  
/dev/vda /dev/vdb /dev/vdc
가상디스크 버전 업 
/dev/xvda /dev/xvdb /dev/xvdc
    
-  파티션이름 
Primaty Patition(1-4)     주파티션 
Extended Partition(5-15) 확장파티션 
Logical Patition     논리파티션 

-  리눅스 최대 파티션 개수 
MBR
15 partitions : /dev/sda1~15 
주 파티션 4 (확장파티션1개 포함)
확장파티션 : 논리파티션을 담을 수 있는 공간
논리 파티션 5-15

주 파티션           
주파티션(Primary Partition), 기본 파티션으로 더 이상 쪼갤 수 없는 파티션이다. 
하나의 하드디스크에는 주 파티션과 확장 파티션을 네 개까지만 만들 수 있다는 제한이 있다. 
네 개 이상의 파티션이 필요한 경우 확장 파티션을 만들어 그 안에 논리 파티션을 두게 된다.
확장 파티션
하드 디스크를 여러 개의 파티션으로 나누고자 할 때 만드는 파티션이다.
확장 파티션은 저장 공간이 없으며, 안에 논리 파티션을 만들 수 있게 해주는 커다란 그릇 역할만 한다.

파티션: 영역을 나누기 위함
루트파티션의 파괴시 다중 파티션으로 자료 보호, 백업의 용이성
파일 시스템 점검 시간 축소

SCSI 컨트롤러에 연결되어 있는 순서에 따라 하드디스크를 표현

IDE
                          Window         Linux 
Primary master HardDisk   disk0          /dev/hda
Primary slave HardDisk    disk1          /dev/hdb
Secondary master HardDisk disk2          /dev/hdc
Secondary slave HardDisk  disk3          /dev/hdd

SCSI
                          Window         Linux 
Primary master HardDisk   disk0          /dev/sda
Primary slave HardDisk    disk1          /dev/sdb
Secondary master HardDisk disk2          /dev/sdc
Secondary slave HardDisk  disk3          /dev/sdd

GPT: 128개의 파티션
2T 이상 DISK에서 사용

장치 인식
# lsblk
# lspci | grep -i scsi
# lsscsi
# lsscsi -d 
# lsscsi -l
# lsscsi -g

[0:0:0:0]
H : Host Adapter ID
C : SCSI Channel on Host Adapter 
T : ID 
L : LUN

[root@servera ~]# find /sys/devices -name scan -exec ls -l {} \;
--w-------. 1 root root 4096 Jul 23 16:14 /sys/devices/pci0000:00/0000:00:01.1/ata1/host0/scsi_host/host0/scan
--w-------. 1 root root 4096 Jul 23 16:14 /sys/devices/pci0000:00/0000:00:01.1/ata2/host1/scsi_host/host1/scan

# echo “- - -” >  /sys/devices/pci0000:00/0000:00:01.1/ata1/host0/scsi_host/host0/scan

echo에서 사용한 “- - -”에 들어가는 인자값은 뒤의 인수에서 자동으로 추가된다. 해당 값을 알아보자. 첫 번째 기호 : Channel Number 두 번째 기호 : SCSI Target ID 세 번째 기호 : LUN Values 자세한 내용은 여기에 : https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/online_storage_reconfiguration_guide/scanning-storage-interconnects

# echo 1 > /sys/block/sdc/device/delete : 블록 디바이스의 이름을 정확히 지정해야 한다.

—------------------------------------------------------------------------------------------
디바이스는 등록되어 있다.
[root@servera ~]# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sr0     11:0    1  558K  0 rom  
vda    252:0    0   10G  0 disk 
├─vda1 252:1    0    1M  0 part 
├─vda2 252:2    0  200M  0 part /boot/efi
├─vda3 252:3    0  500M  0 part /boot
└─vda4 252:4    0  9.3G  0 part /
vdb    252:16   0    5G  0 disk 
vdc    252:32   0    5G  0 disk 
vdd    252:48   0    5G  0 disk 

현재 시스템의 장치는 크게 두분류
DVDROM [sr0]/ DISK[vda..]
disk로만 나오게 되면 아직 사용하지 않은 상태
파티션이 붙어있는 vda의 경우에는 파티셔닝이 되어 사용중

[root@servera ~]# lsblk --fs
NAME FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
sr0  iso966 Jolie config-2
                        2024-07-22-01-47-15-00                              
vda                                                                         
├─vda1
│                                                                           
├─vda2
│    vfat   FAT16       7B77-95E7                             192.8M     3% /boot/efi
├─vda3
│    xfs          boot  5e75a2b9-1367-4cc8-bb38-4d6abc3964b8  334.7M    32% /boot
└─vda4
     xfs          root  fb535add-9799-4a27-b8bc-e8259f39a767    5.7G    38% /
vdb                                                                         
vdc                                                                         
vdd 
–fs 명령을 통하여 UUID를 같이 출력

[root@servera ~]# lsblk --fs /dev/vda4
NAME FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
vda4 xfs          root  fb535add-9799-4a27-b8bc-e8259f39a767    5.7G    38% /
한개의 디스크만 명시하여 사용가능

[root@servera ~]# blkid
/dev/sr0: BLOCK_SIZE="2048" UUID="2024-07-22-01-47-15-00" LABEL="config-2" TYPE="iso9660"
/dev/vda4: LABEL="root" UUID="fb535add-9799-4a27-b8bc-e8259f39a767" BLOCK_SIZE="512" TYPE="xfs" PARTUUID="6264d520-3fb9-423f-8ab8-7a0a8e3d3562"
/dev/vda2: SEC_TYPE="msdos" UUID="7B77-95E7" BLOCK_SIZE="512" TYPE="vfat" PARTUUID="68b2905b-df3e-4fb3-80fa-49d1e773aa33"
/dev/vda3: LABEL="boot" UUID="5e75a2b9-1367-4cc8-bb38-4d6abc3964b8" BLOCK_SIZE="512" TYPE="xfs" PARTUUID="cb07c243-bc44-4717-853e-28852021225b"
/dev/vda1: PARTUUID="fac7f1fb-3e8d-4137-a512-961de09a5549"

[root@servera ~]# df 
Filesystem     1K-blocks    Used Available Use% Mounted on
devtmpfs          863980       0    863980   0% /dev
tmpfs             908696       0    908696   0% /dev/shm
tmpfs             363480    5120    358360   2% /run
/dev/vda4        9756652 3750508   6006144  39% /
/dev/vda3         506528  163796    342732  33% /boot
/dev/vda2         204580    7148    197432   4% /boot/efi
tmpfs             181736       0    181736   0% /run/user/1000
tmpfs             181736       0    181736   0% /run/user/0
현재 마운트 된 자원을 확인

[root@servera ~]# df -h
Filesystem      Size  Used Avail Use% Mounted on
devtmpfs        844M     0  844M   0% /dev
tmpfs           888M     0  888M   0% /dev/shm
tmpfs           355M  5.0M  350M   2% /run
/dev/vda4       9.4G  3.6G  5.8G  39% /
/dev/vda3       495M  160M  335M  33% /boot
/dev/vda2       200M  7.0M  193M   4% /boot/efi
tmpfs           178M     0  178M   0% /run/user/1000
tmpfs           178M     0  178M   0% /run/user/0
자원의 용량을 사람이 보기 좋게 확인

[root@servera ~]# df -hT
Filesystem     Type      Size  Used Avail Use% Mounted on
devtmpfs       devtmpfs  844M     0  844M   0% /dev
tmpfs          tmpfs     888M     0  888M   0% /dev/shm
tmpfs          tmpfs     355M  5.0M  350M   2% /run
/dev/vda4      xfs       9.4G  3.6G  5.8G  39% /
/dev/vda3      xfs       495M  160M  335M  33% /boot
/dev/vda2      vfat      200M  7.0M  193M   4% /boot/efi
tmpfs          tmpfs     178M     0  178M   0% /run/user/1000
tmpfs          tmpfs     178M     0  178M   0% /run/user/0
파일시스템 형식도 확인

[root@servera ~]# df -hT /
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/vda4      xfs   9.4G  3.6G  5.8G  39% /
특정한 마운트 포인트만 확인

[root@servera ~]# df -hT | grep -v tmp
Filesystem     Type      Size  Used Avail Use% Mounted on
/dev/vda4      xfs       9.4G  3.6G  5.8G  39% /
/dev/vda3      xfs       495M  160M  335M  33% /boot
/dev/vda2      vfat      200M  7.0M  193M   4% /boot/efi
임시 파일시스템을 제거

[219]
파티션 관리
관리자는 파티션 편집기 프로그램을 사용하여 파티션 생성, 파티션 삭제, 파티션 유형 변경 등 디스크의 파티 션을 변경할 수 있습니다. 
Red Hat Enterprise Linux 명령줄의 표준 파티션 편집기는 parted입니다. 
하지만 우리는 수업에서 그리고 현장에서 fdisk 입니다.
MBR 파티셔닝 체계 또는 GPT 파티셔닝 체계를 사용하는 스토리지와 함께 parted 파티션 편집기를 사용할 수 있습니다. parted 명령은 수정할 전체 스토리지 장치 또는 디스크를 나타내는 장치 이름을 첫 번째 인수로 사용하고 그 뒤에 하위 명령을 사용합니다. 다음 예에서는 print 하위 명령을 사용하여 /dev/vda 블록 장치(시스템 에서 감지한 첫 번째 '가상화된 I/O' 디스크)인 디스크의 파티션 테이블을 표시합니다.


아 그럼 왜 fdisk인가?
p
parted의 위험성 때문:
fdisk : 모든 작업을 메모리 상에서 작업한 다음 특정명령[w]실제 저장이 되는 방식
Welcome to fdisk (util-linux 2.37.4).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.
parted는 fdisk와 달리 명령을 내리는 순간 작업이 진행

이용가능한 파티션 프로그램
fdisk : 과거부터 이용하던 표준 파티셔닝 프로그램
cfdisk: fdisk의 TUI 버전
parted : RHEL 표준 파티셔닝 프로그램 [ansible쪽에서는 parted를 이용]
gdisk: parted가 들어오기전에 GPT 지원을 위해 사용하던 프로그램 현재는 이용하지 않음
sfdisk: 파티션 구조를 가지고 다른 파티션에 동일하게 적용

디스크 초기화를 위해서는 dd 명령을 다시 이용
fdisk 특징: 메모리상에서 작업된다.
그렇기 때문에 실수를 하더라도 안전하다. 단점: 자동화로 구성하기는 꽤 까다롭다.
예시:  echo -e "n\np\n1\n\n\nt\n8e\nw" | fdisk /dev/sdX

fdisk에서 사용되는 옵션
-l : List the partition tables * 
-v : Print version number of fdisk program and exit
-s : The size of the partition (in blocks) is printed on the standard output
-b sectorsize : Specify the sector size of the disk

예시:
# fdisk /dev/device : fdisk 프로그램으로 진입
# fdisk -l /dev/device : 현재 디바이스의 파티셔닝 상태 확인
 # fdisk -l : 전체 블록 디바이스의 파티셔닝 상태 확인

fdisk의 경우에는 이미 파티셔닝이 되어 있는 경우 진입하자 마자 경고가 뜬다.

여기서 부터 시작
[root@servera ~]# fdisk /dev/vdb

Welcome to fdisk (util-linux 2.37.4).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x16e59f00.


Command (m for help): m
기능들 중에 필요한 것만 빼도록 하겠습니다.
Help:

  DOS (MBR)
   a   toggle a bootable flag    부트 가능한 플래그로 변경 (부트 파티션 저장)
   b   edit nested BSD disklabel bsd 디스크 레이블편집
   c   toggle the dos compatibility flag 도스 호환 플래그로 변경

  Generic
   d   delete a partition 파티션 삭제
   F   list free unpartitioned space 
   l   list known partition types 알려진 파티션 목록 확인
   n   add a new partition 새로운 파티션 생성
   p   print the partition table 현재 진행중인 파티션 상태 확인
   t   change a partition type 파티션의 id를 변경 
   v   verify the partition table 파티션 테이블 확인
   i   print information about a partition

  Misc
   m   print this menu 도움말 출력
   u   change display/entry units 엔트리 단위 변경
   x   extra functionality (experts only) 추가 기능 표시

  Script
   I   load disk layout from sfdisk script file sfdisk dump 파일 입력
   O   dump disk layout to sfdisk script file sfdisk dump 파일 출력

  Save & Exit
   w   write table to disk and exit  실제 저장+빠져나가기
   q   quit without saving changes 빠져나가기

  Create a new label
   g   create a new empty GPT partition table  GPT 라벨링 변경
   G   create a new empty SGI (IRIX) partition table
   o   create a new empty DOS partition table MBR 라벨링 변경
   s   create a new empty Sun partition table

실제 사용하는것:

   g   create a new empty GPT partition table  GPT 라벨링 변경
   o   create a new empty DOS partition table MBR 라벨링 변경
   n   add a new partition 새로운 파티션 생성
   p   print the partition table 현재 진행중인 파티션 상태 확인
   t   change a partition type 파티션의 id를 변경 
   d   delete a partition 파티션 삭제


   w   write table to disk and exit  실제 저장+빠져나가기
   q   quit without saving changes 빠져나가기

참고로 구버전에서는 fdisk에서 gpt 라벨링 기능이 없음.


Command (m for help): o
Created a new DOS disklabel with disk identifier 0xdc5eea69.
mbr 영역으로 파티셔닝 하기 위한 준비

Command (m for help): p

Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos  << 
Disk identifier: 0xdc5eea69

파티셔닝 작업을 위해서 진입

전체 용량 사용으로 작업

Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free) : 주파티션
   e   extended (container for logical partitions) : 확장파티션
Select (default p): enter

Using default response p.
Partition number (1-4, default 1): enter

First sector (2048-10485759, default 2048): enter : 시작 섹터를 지정
DB ->  20480 이를 제외한 경우에는 그냥 기본값인 2048을 사용하면 된다.
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-10485759, default 10485759): enter
용량을 지정

Created a new partition 1 of type 'Linux' and of size 5 GiB.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xdc5eea69

Device     Boot Start      End  Sectors Size Id Type
/dev/vdb1        2048 10485759 10483712   5G 83 Linux
방금 생성한 하나의 파티션으로 5G 생성


Command (m for help): q  : 빠져나가기

[root@servera ~]# fdisk /dev/vdb

Welcome to fdisk (util-linux 2.37.4).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x1d3647ea.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Command (m for help): 
저장을 하지 않으면 내용을 가지고 있음.

용량을 지정 1G 4G
Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (1-4, default 1): 1
First sector (2048-10485759, default 2048): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-10485759, default 10485759): +1G
용량을 지정   +1G 
Created a new partition 1 of type 'Linux' and of size 1 GiB.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot Start     End Sectors Size Id Type
/dev/vdb1        2048 2099199 2097152   1G 83 Linux

Command (m for help): n
Partition type
   p   primary (1 primary, 0 extended, 3 free)
   e   extended (container for logical partitions)
Select (default p): 

Using default response p.
Partition number (2-4, default 2): 
First sector (2099200-10485759, default 2099200): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2099200-10485759, default 10485759): 

Created a new partition 2 of type 'Linux' and of size 4 GiB.
남은 용량에서 전체 파티션을 구성하는 경우에는 enter을 연속하여 입력하면 편하게 구성 가능

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdb1          2048  2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 10485759 8386560   4G 83 Linux

Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

[root@servera ~]# fdisk -l /dev/vdb
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdb1          2048  2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 10485759 8386560   4G 83 Linux

sfdisk 사용방법
1 같은 서버에서 구성하기
[root@servera ~]# sfdisk -d /dev/vdb
label: dos
label-id: 0x1d3647ea
device: /dev/vdb
unit: sectors
sector-size: 512

/dev/vdb1 : start=        2048, size=     2097152, type=83
/dev/vdb2 : start=     2099200, size=     8386560, type=83

[root@servera ~]# sfdisk -d /dev/vdb | sfdisk /dev/vdc
Checking that no-one is using this disk right now ... OK

Disk /dev/vdc: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Created a new DOS disklabel with disk identifier 0x1d3647ea.
/dev/vdc1: Created a new partition 1 of type 'Linux' and of size 1 GiB.
/dev/vdc2: Created a new partition 2 of type 'Linux' and of size 4 GiB.
/dev/vdc3: Done.

New situation:
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdc1          2048  2099199 2097152   1G 83 Linux
/dev/vdc2       2099200 10485759 8386560   4G 83 Linux

The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

[root@servera ~]# fdisk -l /dev/vdc
Disk /dev/vdc: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdc1          2048  2099199 2097152   1G 83 Linux
/dev/vdc2       2099200 10485759 8386560   4G 83 Linux
vdc에 동일한 파티션 정보를 입력한것을 확인

2
다른서버에 저장하기
[root@servera ~]# sfdisk -d /dev/vdb  > part.info
표준 출력 리다이렉션을 통해 정보 저장
[root@servera ~]# cat part.info 
label: dos
label-id: 0x1d3647ea
device: /dev/vdb
unit: sectors
sector-size: 512

/dev/vdb1 : start=        2048, size=     2097152, type=83
/dev/vdb2 : start=     2099200, size=     8386560, type=83

[root@servera ~]# sfdisk /dev/vdd < part.info 
Checking that no-one is using this disk right now ... OK

Disk /dev/vdd: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes

>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Script header accepted.
>>> Created a new DOS disklabel with disk identifier 0x1d3647ea.
/dev/vdd1: Created a new partition 1 of type 'Linux' and of size 1 GiB.
/dev/vdd2: Created a new partition 2 of type 'Linux' and of size 4 GiB.
/dev/vdd3: Done.

New situation:
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdd1          2048  2099199 2097152   1G 83 Linux
/dev/vdd2       2099200 10485759 8386560   4G 83 Linux

The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

[root@servera ~]# fdisk -l /dev/vdd
Disk /dev/vdd: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdd1          2048  2099199 2097152   1G 83 Linux
/dev/vdd2       2099200 10485759 8386560   4G 83 Linux
위 두가지 방법을 통해서 같은 디스크, 같은 서버에서 파티셔닝 작업을 할 때 이용 


vdc vdd 삭제
1
[root@servera ~]# dd if=/dev/zero of=/dev/vdc bs=1M count=100 

2
[root@servera ~]# time dd if=/dev/zero of=/dev/vdd
512byte씩 덮어쓰기가 이루어지기 때문에 용량이 크면 클수록 오래 걸린다.
dd: writing to '/dev/vdd': No space left on device
10485761+0 records in
10485760+0 records out
5368709120 bytes (5.4 GB, 5.0 GiB) copied, 92.6355 s, 58.0 MB/s

real    1m32.637s
user    0m1.753s
sys     0m16.859s

[root@servera ~]# fdisk /dev/vdb

Welcome to fdisk (util-linux 2.37.4).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.


Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdb1          2048  2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 10485759 8386560   4G 83 Linux

Command (m for help): d  파티션 삭제
Partition number (1,2, default 2): 1  두개 이상 있을때는 선택

Partition 1 has been deleted.

Command (m for help): d  남은 파티션이 하나인 경우에는 바로 삭제
Selected partition 2
Partition 2 has been deleted.

확장 파티션 생성하기
Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (1-4, default 1): 
First sector (2048-10485759, default 2048): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-10485759, default 10485759): +1G

Created a new partition 1 of type 'Linux' and of size 1 GiB.

Command (m for help): n
Partition type
   p   primary (1 primary, 0 extended, 3 free)
   e   extended (container for logical partitions)
Select (default p): e
Partition number (2-4, default 2): 
First sector (2099200-10485759, default 2099200):  enter
확장파티션은 그 자체로 사용되는것이 아니라 공간을 지정하는 것이기 때문에 논리 파티션에서 사용할 용량을 지정
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2099200-10485759, default 10485759): 

Created a new partition 2 of type 'Extended' and of size 4 GiB.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdb1          2048  2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 10485759 8386560   4G  5 Extended
두개의 파티션으로 이미 모든 용량을 사용

Command (m for help): n
All space for primary partitions is in use.
Adding logical partition 5
First sector (2101248-10485759, default 2101248): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2101248-10485759, default 10485759): +1G

Created a new partition 5 of type 'Linux' and of size 1 GiB.

Command (m for help): n
All space for primary partitions is in use.
Adding logical partition 6
First sector (4200448-10485759, default 4200448): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (4200448-10485759, default 10485759): +1G

Created a new partition 6 of type 'Linux' and of size 1 GiB.
두개의 1G 디스크를 논리 파티션으로 생성
Command (m for help): n
All space for primary partitions is in use.
Adding logical partition 7
First sector (6299648-10485759, default 6299648): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (6299648-10485759, default 10485759): 

Created a new partition 7 of type 'Linux' and of size 2 GiB.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdb1          2048  2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 10485759 8386560   4G  5 Extended  < 실제 파티션 용량에서는 제외
/dev/vdb5       2101248  4198399 2097152   1G 83 Linux
/dev/vdb6       4200448  6297599 2097152   1G 83 Linux
/dev/vdb7       6299648 10485759 4186112   2G 83 Linux

파티션 타입 변경
리눅스는 vfs라는것을 지원

Command (m for help): t
Partition number (1,2,5-7, default 7): 

Command (m for help): t
Partition number (1,2,5-7, default 7): 7
Hex code or alias (type L to list all): L

00 Empty            24 NEC DOS          81 Minix / old Lin  bf Solaris        
01 FAT12            27 Hidden NTFS Win  82 Linux swap / So  c1 DRDOS/sec (FAT-
02 XENIX root       39 Plan 9           83 Linux            c4 DRDOS/sec (FAT-
03 XENIX usr        3c PartitionMagic   84 OS/2 hidden or   c6 DRDOS/sec (FAT-
04 FAT16 <32M       40 Venix 80286      85 Linux extended   c7 Syrinx         
05 Extended         41 PPC PReP Boot    86 NTFS volume set  da Non-FS data    
06 FAT16            42 SFS              87 NTFS volume set  db CP/M / CTOS / .
07 HPFS/NTFS/exFAT  4d QNX4.x           88 Linux plaintext  de Dell Utility   
08 AIX              4e QNX4.x 2nd part  8e Linux LVM        df BootIt         
09 AIX bootable     4f QNX4.x 3rd part  93 Amoeba           e1 DOS access     
0a OS/2 Boot Manag  50 OnTrack DM       94 Amoeba BBT       e3 DOS R/O        
0b W95 FAT32        51 OnTrack DM6 Aux  9f BSD/OS           e4 SpeedStor      
0c W95 FAT32 (LBA)  52 CP/M             a0 IBM Thinkpad hi  ea Linux extended 
0e W95 FAT16 (LBA)  53 OnTrack DM6 Aux  a5 FreeBSD          eb BeOS fs        
0f W95 Ext'd (LBA)  54 OnTrackDM6       a6 OpenBSD          ee GPT            
10 OPUS             55 EZ-Drive         a7 NeXTSTEP         ef EFI (FAT-12/16/
11 Hidden FAT12     56 Golden Bow       a8 Darwin UFS       f0 Linux/PA-RISC b
12 Compaq diagnost  5c Priam Edisk      a9 NetBSD           f1 SpeedStor      
14 Hidden FAT16 <3  61 SpeedStor        ab Darwin boot      f4 SpeedStor      
16 Hidden FAT16     63 GNU HURD or Sys  af HFS / HFS+       f2 DOS secondary  
17 Hidden HPFS/NTF  64 Novell Netware   b7 BSDI fs          fb VMware VMFS    
18 AST SmartSleep   65 Novell Netware   b8 BSDI swap        fc VMware VMKCORE 
1b Hidden W95 FAT3  70 DiskSecure Mult  bb Boot Wizard hid  fd Linux raid auto
1c Hidden W95 FAT3  75 PC/IX            bc Acronis FAT32 L  fe LANstep        
1e Hidden W95 FAT1  80 Old Minix        be Solaris boot     ff BBT            

Aliases:
   linux          - 83   *
   swap           - 82  * 
   extended       - 05
   uefi           - EF
   raid           - FD
   lvm            - 8E *
   linuxex        - 85
Hex code or alias (type L to list all):  82   < swap 파티션으로 변경

Changed type of partition 'Empty' to 'Linux swap / Solaris'.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start      End Sectors Size Id Type
/dev/vdb1          2048  2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 10485759 8386560   4G  5 Extended
/dev/vdb5       2101248  4198399 2097152   1G 83 Linux
/dev/vdb6       4200448  6297599 2097152   1G 83 Linux
/dev/vdb7       6299648 10485759 4186112   2G 82 Linux swap / Solaris  < 변경된 내용을 확인

일단 기존 생성된 파일시스템을 모두 삭제
두개만 남김.

Command (m for help): d
Partition number (1,2,5, default 5): 2   확장 파티션을 삭제하게 되면 내부의 논리 파티션도 모두 삭제 되기 때문에 조심해서 삭제 !

Partition 2 has been deleted.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot Start     End Sectors Size Id Type
/dev/vdb1        2048 2099199 2097152   1G 83 Linux

Command (m for help): d
Selected partition 1
Partition 1 has been deleted.

파일 시스템 생성에 필요한 구조
/dev/vdb1 1G
/dev/vdb2 1G

Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (1-4, default 1): 1
First sector (2048-10485759, default 2048): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-10485759, default 10485759): +1G

Created a new partition 1 of type 'Linux' and of size 1 GiB.

Command (m for help): n
Partition type
   p   primary (1 primary, 0 extended, 3 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (2-4, default 2): 
First sector (2099200-10485759, default 2099200): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2099200-10485759, default 10485759): +1G

Created a new partition 2 of type 'Linux' and of size 1 GiB.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start     End Sectors Size Id Type
/dev/vdb1          2048 2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 4196351 2097152   1G 83 Linux

Command (m for help): w

The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

작업 내역 확인
[root@servera ~]# fdisk -l /dev/vdb
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start     End Sectors Size Id Type
/dev/vdb1          2048 2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 4196351 2097152   1G 83 Linux
[root@servera ~]# lsblk 
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sr0     11:0    1  558K  0 rom  
vda    252:0    0   10G  0 disk 
├─vda1 252:1    0    1M  0 part 
├─vda2 252:2    0  200M  0 part /boot/efi
├─vda3 252:3    0  500M  0 part /boot
└─vda4 252:4    0  9.3G  0 part /
vdb    252:16   0    5G  0 disk 
├─vdb1 252:17   0    1G  0 part 
└─vdb2 252:18   0    1G  0 part 
vdc    252:32   0    5G  0 disk 
vdd    252:48   0    5G  0 disk 


파일시스템 : 자료구조
파일과 디렉토리들을 저장 하기 위한 파일구조

파일시스템의 종류
---------------------------------------------------------------------------
ext2     (mkfs.ext2)     리눅스 이전 버전에서 사용하는 파일시스템
ext3     (mkfs.ext3)     현재 CentOS 5.X 사용하는 파일시스템
ext4     (mkfs.ext4)     CentOS 6.X 사용하는 파일시스템
xfs      (mkfs.xfs)      고성능 저널링 파일 시스템(eXtended File System)
msdos    mfs.msdos)      MS-DOS filesystem
---------------------------------------------------------------------------

VFS에 대한 추가
리눅스에서 파일의 개념은 파일을 바이트의 단순한 연속으로 간주한다. 리눅스는 모든 입출력 디바이스들을 파일로 취급하여 심지어 NIC, Disk, Keyborad, printer도 파일로 취급한다. 파일시스템은 모든 입출력을 제어하는 Device Driver와 연결되어 있고 시스템 호출을 통하여 사용자로부터 데이터를 받아 처리한다.

리눅스는 VFS(Virtual File System)을 통하여 ext2, jfs2, proc와 같은 다양한 파일시스템을 지원하고 있다.

가상 파일시스템 구조(Virtual File System)

VFS는 모든 파일 시스템이 필요로 하는 일련의 function을 정의하고 있다. 이러한 인터페이스는 세 종류 대상과 관련된 일련의 동작으로 구성된다. 즉 파일시스템, 아이노드, 열린 파일이다.

VFS는 커널에서 지원하는 파일시스템 타입을 알고 있다. 이는 커널 구성 동안에 만들어지는 테이블을 사용한다. 이 테이블 내의 각 엔트리가 하나의 파일 시스템 타입을 정의한다. 즉 파일시스템 타입의 이름과 function에 대한 포인터이다.

어떤 파일 시스템이 마운트 될 때 그에 맞는 마운트 function이 불러 들여진다. 이 function이 디스크의 슈퍼블록을 읽는 것을 담당하며, 내부 변수를 초기화하고 VFS에 마운트된 파일시스템 descriptor를 돌려준다.

파일시스템이 마운트 된 후에 VFS function은 물리적인 파일시스템 루틴을 접근하는데 이 descriptor를 사용한다. 마운트된 파일시스템 descriptor는 모든 파일시스템 타입에 공통된 몇 가지의 데이터를 포함 한다. 물리적 파일시스템 커널코드에 의해 제공되는 function에 대한 포인터와 물리적인 파일 시스템 코드에 필요한 고유정보이다. 파일시스템 descriptor에 포함된 function 포인터는 VFS가 파일 시스템 내부 루틴을 접근하는 것을 허용한다.

두 가지의 다른 descriptor가 VFS에 의해 사용된다. 아이노드 descriptor와 열린 파일 descriptor이다. 각 descriptor는 사용 중인 파일에 관련된 정보와 물리적인 파일시스템 코드에 의해 제공되는 일련의 동작을 갖고 있다. 아이노드 descriptor는 어떤 파일에 대해서도 사용되는 function에 대한 포인터를 포함하는데 비해 (e.g. create, unlink) 파일 descriptor는 열린 파일에 대해서만 작동하는 function에 대한 포인터를 갖는다. (e.g.read,write)

파일시스템 생성
mkfs
1 mkfs -t ext4
2 mkfs.ext4
mkfs.ext4 /dev/devicename_num

[root@servera ~]# lsblk /dev/vdb
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
vdb    252:16   0   5G  0 disk 
├─vdb1 252:17   0   1G  0 part 
└─vdb2 252:18   0   1G  0 part 
파일시스템으로 생성 가능한 파티션은 두개
/dev/vdb1 ext4
/dev/vdb xfs

[root@servera ~]# mkfs.
mkfs.cramfs  mkfs.ext3    mkfs.fat     mkfs.msdos   mkfs.xfs
mkfs.ext2    mkfs.ext4    mkfs.minix   mkfs.vfat    





[root@servera ~]# mkfs.ext4 /dev/vdb1 
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 262144 4k blocks and 65536 inodes
Filesystem UUID: dd0d4df3-08d5-49e2-bf0b-07af71c7ea02
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done

[root@servera ~]# mkfs.ext4 /dev/vdb1  기존의 FS을 버리고 새로 생성한다면 확인
mke2fs 1.46.5 (30-Dec-2021)
/dev/vdb1 contains a ext4 file system
        created on Wed Jul 24 14:28:29 2024
Proceed anyway? (y,N) y
Creating filesystem with 262144 4k blocks and 65536 inodes
Filesystem UUID: 177155e8-d4b6-4254-8322-be21915fcceb
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done

[root@servera ~]# mkfs.xfs /dev/vdb2
meta-data=/dev/vdb2              isize=512    agcount=4, agsize=65536 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=1 inobtcount=1
data     =                       bsize=4096   blocks=262144, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@servera ~]# mkfs.xfs /dev/vdb2
mkfs.xfs: /dev/vdb2 appears to contain an existing filesystem (xfs).
mkfs.xfs: Use the -f option to force overwrite.
[root@servera ~]# mkfs.xfs -f /dev/vdb2
meta-data=/dev/vdb2              isize=512    agcount=4, agsize=65536 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=1 inobtcount=1
data     =                       bsize=4096   blocks=262144, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

ext4 : 작은 파일들 저장에 장점
xfs   : 대용량 파일에 장점

[root@servera ~]# lsblk --fs /dev/vdb
NAME   FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
vdb                                                                           
├─vdb1 ext4   1.0         177155e8-d4b6-4254-8322-be21915fcceb                
└─vdb2 xfs                36241924-9a60-4afe-89b2-141fce559552                

파일 시스템을 사용하는 방법
자원 -  디스크라는 자원을 이용하는 방법
마운트를 통해서 디스크와 디렉토리를 연결 
컴퓨터에서 Mount란 어떠한 것을 Available 한 상태로 준비하는 것을 말한다.
좀 더 추상적인 개념이지, 딱 하나로 이어지는 구체적인 단어는 아니다.


마운트 ->
디스크-> 파티션 -> 파일시스템 —---[마운트]---------DIR
마운트 : 파일시스템을 이용하기 위하여 디렉토리와 연결해주는 작업

[root@servera ~]# mkdir /stage1
[root@servera ~]# df /stage1
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/vda4        9756652 3750536   6006116  39% /

마운트 명령을 사용하는 형식
# mount [파일시스템구성이완료된디스크] [마운트포인트,디렉토리]
 # mount [/dev/[PATH]] [마운트포인트,디렉토리]
# mount [UUID] [마운트포인트,디렉토리]

[root@servera ~]# mount /dev/vdb1 /stage1
[root@servera ~]# df /stage1
Filesystem     1K-blocks  Used Available Use% Mounted on
/dev/vdb1         996780    24    927944   1% /stage1

[root@servera ~]# cd /stage1/
[root@servera stage1]# ls
lost+found
[root@servera stage1]# touch file1
[root@servera stage1]# echo '/stage1 /dev/vdb1 mount' > info

[root@servera stage1]# cd   마운트 해제를 할때 자원에 접속해 있으면 umount가 불가능
umount : 마운트된 자원을 해제
[root@servera ~]# umount /stage1
[root@servera ~]# df 
Filesystem     1K-blocks    Used Available Use% Mounted on
devtmpfs          863980       0    863980   0% /dev
tmpfs             908696       0    908696   0% /dev/shm
tmpfs             363480    5140    358340   2% /run
/dev/vda4        9756652 3750536   6006116  39% /
/dev/vda3         506528  163796    342732  33% /boot
/dev/vda2         204580    7148    197432   4% /boot/efi
tmpfs             181736       0    181736   0% /run/user/1000
tmpfs             181736       0    181736   0% /run/user/0
[root@servera ~]# cd  /stage1
[root@servera stage1]# df .
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/vda4        9756652 3750536   6006116  39% /
df . 은 현재 디렉토리의 마운트 연결 정보를 확인
[root@servera stage1]# df /stage1
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/vda4        9756652 3750536   6006116  39% /
[root@servera stage1]# pwd
/stage1
[root@servera stage1]# ls
[root@servera stage1]#
umount 이후 확인된 데이터는 기존의 /dev/vdb1에 존재하던  info, file1이 없다.

신입 X 경력? X
확인안해서 나오는 실수.

마운트 방법 ..
1 전통적인 방법
# mount [device-name] [mountpoint]
2 UUID 작성 : 범용 고유 식별자

[root@servera ~]# lsblk --fs /dev/vdb1
NAME FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
vdb1 ext4   1.0         177155e8-d4b6-4254-8322-be21915fcceb                
UUID는 생성할 때 마다 다르기 때문에 본인의 머신을 확인 한뒤 복사 하시면 됩니다.
[root@servera ~]# mount UUID=177155e8-d4b6-4254-8322-be21915fcceb /stage1
[root@servera ~]# df  /stage1
Filesystem     1K-blocks  Used Available Use% Mounted on
/dev/vdb1         996780    28    927940   1% /stage1

권장 사항이 UUID인 이유:
임시 마운트에서는 뭘 사용하든 상관 없음.
하지만 fstab은 부팅시에 해당 파일을 보고 마운트 포인트 + 장치명을 연결
장치명으로 되어 있는 경우
vmware, 가상화 장치의 경우에는 scsi 어댑터, 채널 ID가 변경되면서 디스크의 이름이 변경되는 경우가 많음.
예시:
system disk : /dev/vda4
재부팅 -> 커널? 부팅 불가

UUID 가 아니라 장치명 자체로 사용을 하게 되면 
vda가 vdb, vdc가 될 수 있음. 부팅이 불가
UUID로 작성해두게 되면 장치명에 대한것을 systemd가 직접 구성하기에 부팅이 안되는 경우가 없으며 디스크데이터도 100% 마운트 가능

[root@servera ~]# mkdir /stage2
[root@servera ~]# mount /dev/vdb2 /stage2
[root@servera ~]# df 
Filesystem     1K-blocks    Used Available Use% Mounted on
devtmpfs          863980       0    863980   0% /dev
tmpfs             908696       0    908696   0% /dev/shm
tmpfs             363480    5140    358340   2% /run
/dev/vda4        9756652 3750556   6006096  39% /
/dev/vda3         506528  163796    342732  33% /boot
/dev/vda2         204580    7148    197432   4% /boot/efi
tmpfs             181736       0    181736   0% /run/user/1000
tmpfs             181736       0    181736   0% /run/user/0
/dev/vdb1         996780      28    927940   1% /stage1
/dev/vdb2        1038336   40292    998044   4% /stage2

UUID가 짧은 것도 있다.
GPT로 작업
[root@servera ~]# fdisk /dev/vdc

Welcome to fdisk (util-linux 2.37.4).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0x02d498e0.

Command (m for help): g
Created a new GPT disklabel (GUID: 848A7886-541A-4448-9ABA-774E143377C3).

Command (m for help): n
Partition number (1-128, default 1): 1
First sector (2048-10485726, default 2048): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-10485726, default 10485726): +1G

Created a new partition 1 of type 'Linux filesystem' and of size 1 GiB.

Command (m for help): 
Command (m for help): t
Selected partition 1
Partition type or alias (type L to list all): L
  1 EFI System                     C12A7328-F81F-11D2-BA4B-00A0C93EC93B
  2 MBR partition scheme           024DEE41-33E7-11D3-9D69-0008C781F39F
  3 Intel Fast Flash               D3BFE2DE-3DAF-11DF-BA40-E3A556D89593
  4 BIOS boot                      21686148-6449-6E6F-744E-656564454649
  5 Sony boot partition            F4019732-066E-4E12-8273-346C5641494F
  6 Lenovo boot partition          BFBFAFE7-A34F-448A-9A5B-6213EB736C22
  7 PowerPC PReP boot              9E1A2D38-C612-4316-AA26-8B49521E5A8B
  8 ONIE boot                      7412F7D5-A156-4B13-81DC-867174929325
  9 ONIE config                    D4E6E2CD-4469-46F3-B5CB-1BFF57AFC149
 10 Microsoft reserved             E3C9E316-0B5C-4DB8-817D-F92DF00215AE
 11 Microsoft basic data           EBD0A0A2-B9E5-4433-87C0-68B6B72699C7
 12 Microsoft LDM metadata         5808C8AA-7E8F-42E0-85D2-E1E90434CFB3
 13 Microsoft LDM data             AF9B60A0-1431-4F62-BC68-3311714A69AD
 14 Windows recovery environment   DE94BBA4-06D1-4D40-A16A-BFD50179D6AC
 15 IBM General Parallel Fs        37AFFC90-EF7D-4E96-91C3-2D7AE055B174
 16 Microsoft Storage Spaces       E75CAF8F-F680-4CEE-AFA3-B001E56EFC2D
 17 HP-UX data                     75894C1E-3AEB-11D3-B7C1-7B03A0000000
 18 HP-UX service                  E2A1E728-32E3-11D6-A682-7B03A0000000
 19 Linux swap                     0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
 20 Linux filesystem               0FC63DAF-8483-4772-8E79-3D69D8477DE4
 21 Linux server data              3B8F8425-20E0-4F3B-907F-1A25A76F98E8
 22 Linux root (x86)               44479540-F297-41B2-9AF7-D131D5F0458A
 23 Linux root (x86-64)            4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709
 24 Linux root (ARM)               69DAD710-2CE4-4E3C-B16C-21A1D49ABED3
 25 Linux root (ARM-64)            B921B045-1DF0-41C3-AF44-4C6F280D3FAE
 26 Linux root (IA-64)             993D8D3D-F80E-4225-855A-9DAF8ED7EA97
 27 Linux reserved                 8DA63339-0007-60C0-C436-083AC8230908
 28 Linux home                     933AC7E1-2EB4-4F13-B844-0E14E2AEF915
 29 Linux RAID                     A19D880F-05FC-4D3B-A006-743F0F84911E
 30 Linux LVM                      E6D6D379-F507-44C2-A23C-238F2A3DF928
 31 Linux variable data            4D21B016-B534-45C2-A9FB-5C16E091FD2D
 32 Linux temporary data           7EC6F557-3BC5-4ACA-B293-16EF5DF639D1
 33 Linux /usr (x86)               75250D76-8CC6-458E-BD66-BD47CC81A812
 34 Linux /usr (x86-64)            8484680C-9521-48C6-9C11-B0720656F69E
 35 Linux /usr (ARM)               7D0359A3-02B3-4F0A-865C-654403E70625
 36 Linux /usr (ARM-64)            B0E01050-EE5F-4390-949A-9101B17104E9
 37 Linux /usr (IA-64)             4301D2A6-4E3B-4B2A-BB94-9E0B2C4225EA
 38 Linux root verity (x86)        D13C5D3B-B5D1-422A-B29F-9454FDC89D76
 39 Linux root verity (x86-64)     2C7357ED-EBD2-46D9-AEC1-23D437EC2BF5
 40 Linux root verity (ARM)        7386CDF2-203C-47A9-A498-F2ECCE45A2D6
 41 Linux root verity (ARM-64)     DF3300CE-D69F-4C92-978C-9BFB0F38D820
 42 Linux root verity (IA-64)      86ED10D5-B607-45BB-8957-D350F23D0571
 43 Linux /usr verity (x86)        8F461B0D-14EE-4E81-9AA9-049B6FB97ABD
 44 Linux /usr verity (x86-64)     77FF5F63-E7B6-4633-ACF4-1565B864C0E6
 45 Linux /usr verity (ARM)        C215D751-7BCD-4649-BE90-6627490A4C05
 46 Linux /usr verity (ARM-64)     6E11A4E7-FBCA-4DED-B9E9-E1A512BB664E
 47 Linux /usr verity (IA-64)      6A491E03-3BE7-4545-8E38-83320E0EA880
 48 Linux extended boot            BC13C2FF-59E6-4262-A352-B275FD6F7172
 49 Linux user's home              773f91ef-66d4-49b5-bd83-d683bf40ad16
 50 FreeBSD data                   516E7CB4-6ECF-11D6-8FF8-00022D09712B
 51 FreeBSD boot                   83BD6B9D-7F41-11DC-BE0B-001560B84F0F
 52 FreeBSD swap                   516E7CB5-6ECF-11D6-8FF8-00022D09712B
 53 FreeBSD UFS                    516E7CB6-6ECF-11D6-8FF8-00022D09712B
 54 FreeBSD ZFS                    516E7CBA-6ECF-11D6-8FF8-00022D09712B
 55 FreeBSD Vinum                  516E7CB8-6ECF-11D6-8FF8-00022D09712B
 56 Apple HFS/HFS+                 48465300-0000-11AA-AA11-00306543ECAC
 57 Apple APFS                     7C3457EF-0000-11AA-AA11-00306543ECAC
 58 Apple UFS                      55465300-0000-11AA-AA11-00306543ECAC
 59 Apple RAID                     52414944-0000-11AA-AA11-00306543ECAC
 60 Apple RAID offline             52414944-5F4F-11AA-AA11-00306543ECAC
 61 Apple boot                     426F6F74-0000-11AA-AA11-00306543ECAC
 62 Apple label                    4C616265-6C00-11AA-AA11-00306543ECAC
 63 Apple TV recovery              5265636F-7665-11AA-AA11-00306543ECAC
 64 Apple Core storage             53746F72-6167-11AA-AA11-00306543ECAC
 65 Solaris boot                   6A82CB45-1DD2-11B2-99A6-080020736631
 66 Solaris root                   6A85CF4D-1DD2-11B2-99A6-080020736631
 67 Solaris /usr & Apple ZFS       6A898CC3-1DD2-11B2-99A6-080020736631
 68 Solaris swap                   6A87C46F-1DD2-11B2-99A6-080020736631
 69 Solaris backup                 6A8B642B-1DD2-11B2-99A6-080020736631
 70 Solaris /var                   6A8EF2E9-1DD2-11B2-99A6-080020736631
 71 Solaris /home                  6A90BA39-1DD2-11B2-99A6-080020736631
 72 Solaris alternate sector       6A9283A5-1DD2-11B2-99A6-080020736631
 73 Solaris reserved 1             6A945A3B-1DD2-11B2-99A6-080020736631
 74 Solaris reserved 2             6A9630D1-1DD2-11B2-99A6-080020736631
 75 Solaris reserved 3             6A980767-1DD2-11B2-99A6-080020736631
 76 Solaris reserved 4             6A96237F-1DD2-11B2-99A6-080020736631
 77 Solaris reserved 5             6A8D2AC7-1DD2-11B2-99A6-080020736631
 78 NetBSD swap                    49F48D32-B10E-11DC-B99B-0019D1879648
 79 NetBSD FFS                     49F48D5A-B10E-11DC-B99B-0019D1879648
 80 NetBSD LFS                     49F48D82-B10E-11DC-B99B-0019D1879648
 81 NetBSD concatenated            2DB519C4-B10E-11DC-B99B-0019D1879648
 82 NetBSD encrypted               2DB519EC-B10E-11DC-B99B-0019D1879648
 83 NetBSD RAID                    49F48DAA-B10E-11DC-B99B-0019D1879648
 84 ChromeOS kernel                FE3A2A5D-4F32-41A7-B725-ACCC3285A309
 85 ChromeOS root fs               3CB8E202-3B7E-47DD-8A3C-7FF2A13CFCEC
 86 ChromeOS reserved              2E0A753D-9E48-43B0-8337-B15192CB1B5E
 87 MidnightBSD data               85D5E45A-237C-11E1-B4B3-E89A8F7FC3A7
 88 MidnightBSD boot               85D5E45E-237C-11E1-B4B3-E89A8F7FC3A7
 89 MidnightBSD swap               85D5E45B-237C-11E1-B4B3-E89A8F7FC3A7
 90 MidnightBSD UFS                0394EF8B-237E-11E1-B4B3-E89A8F7FC3A7
 91 MidnightBSD ZFS                85D5E45D-237C-11E1-B4B3-E89A8F7FC3A7
 92 MidnightBSD Vinum              85D5E45C-237C-11E1-B4B3-E89A8F7FC3A7
 93 Ceph Journal                   45B0969E-9B03-4F30-B4C6-B4B80CEFF106
 94 Ceph Encrypted Journal         45B0969E-9B03-4F30-B4C6-5EC00CEFF106
 95 Ceph OSD                       4FBD7E29-9D25-41B8-AFD0-062C0CEFF05D
 96 Ceph crypt OSD                 4FBD7E29-9D25-41B8-AFD0-5EC00CEFF05D
 97 Ceph disk in creation          89C57F98-2FE5-4DC0-89C1-F3AD0CEFF2BE
 98 Ceph crypt disk in creation    89C57F98-2FE5-4DC0-89C1-5EC00CEFF2BE
 99 VMware VMFS                    AA31E02A-400F-11DB-9590-000C2911D1B8
100 VMware Diagnostic              9D275380-40AD-11DB-BF97-000C2911D1B8
101 VMware Virtual SAN             381CFCCC-7288-11E0-92EE-000C2911D0B2
102 VMware Virsto                  77719A0C-A4A0-11E3-A47E-000C29745A24
103 VMware Reserved                9198EFFC-31C0-11DB-8F78-000C2911D1B8
104 OpenBSD data                   824CC7A0-36A8-11E3-890A-952519AD3F61
105 QNX6 file system               CEF5A9AD-73BC-4601-89F3-CDEEEEE321A1
106 Plan 9 partition               C91818F9-8025-47AF-89D2-F030D7000C2C
107 HiFive Unleashed FSBL          5B193300-FC78-40CD-8002-E86C45580B47
108 HiFive Unleashed BBL           2E54B353-1271-4842-806F-E436D6AF6985

Aliases:
   linux          - 0FC63DAF-8483-4772-8E79-3D69D8477DE4
   swap           - 0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
   home           - 933AC7E1-2EB4-4F13-B844-0E14E2AEF915
   uefi           - C12A7328-F81F-11D2-BA4B-00A0C93EC93B
   raid           - A19D880F-05FC-4D3B-A006-743F0F84911E
   lvm            - E6D6D379-F507-44C2-A23C-238F2A3DF928
q
Partition type or alias (type L to list all): 0657FD6D-A4AB-43C4-84E5-0933C84B4F4F
Changed type of partition 'Linux filesystem' to 'Linux swap'.

Command (m for help): t
Selected partition 1
Partition type or alias (type L to list all): swap
Changed type of partition 'Linux swap' to 'Linux swap'.

Command (m for help): t
Selected partition 1
Partition type or alias (type L to list all): linux
Changed type of partition 'Linux swap' to 'Linux filesystem'.

Command (m for help): 
Command (m for help): p
Disk /dev/vdc: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: gpt
Disk identifier: 848A7886-541A-4448-9ABA-774E143377C3

Device     Start     End Sectors Size Type
/dev/vdc1   2048 2099199 2097152   1G Linux filesystem

Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

[root@servera ~]# 

[root@servera ~]# mkfs.vfat /dev/vdc1
mkfs.fat 4.2 (2021-01-31)
[root@servera ~]# lsblk --fs /dev/vdc1
NAME FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
vdc1 vfat   FAT32       D833-2F43         

위에서 작성한 마운트의 경우에는 임시 마운트

사용중인 파일 시스템 마운트 해제
[root@servera stage1]# pwd
/stage1
[root@servera stage1]# umount /stage1
umount: /stage1: target is busy.
[root@servera stage1]# df /stage1
Filesystem     1K-blocks  Used Available Use% Mounted on
/dev/vdb1         996780    28    927940   1% /stage1
[root@servera stage1]# cd 
[root@servera ~]# umount /stage1
[root@servera ~]# df /stage1
Filesystem     1K-blocks    Used Available Use% Mounted on
/dev/vda4        9756652 3750536   6006116  39% /
본인 계정이 접속된 상황이었기 때문에 다른곳으로 이동만 하면 그만

[student@servera stage2]$ df .
Filesystem     1K-blocks  Used Available Use% Mounted on
/dev/vdb2        1038336 40292    998044   4% /stage2
타 계정으로 /stage2 에 접근
[root@servera ~]# who
root     pts/0        2024-07-24 10:00 (172.25.250.9)
student  pts/1        2024-07-23 17:25 (172.25.250.9)
student  pts/2        2024-07-24 15:25 (172.25.250.9)
하지만 접근 계정이 몇십 몇백?
[root@servera ~]# fuser -cu /stage2
/stage2:             30205c(student)
30205c   -> PID
[root@servera ~]# ps -ef | grep 30205
student    30205   30204  0 15:25 pts/2    00:00:00 -bash
root       30235   29767  0 15:27 pts/0    00:00:00 grep --color=auto 30205

[root@servera ~]# fuser -ck /stage2
/stage2:             30205c
[root@servera ~]# umount /stage2

minimal 버전에서는 설치가 되어 있지 않음
# dnf - y install psmisc

위에서 작성한 마운트의 경우에는 임시 마운트
영구마운트?

1 /etc/fstab
2 사용할 자원
/dev/vdb1 vdb2

[root@servera ~]# vi /etc/fstab
shift + g > o 

defaults        0       0
fstab 형식
/dev/vdb1            /stage1             ext4    defaults        0       0
device,UUID       mountpoint         fs       option          dump fsck 

dump 0 1 
fsck 0 1 2
0 사용하지 않음
1 fsck 사용 : / 
2 fsck 사용 : /를 제외한 경우
fsck  -> ext 계열
xfs 는 자동으로 다른 명령을 사용하기 때문에 0으로 처리

UUID=
esc > :!lsblk –fs
내용을 복사
enter
vi로 복귀 > shift + A 
붙여넣기

1 /dev/vdb1            /stage1             ext4    defaults        0       0
2 UUID=36241924-9a60-4afe-89b2-141fce559552       /stage2 xfs     defaults        0       0
권장 사항: UUID를 사용하는것을 권장
esc
:wq

[root@servera ~]# systemctl daemon-reload  : systemd가 변경 된 사항을 업데이트
[root@servera ~]# mount -a : fstab 파일에서 마운트 가능한 모든 자원을 마운트 * 
[root@servera ~]# df 
Filesystem     1K-blocks    Used Available Use% Mounted on
devtmpfs          863980       0    863980   0% /dev
tmpfs             908696       0    908696   0% /dev/shm
tmpfs             363480    5152    358328   2% /run
/dev/vda4        9756652 3750556   6006096  39% /
/dev/vda3         506528  163796    342732  33% /boot
/dev/vda2         204580    7148    197432   4% /boot/efi
tmpfs             181736       0    181736   0% /run/user/1000
tmpfs             181736       0    181736   0% /run/user/0
/dev/vdb1         996780      28    927940   1% /stage1
/dev/vdb2        1038336   40292    998044   4% /stage2

부팅 전에 꼭 마운트를 해보는것이 좋다!

[root@servera ~]# mount /stage2
fstab에 자원이 등록되어 있다면 마운트 포인트만 작성하여도 마운트가 가능

자원등록이 정상적이지 않다면 부팅이 되지 않는다.

부팅불가 상태가 되면
fstab에서 생기는 문제
1 fstab에서 추가한 자원들을 먼저 주석처리하고 부팅
2 확인 뒤 수정 후 부팅

serverb에서 작업
227페이지
랩명령은 입력하지 마시고 serverb에서 작성

[231]
swap

가상메모리
사용이유 : 물리 메모리가 부족한 경우 메모리를 대체하기 위하여 2차 저장장치인 디스크를 사용하여 메모리의 데이터를 대체

RAM 및 스왑 공간 권장 사항
4GB

스왑공간을 사용하게 되는 이유
1 물리 메모리가 부족한 경우
2 프로그램 개발시 swap을 이용하도록 개발자가 정의한 경우

1번의 경우 스왑공간을 메모리처럼 이용하다보니 생기는 단점

느려짐
디스크의 특정 공간, 파티션 점유

스왑은 사용을 해야 하는 경우가 아니라면 빠르게 물리 메모리 증설
프로그램을 분산

스왑을 생성하는 두가지 방법
1 파티션 형태 * 시험에서는 이 방법을 사용해야 한다
2  파일

[root@servera ~]# fdisk /dev/vdb

Welcome to fdisk (util-linux 2.37.4).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

This disk is currently in use - repartitioning is probably a bad idea.
It's recommended to umount all file systems, and swapoff all swap
partitions on this disk.
현재 사용중인 디스크이기 때문에 경고 메시지를 한번 뿌림

Command (m for help): n
Partition type
   p   primary (2 primary, 0 extended, 2 free)
   e   extended (container for logical partitions)
Select (default p): p
Partition number (3,4, default 3): 
First sector (4196352-10485759, default 4196352): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (4196352-10485759, default 10485759): +1G

Created a new partition 3 of type 'Linux' and of size 1 GiB.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start     End Sectors Size Id Type
/dev/vdb1          2048 2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 4196351 2097152   1G 83 Linux
/dev/vdb3       4196352 6293503 2097152   1G 83 Linux

Command (m for help): t
Partition number (1-3, default 3): 3
Hex code or alias (type L to list all): 82

Changed type of partition 'Linux' to 'Linux swap / Solaris'.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x1d3647ea

Device     Boot   Start     End Sectors Size Id Type
/dev/vdb1          2048 2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 4196351 2097152   1G 83 Linux
/dev/vdb3       4196352 6293503 2097152   1G 82 Linux swap / Solar

Command (m for help): w
The partition table has been altered.
Syncing disks.

파티션 준비는 종료
[root@servera ~]# lsblk --fs /dev/vdb
NAME FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
vdb                                                                         
├─vdb1
│    ext4   1.0         177155e8-d4b6-4254-8322-be21915fcceb  906.2M     0% /stage1
├─vdb2
│    xfs                36241924-9a60-4afe-89b2-141fce559552  974.7M     4% /stage2
└─vdb3

[root@servera ~]# mkswap /dev/vdb3
Setting up swapspace version 1, size = 1024 MiB (1073737728 bytes)
no label, UUID=81f211ee-cadb-4d77-b774-54108e8001df

[root@servera ~]# lsblk --fs /dev/vdb
NAME FSTYPE FSVER LABEL UUID                                 FSAVAIL FSUSE% MOUNTPOINTS
vdb                                                                         
├─vdb1
│    ext4   1.0         177155e8-d4b6-4254-8322-be21915fcceb  906.2M     0% /stage1
├─vdb2
│    xfs                36241924-9a60-4afe-89b2-141fce559552  974.7M     4% /stage2
└─vdb3
     swap   1           81f211ee-cadb-4d77-b774-54108e8001df    
swap의 UUID도 블록 디바이스라면 저장하고 있기 때문에 따로 저장하지 않아도 된다.

스왑 활성화
[root@servera ~]# swapon /dev/vdb3

활성화된 스왑 확인
[root@servera ~]# swapon -s
Filename                                Type            Size            Used            Priority
/dev/vdb3                               partition       1048572         0               -2

[root@servera ~]# swapoff /dev/vdb3
[root@servera ~]# swapon -s
[root@servera ~]# 


파일로 생성하는 스왑
임시로 잠깐 사용하고 버리는 용도

[root@servera ~]# mkdir /swap
[root@servera ~]# cd /swap/
[root@servera swap]# dd if=/dev/zero of=swapfile bs=1M count=1024
1024+0 records in
1024+0 records out
1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.62763 s, 660 MB/s
[root@servera swap]# mkswap swapfile 
mkswap: swapfile: insecure permissions 0644, fix with: chmod 0600 swapfile
Setting up swapspace version 1, size = 1024 MiB (1073737728 bytes)
no label, UUID=96e7d89b-17cd-419a-9e03-1d90fa489d93
권한을 변경하라 0600로 변경
[root@servera swap]# chmod 0644 swapfile 

[root@servera swap]# chmod 0600 swapfile 
[root@servera swap]# swapon /swap/swapfile
[root@servera swap]# swapon -s
Filename                                Type            Size            Used            Priority
/swap/swapfile                          file            1048572         0               -2

[root@servera swap]# swapon /dev/vdb3
[root@servera swap]# swapon -s
Filename                                Type            Size            Used            Priority
/swap/swapfile                          file            1048572         0               -2
/dev/vdb3                               partition       1048572         0               -3

Priority : 우선순위 스왑을 이용할때 빠른 디스크 먼저 사용할 수 있도록 숫자를 변경
swapon 명령에서는 -p 옵션
fstab에서는 옵션에서 pri=   32767까지 사용 가능하며 정수로 높은 숫자가 우선순위를 가진다.

[root@servera swap]# swapoff /dev/vdb3
[root@servera swap]# swapon -p 10 /dev/vdb3
[root@servera swap]# swapon -s
Filename                                Type            Size            Used            Priority
/swap/swapfile                          file            1048572         0               -2
/dev/vdb3                               partition       1048572         0               10

fstab을 이용하여 등록
[root@servera swap]# swapoff /swap/swapfile
[root@servera swap]# swapoff /dev/vdb3

[root@servera swap]# vi /etc/fstab
UUID=81f211ee-cadb-4d77-b774-54108e8001df       swap    swap    pri=10        0       0
/swap/swapfile  swap swap       defaults        0       0
[root@servera swap]# systemctl daemon-reload 
[root@servera swap]# swapon -a
fstab에 존재하는 swap들을 한번에 활성화 시키는 옵션 
[root@servera swap]# swapon -s
Filename                                Type            Size            Used            Priority
/dev/vdb3                               partition       1048572         0               10
/swap/swapfile                          file            1048572         0               -2
[root@servera swap]# swapoff -a
[root@servera swap]# swapon -s
[root@servera swap]# 

디스크 초기화 및 나머지 정리
H/W > 파티션 생성 > 파일시스템 > 마운트 > fstab
fstab 삭제 > umount > 디스크 초기화
[root@servera ~]# vi /etc/fstab
기존에 생성한 내용들을 삭제
# umount /stage1
# umount /stage2
# swapoff /dev/vdb3
# swapoff /swap/swapfile
[root@servera ~]# dd if=/dev/zero of=/dev/vdb

[239] 랩을 실행 3,4 
[235] 전체 
둘중에 하나 선택
17:20분
LVM  개념정리만 하고 끝

표준파티션의 가장 큰 단점:
파티션을 한번 생성하고 나면 용량 변경이 힘들다.

LVM : 용량에 대한 조절이 쉽다.
클라우드에서도 보통 LVM 체계를 많이 사용하고 있기 때문에 나중을 위해서라도 섹션은 잘 기억해 두시기 바랍니다.

LVM
  LVM(Logical Volume Manage)이란?

(Virtual Volume Technology)  - LVM(Logical Volume Manager), RAID(Redudant Array Inexpensive Disk) 

 가상 볼륨 방식 장점: (ㄱ) 확장성, (ㄴ) 안정성, (ㄷ) 편의성(관리 용이) 

 (권장)  OS(Operating System) - LVM 기술 사용 권장

DATA 공간 - Storage(Hardware RAID)

물리적인 디스크를 논리적 볼륨그룹으로 구성해서 이 논리적인 볼륨그룹 내에 사용자가 원하는 크기 만큼의 논리볼륨을 할당하여 사용하는 방법으로, 

여러 개의 디스크를 하나의 논리적인 볼륨그룹(VG)으로 구성할 수 있으며, 

각 논리볼륨(LV)는 독립적이므로, 하나의 디스크에도 여러 가지의 파일 시스템을 설치할 수 있다. 

또한 디스크의 용량증설이 요구 될 경우에도 볼륨그룹에 물리볼륨(PV)만 추가하여 용량증설이 가능하고, 
데이터량이 감소하여 더 이상 많은 양의 저장공간이 필요 없는 경우, 볼륨그룹에 속한 물리볼륨 개수를 축소하여 용량 감소가 가능하다.

LVM은 PV(Physical Volume), VG(Volume Group), LV(Logical Volume)으로 구성된다
구성요소                      설명
Physical Volume             PV는 하나의 물리적인 Disk에 LVM을 사용할 수 있도록 LVM 데이터 구조를 생성한 것을 말한다. 
                            디스크에 PV가 생성되면 LVM은 디스크를 하나의 물리볼륨으로 간주하게 되고, 볼륨그룹에 포함시킬 수 있게 된다.
                            따라서 하나의 디스크를 몇 개의 섹션은 논리볼륨으로, 몇 개의 섹션은 전체디스크로 관리하는 등의 방법은 적용 할 수 없다.
			    (예) 물리적인 디스크(EX:/dev/sda) or 물리적인 파티션(EX:/dev/sda1)

Volume Group                VG는 하나 또는 그 이상의 PV를 포함하며, LV를 생성할 수 있는 Volume Group의 집합이다. 
			    vg00은 부팅정보와 OS가 있는 root Volume Group으로 변경이 불가능하다.
			    관리자가 관리상의 편의를 위해 DB의 경우에는 vgdb##, 일반적인 파일시스템의 경우에는vgfil##등으로 VG이름을 지정하여 사용한다.
			    (예) 가상 디스크

Logical Volume              LV는 하나 또는 그 이상의 PV로 구성되어 있는 VG 공간을 전체 또는 분할하여 일반 파일시스템, swap or dump area,
			    raw disk로 사용할 수 있도록 논리적으로 할당한공간이다. 
			    운영 중 공간이 부족할 경우 볼륨그룹에 속해 있는 또 다른 물리볼륨을 사용하여 확장이 가능하고, 필요하다면 크기를 변경하거나 다른 디스크로 데이터를 옮길 수 있다. 
			    Volume Group vg00에 속한 Logical Volume lvol1,lvol2,lvol3은 각각 /stand, primary swap, / 디렉토리로 변경이 불가능 하다.
			    (예) 가상 파티션

LVM 작동방식
 PE (Physical Extent)  : PV가 갖는 일정한 블록

디스크에 PV를 생성하게 되면, LVM은 주소를 지정할 수 있는 PE(Physical Extent)라는 단위로 각 물리 디스크를 나눈다. extent는 주소 0번부터 시작하여 1씩 증분하며 순차적으로 디스크에 할당되고, PE의 크기는 볼륨 그룹을 만들 때 구성할 수 있다. 그리고 각 PE의 크기는 default 값이 4MB이며, 이 값은 필요에 따라 볼륨 그룹을 생성 시 1MB에서 256MB 사이의 값으로 지정할 수 있다.

 LE (Logical Extent)  : LV가 갖는 일정한 블록

디스크에 PV를 생성하고, 생성된 PV를 이용하여 VG(Volume Group)을 구성한 후 이 VG에 LV(Logical Volume)를 생성할 수 있다. LV의 기본 할당 단위를 LE(Logical Extent)라고 하는데, 이 LE는 PV 생성 시 나누어진 PE 영역과 Mapping된다. 따라서 PE의 크기가 4MB이면 LE의 크기도 4MB가 된다. 그리고 LV의 크기는 구성된 LE의 개수 또는 할당할 디스크 용량에 의해 결정된다.

VG에서 PE 사이즈를 결정하고 나면  LE는 매핑되어 있는 상태기 때문에 용량을 다시 변경 할 수 없다.

LVM과 Data Access

앞서 말한 바와 같이 LVM이 논리볼륨에 디스크 공간을 할당할 때 LVM은 주소 0에서 시작하여 각 디스크에 순차적으로 할당된 PE와 할당된 LE의 Mapping Table을 만든다. 따라서 LVM은 그림 9.3과 같이 실제 데이터가 물리 볼륨에 상주하는 위치와 상관없이 논리 볼륨을 Access 함으로써 데이터에 액세스 한다.

LVM과 root 파티션

논리 볼륨이 루트, 부팅, 기본 스왑 또는 덤프에 사용될 경우에는 PE영역은 연속적이어야 한다. 즉, 단일 물리 볼륨에 공백 없이 PE를 할당해야한다. 그러나 루트가 아닌 디스크의 논리 볼륨의 LE에 대응되는 PE는 물리 볼륨에서 연속적이지 않거나 전혀 다른 디스크에 상주할 수 있다. 결과적으로 한 논리 볼륨에 속한 파일 시스템이 둘 이상의 디스크에 상주할 수 있다.

fstab lvm으로 정상 데이터 저장이 되는가?

시험에서도 LVM 생성 -> 재부팅이후에도 자동 마운트 되는지까지


https://meet.google.com/ejo-fquq-bdc

[250]


fstab
마운트
파일시스템
[LVM]  <<<
파티션
H/W

LVM의 구성요소

LV : 실제 LVM에서 볼륨으로 활용하는 영역
VG : PV들을 모아 하나의 그룹으로 생성
PV : LVM의 metadata를 추가하여 LVM에서 이용 가능한 디스크로 전환

구성된 디바이스 명
sample : vg: vgname1 lv:lvname1
/dev/mapper:  /dev/vgname1-lvname1
/dev/LVMname: /dev/vgname1/lvname1

LVM - vdo
가상화 디스크 -> img / qcow2
중복 제거용도로 사용하기 위해 개발

LVM 생성 및 확장
이번 작업은 serverb에서 실시 하고 실습랩은 a를 통해 작업

[root@serverb ~]# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sr0     11:0    1  558K  0 rom  
vda    252:0    0   10G  0 disk 
├─vda1 252:1    0    1M  0 part 
├─vda2 252:2    0  200M  0 part /boot/efi
├─vda3 252:3    0  500M  0 part /boot
└─vda4 252:4    0  9.3G  0 part /
vdb    252:16   0    5G  0 disk    <<<< *
vdc    252:32   0    5G  0 disk    <<<<<* 
vdd    252:48   0    5G  0 disk 


파티션부터 생성
[root@serverb ~]# fdisk /dev/vdb
Command (m for help): n
Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): 

Using default response p.
Partition number (1-4, default 1): 
First sector (2048-10485759, default 2048): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2048-10485759, default 10485759): +1G

Created a new partition 1 of type 'Linux' and of size 1 GiB.
Command (m for help): n
Partition type
   p   primary (1 primary, 0 extended, 3 free)
   e   extended (container for logical partitions)
Select (default p): 

Using default response p.
Partition number (2-4, default 2): 
First sector (2099200-10485759, default 2099200): 
Last sector, +/-sectors or +/-size{K,M,G,T,P} (2099200-10485759, default 10485759): +1G

Created a new partition 2 of type 'Linux' and of size 1 GiB.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x454d9db2

Device     Boot   Start     End Sectors Size Id Type
/dev/vdb1          2048 2099199 2097152   1G 83 Linux
/dev/vdb2       2099200 4196351 2097152   1G 83 Linux

Command (m for help): t
Partition number (1,2, default 2): 
Hex code or alias (type L to list all): 8e

Changed type of partition 'Linux' to 'Linux LVM'.

Command (m for help): t
Partition number (1,2, default 2): 1
Hex code or alias (type L to list all): 8e

Changed type of partition 'Linux' to 'Linux LVM'.

Command (m for help): p
Disk /dev/vdb: 5 GiB, 5368709120 bytes, 10485760 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x454d9db2

Device     Boot   Start     End Sectors Size Id Type
/dev/vdb1          2048 2099199 2097152   1G 8e Linux LVM
/dev/vdb2       2099200 4196351 2097152   1G 8e Linux LVM
두개의 파티션을 생성하고 1G 용량으로 설정, LVM으로 type을 변경
Command (m for help): w
The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

[root@serverb ~]# lsblk /dev/vdb
NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS
vdb    252:16   0   5G  0 disk 
├─vdb1 252:17   0   1G  0 part 
└─vdb2 252:18   0   1G  0 part 

요기까지가 파티션 작업
LVM 으로 변경
확인 용도의 명령어
1 s set 간략한 정보를 나타내는 용도
# pvs
# vgs
# lvs
2 display set 상세한 정보를 나타내는 용도
# pvdisplay 
# vgdisplay 
# lvdisplay 

생성 , 삭제 
[pv,vg,lv]create
[pv,vg,lv]remove

확장 , 축소
extend
reduce

지금 가지고 있는 디스크는 총 2개
/dev/vdb1
/dev/vdb2

[root@serverb ~]# pv
pvchange   pvcreate   pvmove     pvresize   pvscan
pvck       pvdisplay  pvremove   pvs        
[root@serverb ~]# vg
vgcfgbackup      vgexport         vgreduce
vgcfgrestore     vgextend         vgremove
vgchange         vgimport         vgrename
vgck             vgimportclone    vgs
vgconvert        vgimportdevices  vgscan
vgcreate         vgmerge          vgsplit
vgdisplay        vgmknodes        
[root@serverb ~]# lv
lvchange        lvm             lvm_import_vdo  lvremove
lvconvert       lvmconfig       lvmpolld        lvrename
lvcreate        lvmdevices      lvmsadc         lvresize
lvdisplay       lvmdiskscan     lvmsar          lvs
lvextend        lvmdump         lvreduce        lvscan

[root@serverb ~]# pvcreate /dev/vdb1
  Physical volume "/dev/vdb1" successfully created.
  Creating devices file /etc/lvm/devices/system.devices

PV를 추가

[root@serverb ~]# pvs
  PV         VG Fmt  Attr PSize PFree
  /dev/vdb1     lvm2 ---  1.00g 1.00g
[root@serverb ~]# pvdisplay /dev/vdb1
  "/dev/vdb1" is a new physical volume of "1.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/vdb1
  VG Name               
  PV Size               1.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               gi52Hr-YFxk-dMdk-qBO2-rdwr-4W9h-v56T3i

[root@serverb ~]# vgcreate vg0 /dev/vdb1 
  Volume group "vg0" successfully created

vgcreate는 vg이름과 vg에 가입될 디스크를 나열 해주면 된다.
[root@serverb ~]# vgcreate vg1 /dev/vdb2
  Physical volume "/dev/vdb2" successfully created.
  Volume group "vg1" successfully created
pv로 구성되어 있지 않은 디스크도 vg를 생성하면서 앞 단계를 생략 가능

[root@serverb ~]# pvs
  PV         VG  Fmt  Attr PSize    PFree   
  /dev/vdb1  vg0 lvm2 a--  1020.00m 1020.00m
  /dev/vdb2  vg1 lvm2 a--  1020.00m 1020.00m
[root@serverb ~]# pvdisplay /dev/vdb1
  --- Physical volume ---
  PV Name               /dev/vdb1
  VG Name               vg0
  PV Size               1.00 GiB / not usable 4.00 MiB
  Allocatable           yes 
  PE Size               4.00 MiB
  Total PE              255
  Free PE               255
  Allocated PE          0
  PV UUID               gi52Hr-YFxk-dMdk-qBO2-rdwr-4W9h-v56T3i


LV는 용량을 지정해 주어야 한다.
-l extents : PE 개수 
-l 100 : 400M
50%FREE : 전체 용량중에 50%를 할당
-Lsize : 실제 용량
-L 400M : 400M

[root@serverb ~]# lvcreate -n lv0 -l 100 vg0 
  Logical volume "lv0" created.
[root@serverb ~]# lvs
  LV   VG  Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv0  vg0 -wi-a----- 400.00m     

[root@serverb ~]# lvcreate -n lv1 -L 400M  vg0 
  Logical volume "lv1" created.
[root@serverb ~]# lvs
  LV   VG  Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv0  vg0 -wi-a----- 400.00m                                                    
  lv1  vg0 -wi-a----- 400.00m     
[root@serverb ~]# ls -l /dev/ | grep vg0
drwxr-xr-x. 2 root root          80 Jul 24 21:36 vg0
[root@serverb ~]# ls -lR /dev/ | grep vg0
drwxr-xr-x. 2 root root          80 Jul 24 21:36 vg0
lrwxrwxrwx. 1 root root 10 Jul 24 21:35 dm-name-vg0-lv0 -> ../../dm-0
lrwxrwxrwx. 1 root root 10 Jul 24 21:36 dm-name-vg0-lv1 -> ../../dm-1
lrwxrwxrwx. 1 root root       7 Jul 24 21:35 vg0-lv0 -> ../dm-0
lrwxrwxrwx. 1 root root       7 Jul 24 21:36 vg0-lv1 -> ../dm-1
/dev/vg0:
[root@serverb ~]# ls -l /dev/mapper/
control  vg0-lv0  vg0-lv1  
[root@serverb ~]# ls -l /dev/mapper/
total 0
crw-------. 1 root root 10, 236 Jul 21 16:23 control
lrwxrwxrwx. 1 root root       7 Jul 24 21:35 vg0-lv0 -> ../dm-0
lrwxrwxrwx. 1 root root       7 Jul 24 21:36 vg0-lv1 -> ../dm-1

mapper 쪽에도 장치가 링크
일반 장치 이름으로도 링크
[root@serverb ~]# lvremove /dev/vg0/lv0
Do you really want to remove active logical volume vg0/lv0? [y/n]: y
  Logical volume "lv0" successfully removed.
한번 삭제 되면 되돌리기 힘듬.

[root@serverb ~]# lvdisplay /dev/vg0/lv1
  --- Logical volume ---
  LV Path                /dev/vg0/lv1
  LV Name                lv1
  VG Name                vg0
  LV UUID                51PQ1e-NJIE-XOHd-mB4t-ett0-UcYY-s7nuid
  LV Write Access        read/write
  LV Creation host, time serverb.lab.example.com, 2024-07-24 21:36:16 -0400
  LV Status              available
  # open                 0
  LV Size                400.00 MiB
  Current LE             100
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:1
  LV는 장치 명으로 확인해야 한다.

[root@serverb ~]# lvremove /dev/vg0/lv1
Do you really want to remove active logical volume vg0/lv1? [y/n]: y
  Logical volume "lv1" successfully removed.

[root@serverb ~]# vgremove vg0
  Volume group "vg0" successfully removed
[root@serverb ~]# pvs
  PV         VG  Fmt  Attr PSize    PFree   
  /dev/vdb1      lvm2 ---     1.00g    1.00g
  /dev/vdb2  vg1 lvm2 a--  1020.00m 1020.00m
[root@serverb ~]# vgremove vg1
  Volume group "vg1" successfully removed

몇몇 옵션을 추가하여 생성
vg0 : 기본 값   > -100
vg1 : PE size : 8M > -l 100

lv0
lv1
[root@serverb ~]# vgcreate vg0 /dev/vdb1
  Volume group "vg0" successfully created
[root@serverb ~]# vgcreate vg1 -s 8m /dev/vdb2
  Volume group "vg1" successfully created

[root@serverb ~]# vgdisplay vg0
  --- Volume group ---
  VG Name               vg0
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               1020.00 MiB
  PE Size               4.00 MiB
  Total PE              255
  Alloc PE / Size       0 / 0   
  Free  PE / Size       255 / 1020.00 MiB
  VG UUID               z6cKlS-KfMH-zqzI-8ioz-QbvB-mHDD-aAhA0L
   
[root@serverb ~]# vgdisplay vg1
  --- Volume group ---
  VG Name               vg1
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               1016.00 MiB

cl
  PE Size               8.00 MiB
  Total PE              127
  Alloc PE / Size       0 / 0   
  Free  PE / Size       127 / 1016.00 MiB
  VG UUID               i88h1m-CA3D-6RdA-FEdz-oykR-kVHL-zc7pMU

[root@serverb ~]# lvcreate -n lv0 -l 100 vg0 
WARNING: xfs signature detected on /dev/vg0/lv0 at offset 0. Wipe it? [y/n]: y
  Wiping xfs signature on /dev/vg0/lv0.
  Logical volume "lv0" created.
[root@serverb ~]# lvcreate -n lv1 -l 100 vg1
  Logical volume "lv1" created.
[root@serverb ~]# lvs
  LV   VG  Attr       LSize   Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  lv0  vg0 -wi-a----- 400.00m                                                    
  lv1  vg1 -wi-a----- 800.00m                                                    

[root@serverb ~]# lvremove /dev/vg0/lv0
Do you really want to remove active logical volume vg0/lv0? [y/n]: y
  Logical volume "lv0" successfully removed.
[root@serverb ~]# lvremove /dev/vg1/lv1
Do you really want to remove active logical volume vg1/lv1? [y/n]: y
  Logical volume "lv1" successfully removed.

[root@serverb ~]# lvcreate -n lv0 -L 400M vg0 
  Logical volume "lv0" created.
[root@serverb ~]# lvcreate -n lv1 -L 400M vg1 
  Logical volume "lv1" created.

[root@serverb ~]# lvdisplay /dev/vg0/lv0
  --- Logical volume ---
  LV Path                /dev/vg0/lv0
  LV Name                lv0
  VG Name                vg0
  LV UUID                Q1Dz0L-fFfp-REzz-nptI-ncG3-nTJj-Yv6UV2
  LV Write Access        read/write
  LV Creation host, time serverb.lab.example.com, 2024-07-24 21:50:19 -0400
  LV Status              available
  # open                 0
  LV Size                400.00 MiB
  Current LE             100
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:0
   
[root@serverb ~]# lvdisplay /dev/vg1/lv1
  --- Logical volume ---
  LV Path                /dev/vg1/lv1
  LV Name                lv1
  VG Name                vg1
  LV UUID                JIG3N5-HcFm-7jX1-wPdq-00fD-DMnk-kJ0ghd
  LV Write Access        read/write
  LV Creation host, time serverb.lab.example.com, 2024-07-24 21:50:52 -0400
  LV Status              available
  # open                 0
  LV Size                400.00 MiB
  Current LE             50
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     8192
  Block device           253:1
   
[root@serverb ~]# 
같은 용량이지만 vg를 생성할 때 PE size가 얼마인가에 따라   Current LE 의 개수가 차이가 난다

마운트 -> 확장 
[root@serverb ~]# lvremove /dev/vg1/lv1
Do you really want to remove active logical volume vg1/lv1? [y/n]: y
  Logical volume "lv1" successfully removed.
[root@serverb ~]# lvremove /dev/vg0/lv0
Do you really want to remove active logical volume vg0/lv0? [y/n]: y
  Logical volume "lv0" successfully removed.
[root@serverb ~]# vgremove vg1
  Volume group "vg1" successfully removed

기존 lv와 vg1까지 삭제 한 뒤 진행
[root@serverb ~]# lvcreate -n lv1 -l 100 vg0
  Logical volume "lv1" created.
[root@serverb ~]# lvcreate -n lv2 -l 100 vg0
  Logical volume "lv2" created.

파일 시스템에서 생성 됐을때 다른점을 찾아보자.
[root@serverb ~]# mkfs.ext4 /dev/vg0/lv1
mke2fs 1.46.5 (30-Dec-2021)
Creating filesystem with 409600 1k blocks and 102400 inodes
Filesystem UUID: 6586f5e7-a728-496e-a194-557886607290
Superblock backups stored on blocks: 
        8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (8192 blocks): done
Writing superblocks and filesystem accounting information: done 

[root@serverb ~]# mkfs.xfs /dev/vg0/lv2
meta-data=/dev/vg0/lv2           isize=512    agcount=4, agsize=25600 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=1 inobtcount=1
data     =                       bsize=4096   blocks=102400, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1368, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@serverb ~]# mkdir /stage1
[root@serverb ~]# mkdir /stage2
[root@serverb ~]# mount /dev/vg0/lv1 /stage1
[root@serverb ~]# mount /dev/mapper/vg0-lv2 /stage2

[root@serverb ~]# df 
Filesystem          1K-blocks    Used Available Use% Mounted on
devtmpfs               863980       0    863980   0% /dev
tmpfs                  908696       0    908696   0% /dev/shm
tmpfs                  363480    9660    353820   3% /run
/dev/vda4             9756652 1745580   8011072  18% /
/dev/vda3              506528  163796    342732  33% /boot
/dev/vda2              204580    7148    197432   4% /boot/efi
tmpfs                  181736       0    181736   0% /run/user/0
/dev/mapper/vg0-lv1    373358      14    348768   1% /stage1
/dev/mapper/vg0-lv2    404128   23564    380564   6% /stage2

영구마운트 진행
[root@serverb ~]# umount /stage1
[root@serverb ~]# umount /stage2

[root@serverb ~]# vi /etc/fstab


vdb                                                                                                     
├─vdb1      LVM2_member LVM2 001                  gi52Hr-YFxk-dMdk-qBO2-rdwr-4W9h-v56T3i                
│ ├─vg0-lv1 ext4        1.0                       6586f5e7-a728-496e-a194-557886607290      <<             
│ └─vg0-lv2 xfs                                   836fd634-75bc-4378-a7b4-97ac8c9f2f4c               <<<  
└─vdb2      LVM2_member LVM2 001                  C2f8L5-4yiA-D30U-mN7a-vTr8-v3Ec-f5SM2s              

UUID=6586f5e7-a728-496e-a194-557886607290       /stage1 ext4    defaults        0       0
UUID=836fd634-75bc-4378-a7b4-97ac8c9f2f4c       /stage2 xfs     defaults        0       0
위와 같이 두개의 파일 시스템을 추가 하기 위해 등록



[root@serverb ~]# systemctl daemon-reload 
[root@serverb ~]# mount -a 
[root@serverb ~]# df 
Filesystem          1K-blocks    Used Available Use% Mounted on
devtmpfs               863980       0    863980   0% /dev
tmpfs                  908696       0    908696   0% /dev/shm
tmpfs                  363480    9668    353812   3% /run
/dev/vda4             9756652 1745584   8011068  18% /
/dev/vda3              506528  163796    342732  33% /boot
/dev/vda2              204580    7148    197432   4% /boot/efi
tmpfs                  181736       0    181736   0% /run/user/0
/dev/mapper/vg0-lv1    373358      14    348768   1% /stage1
/dev/mapper/vg0-lv2    404128   23564    380564   6% /stage2

확장 *>
디스크 확장 > 

물리적인 디스크의 경우 : 확장 X
내용 백업 > 추가로 큰 디스크 장착 > 이동
기존의 디스크를 유지 한채 새로운 디스크를 추가 장착 하여 LV의 사이즈를 크게 생성

[root@serverb ~]# pvs
  PV         VG  Fmt  Attr PSize    PFree  
  /dev/vdb1  vg0 lvm2 a--  1020.00m 220.00m
  /dev/vdb2      lvm2 ---     1.00g   1.00g
현재 사용하지 않고 있는   /dev/vdb2 존재
[root@serverb ~]# vgs
  VG  #PV #LV #SN Attr   VSize    VFree  
  vg0   1   2   0 wz--n- 1020.00m 220.00m
VFree 현재 220m 남아 있음. > LV에 추가 할 수 있는 용량이 220m

각각 500M 추가 하고 싶다.
[root@serverb ~]# vgextend vg0 /dev/vdb2
  Volume group "vg0" successfully extended
vg0에 /dev/vdb2 디스크를 추가
[root@serverb ~]# pvs
  PV         VG  Fmt  Attr PSize    PFree   
  /dev/vdb1  vg0 lvm2 a--  1020.00m  220.00m
  /dev/vdb2  vg0 lvm2 a--  1020.00m 1020.00m
[root@serverb ~]# vgs
  VG  #PV #LV #SN Attr   VSize VFree
  vg0   2   2   0 wz--n- 1.99g 1.21g
VFree의 용량이 늘어남
stage1,2에 강제로 데이터를 넣어 100%로 생성

[root@serverb ~]# dd if=/dev/zero of=/stage1/file bs=1M count=500 
dd: error writing '/stage1/file': No space left on device
361+0 records in
360+0 records out
378105856 bytes (378 MB, 361 MiB) copied, 1.1352 s, 333 MB/s
[root@serverb ~]# dd if=/dev/zero of=/stage2/file bs=1M count=500 
dd: error writing '/stage2/file': No space left on device
372+0 records in
371+0 records out
389087232 bytes (389 MB, 371 MiB) copied, 0.66043 s, 589 MB/s
[root@serverb ~]# df -h
Filesystem           Size  Used Avail Use% Mounted on
devtmpfs             844M     0  844M   0% /dev
tmpfs                888M     0  888M   0% /dev/shm
tmpfs                355M  9.5M  346M   3% /run
/dev/vda4            9.4G  1.7G  7.7G  18% /
/dev/vda3            495M  160M  335M  33% /boot
/dev/vda2            200M  7.0M  193M   4% /boot/efi
tmpfs                178M     0  178M   0% /run/user/0
/dev/mapper/vg0-lv1  365M  361M     0 100% /stage1
/dev/mapper/vg0-lv2  395M  395M  596K 100% /stage2

[root@serverb ~]# lvextend -l +125 /dev/vg0/lv1 
  Size of logical volume vg0/lv1 changed from 400.00 MiB (100 extents) to 900.00 MiB (225 extents).
  Logical volume vg0/lv1 successfully resized.

[root@serverb ~]# lvextend -L +500M /dev/vg0/lv2
  Size of logical volume vg0/lv2 changed from 400.00 MiB (100 extents) to 900.00 MiB (225 extents).
  Logical volume vg0/lv2 successfully resized.

[root@serverb ~]# df -h
Filesystem           Size  Used Avail Use% Mounted on
devtmpfs             844M     0  844M   0% /dev
tmpfs                888M     0  888M   0% /dev/shm
tmpfs                355M  9.5M  346M   3% /run
/dev/vda4            9.4G  1.7G  7.7G  18% /
/dev/vda3            495M  160M  335M  33% /boot
/dev/vda2            200M  7.0M  193M   4% /boot/efi
tmpfs                178M     0  178M   0% /run/user/0
/dev/mapper/vg0-lv1  365M  361M     0 100% /stage1
/dev/mapper/vg0-lv2  395M  395M  596K 100% /stage2


ext :  resize2fs   [devicename]
xfs : xfs_growfs [mountpoint]

[root@serverb ~]# resize2fs /dev/vg0/lv1
resize2fs 1.46.5 (30-Dec-2021)
Filesystem at /dev/vg0/lv1 is mounted on /stage1; on-line resizing required
old_desc_blocks = 4, new_desc_blocks = 8
The filesystem on /dev/vg0/lv1 is now 921600 (1k) blocks long.

[root@serverb ~]# xfs_growfs /stage2
meta-data=/dev/mapper/vg0-lv2    isize=512    agcount=4, agsize=25600 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=1 inobtcount=1
data     =                       bsize=4096   blocks=102400, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1368, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 102400 to 230400
[root@serverb ~]# 

/dev/mapper/vg0-lv1  836M  364M  429M  46% /stage1
/dev/mapper/vg0-lv2  895M  398M  497M  45% /stage2
LVM에서만 구성하는것이 아니라. 파일 시스템에 대한 업데이트도 해야 한다.
[root@serverb ~]# lvextend -r -l +10 /dev/vg0/lv1 ; df -h /stage1
  Size of logical volume vg0/lv1 changed from 900.00 MiB (225 extents) to 940.00 MiB (235 extents).
  Logical volume vg0/lv1 successfully resized.
resize2fs 1.46.5 (30-Dec-2021)
Filesystem at /dev/mapper/vg0-lv1 is mounted on /stage1; on-line resizing required
old_desc_blocks = 8, new_desc_blocks = 8
The filesystem on /dev/mapper/vg0-lv1 is now 962560 (1k) blocks long.

Filesystem           Size  Used Avail Use% Mounted on
/dev/mapper/vg0-lv1  873M  364M  465M  44% /stage1

-r 옵션을 사용 하면 이후에 파일시스템쪽에서 업데이트 해야 하는 것도 한번에 가능
 -r|--resizefs  

reduce : 축소
주의점:   축소할때 데이터는? 어떻게 하는게 좋을까요?
LV에 있는 데이터를 먼저 마이그레이션 > LV를 축소, 변경 > LV내부의 데이터 삭제 >
백업된 데이터를 다시 복구

LV를 확장하는 문제에서는 기존의 UUID에서 변경 되면 안된다. 삭제 > 생성 작업이 아니라 확장으로 문제를 해결 해야 한다.
gnDCNb-4UID-FgNT-a0ri-HrEh-uZ3d-0tGDxb
j1uEs1-GWva-zT1m-AXsl-ynP0-b58u-wFzsKt
이렇게 다른 UUID가 있다면 이는 0점 처리.

스토리지 문제들의 경우에는 재부팅시 정상적으로 올라와야 한다. *

시험 > 머신을 자동 재부팅 > 머신이 올라오지 않으면 해당 머신 0 

[260]
연습 가이드

[287]
서버 자체에 접속되어 있는 HDD.. 
슬롯의 부족으로 HBA/ 
기존의 network활용 가능한 NFS / NAS 
network file system

nfs
세가지 형태로 nfs를 사용 가능하다
mount 명령을 사용하여 수동으로
/etc/fstab 파일의 항목을 구성하여 부팅 시 영구적으로
자동 마운터 메서드를 구성하여 온디맨드로 autofs *  / systemd-automount [많이 사용되지는 않음]

필요패키지 : nfs-utils
실습및 확인을 위해 다음 명령을 입력
[student@workstation ~]$ lab start netstorage-nfs

serverrb : nfs server
servera : nfs client

nfs server
/etc/exports < 내보내기 목록을 가지고 있음
[root@serverb ~]# cd /etc/exports.d/
[root@serverb exports.d]# ls
public.exports
[root@serverb exports.d]# cat public.exports 
/shares/public *(rw,no_root_squash)
디렉토리         호스트(옵션)

no_root_squash : nfs로 클라이언트에서 접근 하면 root의 경우 noboody 계정으로 접근
하지만 no_root_squash옵션이 적용되면 클라이언트의 root도 서버의 root처럼 동일하게 이용 가능한 설정

nfs도 시간을 사용한다. ntp 설정이 필요
파일을 변경한다 > 
[root@serverb exports.d]# exportfs -r  : exports 파일을 다시 읽어서 내용을 업데이트 

클라이언트로 돌아온다.

nfs의 서버에서는 모든 작업이 완료 되어 있다.
마운트

이전 명령 : showmount 를 사용하여 내가 사용가능한 자원을 확인
하지만 지금은 불가능
현재: [root@servera ~]# mount -t nfs serverb:/ /stage1
위와 같이 작업을 하게 되면 이전의 showmount  처럼 해당서버에서 제공 가능한 디렉토리를 확인 할 수 있음.

[root@servera ~]# cd /stage1
[root@servera stage1]# ls
ls: cannot access 'efi': No such file or directory
efi  shares
[root@servera stage1]# trrr
-bash: trrr: command not found
[root@servera stage1]# tree
.
└── shares
    └── public
        ├── Delivered.txt
        ├── NOTES.txt
        ├── README.txt
        └── Trackings.txt

2 directories, 4 files
[root@servera ~]# df  /stage1
Filesystem     1K-blocks    Used Available Use% Mounted on
serverb:/        9756672 1767168   7989504  19% /stage1

[root@servera ~]# df -T  /stage1
Filesystem     Type 1K-blocks    Used Available Use% Mounted on
serverb:/      nfs4   9756672 1767168   7989504  19% /stage1

[root@servera ~]# umount /stage1
[root@servera ~]# df -T  /stage1
Filesystem     Type 1K-blocks    Used Available Use% Mounted on
/dev/vda4      xfs    9756652 4799588   4957064  50% /

1 수동 마운트
2 자동 마운트
serverb:/shares/public  < 요기로 붙음
[root@servera ~]# mount -t nfs serverb:/shares/public /stage1 
-t nfs를 사용해주면 좀 더 빠르게 자원을 접속 시킨다. 나머지는 동일
[root@servera ~]# cd /stage1
[root@servera stage1]# df 
Filesystem             1K-blocks    Used Available Use% Mounted on
devtmpfs                  863980       0    863980   0% /dev
tmpfs                     908696       0    908696   0% /dev/shm
tmpfs                     363480    5128    358352   2% /run
/dev/vda4                9756652 4799088   4957564  50% /
/dev/vda3                 506528  163796    342732  33% /boot
/dev/vda2                 204580    7148    197432   4% /boot/efi
tmpfs                     181736       0    181736   0% /run/user/0
serverb:/shares/public   9756672 1767168   7989504  19% /stage1
[root@servera stage1]# df .
Filesystem             1K-blocks    Used Available Use% Mounted on
serverb:/shares/public   9756672 1767168   7989504  19% /stage1
[root@servera stage1]# df /stage1
Filesystem             1K-blocks    Used Available Use% Mounted on
serverb:/shares/public   9756672 1767168   7989504  19% /stage1
[root@servera stage1]# 

디렉토리 자체는 local dirve에서 생성되었기 때문에 /dev/vda4에 존재.
nfs mount가 들어가게 되면 경로 출력이 serverb:/shares/public 와 같이 [HOST|IP:/PATH] 출력
[root@servera ~]# mount -t nfs serverb:/shares/public1 /stage1 
Created symlink /run/systemd/system/remote-fs.target.wants/rpc-statd.service → /usr/lib/systemd/system/rpc-statd.service.
mount.nfs: mounting serverb:/shares/public1 failed, reason given by server: No such file or directory
틀린 경로를 적게 되면 마운트가 실패

영구 마운트 등록
# vi /etc/fstab
serverb:/shares/public  /stage1 nfs     rw,sync 0 0 

serverb:/shares/public   : nfs 서버쪽에서 export 해주고 있는 디렉토리
/stage1  : 마운트 포인트
nfs     : 이용하려는 프로토콜
rw,sync  : nfs를 사용하면서 읽고쓰기, 동기화 작업을 실시
0 0 : NFS로 접속되는 데이터에는 따로 작업 하지 않음
[root@servera ~]# vi /etc/fstab
[root@servera ~]# mount -a
[root@servera ~]# df 
Filesystem             1K-blocks    Used Available Use% Mounted on
devtmpfs                  863980       0    863980   0% /dev
tmpfs                     908696       0    908696   0% /dev/shm
tmpfs                     363480    5116    358364   2% /run
/dev/vda4                9756652 4800104   4956548  50% /
/dev/vda3                 506528  163796    342732  33% /boot
/dev/vda2                 204580    7148    197432   4% /boot/efi
tmpfs                     181736       0    181736   0% /run/user/0
serverb:/shares/public   9756672 1767168   7989504  19% /stage1

nfs나 네트워크를 사용하는 무언가 솔루션을 이용할때 서버의 부팅 순서
외부 서버부터 기동 > 클라이언트로 사용하는 프로그램들이 있는 서버를 기동
이전에 사용되던 nfs의 옵션
serverb:/shares/public  /stage1 nfs     rw,sync,_netdev 0 0 

_netdev : init 시스템에서 사용하던 옵션 : 
부팅 > POST > BIOS > ramfs > swaper > init -> 나머지 프로그램들이 기동..
fstab > network
네트워크를 가동 할 때 까지 기존 메인 프로세스를 잠시 wait 


[student@workstation ~]$ ssh root@servera
Activate the web console with: systemctl enable --now cockpit.socket

Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Thu Jul 25 12:27:50 2024 from 172.25.250.9
[root@servera ~]# ㅇㄹ 
-bash: ㅇㄹ: command not found
[root@servera ~]# df 
Filesystem             1K-blocks    Used Available Use% Mounted on
devtmpfs                  863980       0    863980   0% /dev
tmpfs                     908696       0    908696   0% /dev/shm
tmpfs                     363480    5112    358368   2% /run
/dev/vda4                9756652 4800128   4956524  50% /
/dev/vda3                 506528  163796    342732  33% /boot
/dev/vda2                 204580    7148    197432   4% /boot/efi
serverb:/shares/public   9756672 1767168   7989504  19% /stage1
tmpfs                     181736       0    181736   0% /run/user/0

정상 부팅 마운트가 되면 위와 같이 마운트된 정보를 확인 할 수 있다.
[student@workstation ~]$ lab finish netstorage-nfs

autofs용 랩을 설치한뒤 적용
[student@workstation ~]$ lab start netstorage-autofs

nfs를 직접 등록하는 방식을 사용하는것도 나쁘지는 않다.

등록해야 하는 라인의 수가 100개가 넘는다.
1  fstab을 읽는데 오래 걸림
2 파일이 너무 길어서 편집이 힘듬
사용자가 직접 마운트를 해야 하는 경우가 있다.
1 사용자는 마운트가 불가능
2 사용자가 마운트 가능한 리소스 USB등 이동식 디스크만 가능 GUI를 사용하면 자동 아니면 수동
등록된 디렉토리가 아주 짧은 시간에 많이 변경 된다.
1 변경되는 디렉토리에 제대로 응답이 불가능
이런서버가 한대가 아니라 수십대 수백대면 변경시마다 동일한 작업을 몇번씩 반복..

autofs 
장점: 리소스의 절약 : 내가 사용할 때만 마운트를 하고  일정시간이 흐른 다음에는 자동 umount가 진행
사용자가 필요한 경우에도 마운트 가능
서버측 설정이 아니라 클라이언트측 설정
서비스가 변경되면 단편하게 reload 한번으로 기존의 연결들을 유지한 상태로 변경, 업데이트 가능
autofs를 사용하기 위해서는 nfs-utils , autofs
[root@servera ~]# dnf -y install autofs nfs-utils 


nfs server 측 설정
[root@serverb ~]# cd /etc/exports.d/
[root@serverb exports.d]# ls
direct.exports  indirect.exports
[root@serverb exports.d]# cat direct.exports 
/shares/direct *(rw,no_root_squash)
[root@serverb exports.d]# cat indirect.exports 
/shares/indirect *(rw,no_root_squash)

[root@serverb exports.d]# tree /shares/
/shares/
├── direct
│   └── external
│       └── README.txt
└── indirect
    ├── central
    │   └── README.txt
    ├── east
    │   └── README.txt
    └── west
        └── README.txt

6 directories, 4 files

autofs를 구성하는 두가지 방식
설정 파일?
/etc/auto.master 

+dir:/etc/auto.master.d
# Include /etc/auto.master.d/*.autofs  <
파일명의 끝을 항상 .autofs로 끝나야 한다.
[root@servera ~]# cd /etc/auto.master.d/
[root@servera auto.master.d]# pwd
/etc/auto.master.d
[root@servera auto.master.d]# ls
[root@servera auto.master.d]# 

파일 구성하는 방법
/etc/auto.master.d 디렉토리에서 파일을 하나 생성 + 마지막은 .autofs 구성
file.autofs
 다이렉트/ 인다이렉트정의 맵파일

맵 - 실제 nfs 정의 + 다이렉트/ 인다이렉트 따라서 추가적인 디렉토리도 작성



두가지 방식

1 direct - 직접 / 다이렉트
2 indirect -  간접/ 인다이렉트

시작은 무조건 /etc/auto.master.d 
각각 방식마다의 포맷이 다르며 사용 용도도 다름
다이렉트
사용형식 
[root@servera auto.master.d]# vi direct.autofs
/- /etc/direct.map
[root@servera auto.master.d]# cat direct.autofs 
/- /etc/direct.map 

[root@servera auto.master.d]# vi /etc/direct.map
/external -rw,sync,fstype=nfs4 serverb.lab.example.com:/shares/direct/external  
마운트 명령으로 변경해보자.
mount -t nfs -o rw,sync  serverb.lab.example.com:/shares/direct/external  /external

[root@servera auto.master.d]# systemctl enable --now autofs
Created symlink /etc/systemd/system/multi-user.target.wants/autofs.service → /usr/lib/systemd/system/autofs.service.
[root@servera auto.master.d]# systemctl status autofs
● autofs.service - Automounts filesystems on demand
     Loaded: loaded (/usr/lib/systemd/system/autofs.service; en>
     Active: active (running) since Thu 2024-07-25 14:43:39 KST>
   Main PID: 4343 (automount)

[root@servera auto.master.d]# cd /external
[root@servera external]# pwd
/external
[root@servera external]# df .
Filesystem                                      1K-blocks    Used Available Use% Mounted on
serverb.lab.example.com:/shares/direct/external   9756672 1767680   7988992  19% /external
[root@servera external]# df /external
Filesystem                                      1K-blocks    Used Available Use% Mounted on
serverb.lab.example.com:/shares/direct/external   9756672 1767680   7988992  19% /external

인다이렉트 
사용방법이 다이렉트랑 조금 다르다.

[root@servera ~]# cd /etc/auto.master.d/
[root@servera auto.master.d]# vi indirect.autofs
/internal  /etc/indirect.map
[root@servera auto.master.d]# vi /etc/indirect.map
central -rw,sync,fstype=nfs4 serverb.lab.example.com:/shares/indirect/central
[root@servera auto.master.d]# systemctl reload-or-restart autofs
[root@servera auto.master.d]# cd /internal/
[root@servera internal]# cd central
[root@servera central]# ls
README.txt
[root@servera central]# 

여기까지는 동일
다이렉트 방식 / 인다이렉트 방식의 차이점:
[296]
인다이렉트 방식에는 와일드카드를 적용 할 수 있다.

central -rw,sync,fstype=nfs4 serverb.lab.example.com:/shares/indirect/central
현재 /etc/indirect.map에는 한줄의 정의가 들어있음.
해당 디렉토리 /shares/indirect/에 만약에 여러개의 디렉토리 정의가 있는 경우에 이것을 한줄에 처리가 가능


예제:
cd /internal/east로 액세스를 시도
/etc/indirect.map의 앰퍼센트[nfs 자원의 최하위 디렉토리 위치]]가 *표[실제 사용자가 접근 하려는 디렉토리의 최하위 디렉토리] 접근 하며 둘의 문자열을 동일하게 동기화
]
마운트 지점(또는 키)은 별표 문자(*)이며 소스 위치의 하위 디렉터리는 앰퍼샌드 문자(&)입니다. 

 [root@servera ~]# cat /etc/indirect.map
* -rw,sync,fstype=nfs4 serverb.lab.example.com:/shares/indirect/&
[root@servera ~]# systemctl reload-or-restart autofs

serverb에서 디렉토리를 추가로 생성하고 확인
cd /shares/indirect/
mkdir dir{1..3000}
[root@servera ~]# cd /internal/
위에서 생성한 dir ~ 3000 사이의 아무 번호나 접근 해보면 된다.

여기서 주의점 : 
*와 &간의 문자열이 동일하게 들어가야 한다.

다이렉트
1:1 방식 디렉토리 명을 커스텀해서 변경
마운트해야 하는 자원이 적은 경우

인다이렉트
1:다 방식 동일한 디렉토리명을 사용
마운트해야 하는 자원이 많고
자원의 등록, 삭제가 계속 일어나는 경우에 사용

[303] 랩 시작
16:00  부팅프로세스 root 암호 재 생성

실습이 끝난 뒤 미리 다음 명령어를 입력해 두시기 바랍니다.
[student@workstation ~]$ lab start boot-resetting

init 0
poweroff
shutdown
..
이런 명령들이 systemctl 에 통합
shutdown  유예 시간 지원 

init                          systemd
0 시스템 종료         0 시스템 종료  
1 싱글 모드            1 싱글 모드
2 멀티[NFS X]        2  멀티 
3 멀티                    3 멀티    multi-user.target 
4 사용하지 않음     4 멀티 
5 그래픽                 5 그래픽   graphical.target  
6 재부팅                  6 재부팅

런타임시 런레벨 변경
이전 :
# init 3
# init 5

현재
# systemctl isolate [multi-user.target|graphical.target ]

영구적인 변경
이전 방식:
 /etc/inittab 파일에서 
init:5:
[root@serverb ~]# cat /etc/inittab 
# inittab is no longer used.
#
# ADDING CONFIGURATION HERE WILL HAVE NO EFFECT ON YOUR SYSTEM.
#
# Ctrl-Alt-Delete is handled by /usr/lib/systemd/system/ctrl-alt-del.target
#
# systemd uses 'targets' instead of runlevels. By default, there are two main targets:
#
# multi-user.target: analogous to runlevel 3
# graphical.target: analogous to runlevel 5
#
# To view current default target, run:
# systemctl get-default
#
# To set a default target, run:
# systemctl set-default TARGET.target
현재 변경 방식:
# systemctl set-default  [multi-user.target|graphical.target ]
헤드리스 서버들의 경우에는 multi-user로 구성된 경우가 많다.
확인
# systemctl get-default
 
루트 암호 재설정
시험에도 나오는 것! 무조건 나옴
한대의 머신이 비밀번호를 모르는 상태로 제공

정책때문에 시간별로 변경
ssh 키도 없고
비밀번호도 모르고
잊어버렸고..
sudo 기능도 활성화 된 계정이 없다
비밀번호를 강제로 변경
해킹


1. 시스템을 재부팅합니다.
GUI 버전 전원 아이콘 > 재부팅
CLI ->  ctrl + alt + del 
VNC -> 3번째 버튼
virt-viewer -> 왼쪽에 보시면 send >  ctrl + alt + del   <t시험
2. Enter 키를 제외한 임의의 키를 눌러서 부트 로더 카운트다운을 중단합니다.
enter > 현재 선택된 커널을 가지고 로드
많이 누르는 키는 방향키 
3. 커서를 복구 커널 항목(이름에 rescue 라는 단어가 있는 항목)으로 이동하여 부팅합니다.
RHEL9 
GRUB에서 부팅 프로세스의 동작은 변경하는데 사용되는 특정 옵션이 이전버전의 RHEL과 달리 9버전에서는 동일하게 동작하지 않음
이전 동작처럼 구현하기  위해 rescue 커널을 이용해야 한다.
rescue 선택한 상태에서 번 지문으로 넘어감
4. e 를 눌러 선택한 항목을 편집합니다.
5. 커서를 커널 명령줄(linux로 시작하는 행)로 이동합니다.
명령줄 마지막으로 가기 위해 end키를 입력
6. rd.break을 추가합니다. 이 옵션을 사용하면 initramfs 이미지에서 실제 시스템으로 제어 권한이이동하기 직전에 중단됩니다.
쉘로 진입하기 위해 rd.break 명령을 추가한다.
7. Ctrl+x 를 눌러 변경 사항을 적용하여 부팅합니다.
8. 메시지가 표시되면 Enter 키를 눌러 유지 관리를 수행합니다
여기서 일반 커널로 진입했다면 비밀번호를 요구 한다.

비밀번호를 변경하자.
root 암호를 재설정하려면 다음 절차를 따릅니다.

1. /sysroot 를 읽기/쓰기로 다시 마운트합니다.
sh-5.1# mount -o remount,rw /sysroot
initramfs에서 아직 실제 disk로 피벗이 일어나지 않은 상태 
디스크 디렉토리를 살펴보면 실제 디스크의 내용이 보이지 않음
읽기 전용으로 마운트 되어 있는 sysroot를 rw로 변경해준다.

2. /sysroot가 파일 시스템 트리의 루트로 간주되는 chroot 환경으로 전환합니다.
sh-5.1# chroot /sysroot

chroot를 이용하여 실제 사용하는 디스크의 최상위 디렉토리로 전환

3. 새 root 암호를 설정합니다.
sh-5.1# passwd root
passwd 명령으로 하면 보이지 않아서 실수할 가능성이 높다.!
그렇기 때문에 echo 를 이용하여 비밀번호를 확인하면서 변경 *
sh-5.1# echo ‘redhat’ | passwd –stdin  root
4. 레이블이 지정되지 않은 모든 파일(이 시점에서는 /etc/shadow 포함)에 부팅 중 다시 레이블이 지정되도록 합니다.
sh-5.1# touch /.autorelabel

만약에 그냥 재부팅이 된다면 위 과정을 다시 진행해야 함
현재 시스템은 SELINUX 강제 모드로 활성화 되어 있다.
비밀번호를 변경 > /etc/shadow : timestamp 
SELINUX DB > /etc/shadow timestamp  : 재부팅 전의 timestamp 
현재 상황에서 강제로 변경 일어난 상태
부팅 SELINUX  > /etc/shadow 자신도 모르는 사이 변경된 파일 
이 파일의 액세스를 거부

 /.autorelabel
부팅중에 커널이 최상위 디렉토리에 있는 .autorelabel을 발견하면
SELINUX 강제로 disable > 그리고 다시 SELINUX enforcing으로 변경
새로운 타임스탬프 파일을 구성
부팅중에 시간은 좀 더 걸리겠지만 이후 SELINUX를 통해 /etc/shadow을 읽는데 문제가 없다.

5. exit 을 두 번 입력합니다. 첫 번째는 chroot 환경을 종료하고 두 번째는 initramfs 디버그 쉘을종료합니다.

실수?

[324] 실습 

[125]
파일 보관 및 전송 
시험 : tar 아카이빙

압축된 tar 아카이브 관리
압축 : 용량을 확보하기 위해서 사용
단일 파일 대상. 
아카이빙 : 보관 
지정하는 파일, 디렉토리등 하나의 파일로 구성 하기 위한 방법

확인!

mkdir /test
cd /test
cp -r /var testvar
cp -r testvar/ dir1   < tar
cp -r testvar/ dir2 < gz

압축된 tar 아카이브 관리

tar 아카이빙
압축

tar  - tape archiving
테이프장비에 데이터를 보관하기 위해서 사용하던 명령
LTO 장비 > 디스크를 통해서 가상 테이프 장치 (10년전)

f 옵션을 사용하면 장비가 아니라 파일로 내보내기 가능

c 아카이빙을 생성
v 아카이빙 작업시 상세한 정보를 확인
f 테이프 장비가 아니라 파일을 대상으로 작업
t 생성된 아카이빙을 확인
x 생성된 아카이빙에서 추출
옵션 두가지 형태
-cvf
cvf    <<< 

위 옵션들을 사용하면 tar 아카이빙을 생성한다. 하지만 하나의 파일로만 생성하는 것이기 때문에 용량에 대한 확보를 하는것은 아님

아카이빙 + 압축

-z   gzip 알고리즘을 사용한 압축
-j    bzip2 알고리즘을 사용한 압축
-J   xz 알고리즘을 사용한 압축
xz > bzip2 > gzip
각각의 알고리즘 마다 압축률 / 진행시간이 다르기 때문에 사이트의 특성에 맞추어 실행

[root@serverb test]# rm -rf *
[root@serverb test]# cp /boot/vmlinuz-5.14.0-70.13.1.el9_0.x86_64  file1
[root@serverb test]# chmod 644 file1 

[root@serverb test]# ls -l
total 65376
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file1
-rw-r--r--. 1 root root 11155056 Jul 25 04:35 file2
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file3
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file4
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file5
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file6

각각의 파일을 생성하여 용량을 확인해보자
[root@serverb test]# ls -l
total 322676
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file1
-rw-r--r--. 1 root root 11155056 Jul 25 04:35 file2
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file3
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file4
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file5
-rw-r--r--. 1 root root 11155056 Jul 25 04:33 file6
-rw-r--r--. 1 root root 66938880 Jul 25 04:35 file.tar
-rw-r--r--. 1 root root 65915684 Jul 25 04:36 file.tar.bz2
-rw-r--r--. 1 root root 65409277 Jul 25 04:35 file.tar.gz
-rw-r--r--. 1 root root 65201568 Jul 25 04:36 file.tar.xz
tar 아카이빙에서 생성된 내용들로 보아 큰 변화는 없다. 
그렇다면 어떤 압축 방식을 이용했는지 ?
# file *
file.tar:     POSIX tar archive (GNU)
file.tar.bz2: bzip2 compressed data, block size = 900k
file.tar.gz:  gzip compressed data, from Unix, original size modulo 2^32 66938880
file.tar.xz:  XZ compressed data

[root@host ~]# tar -cf /root/etc-backup.tar /etc 
tar: Removing leading `/' from member names  << * 
파일은 생성 …
예전 버전은 최상위 디렉토리를 포함하여 절대경로로 아카이빙을 생성하게 되면 최상위 디렉토리를 포함하여 구성
단점 : 아카이빙 해제 > /etc에 기존 파일을 overwrite
덮어쓰기 한다.
기존 내용 사라지고 백업된 내용으로 그냥 교체
7버전 이상 최상위 디렉토리를  포함 
/를 제거하고 상대경로의 디렉토리로 생성

가끔 최상위 디렉토리로 이용해야 하는 경우도 존재 하기는 한다. 그렇때는 
-P, --absolute-names 옵션을 사용해준다.
[root@host ~]# tar -cf /root/etc-backup.tar /etc -P
주의점 : 하지만 압축 해제를 할때도 동일하게 -P 옵션을 사용해야 한다.
[root@host ~]# tar -xf /root/etc-backup.tar -P
	# mkdir /test1
	# mkdir /test2
	# cd /test1 
	# touch file1
	# tar cvzf /test1/file1.tar.gz /test1/file1
	tar: 구성 요소 이름 앞에 붙은 `/' 제거 중
	/test/file1
	# ls
	file1  file1.tar.gz
	# mv file1.tar.gz /test2
	# echo 1111 > file1
	# cd /test2
	# tar xvzf file1.tar.gz 
	test/file1
	# ls
	file1.tar.gz  test
	# cd test
	# ls
	file1
----------------------------------<< 7,8 버전에서의 확인

구 버전에서의 문제

	# mkdir /test1
	# mkdir /test2
	# cd /test1 
	# echo 2222 > file1
	# tar cvzf /test1/file1.tar.gz /test1/file1
	/test1/file1
	# ls
	file1  file1.tar.gz
	# mv file1.tar.gz /test2
	# echo 1111 > file1
	# cd /test2
	# tar xvzf file1.tar.gz 
	/test1/file1
	# ls
	file1.tar.gz 
  # cd /test1
  # ls
  # file1
  # cat file1
  2222
  * 백업파일이 원본파일을 그대로 덮어쓰기 해버리는 문제점이 존재
    왜 이런 문제점이 생길까? 운영중인 파일에 바로 덮어쓰기를 하기 때문에 파일의 버전 정보, 설정 정보등을 정확히 확인 할 수 없다.
  
	이전 버전에서는 파일을 생성할 때 tar cvzf /test1/file1.tar.gz . 형태로 생성하였다.






gzip : 압축을 진행 할때 
gzip -d : 압축을 해제 할 때 

아카이빙 만으로는 용량 확보를 하는 것은 힘들다. 보통은 압축을 같이 사용하여 구성
tar 이용 하는 경우중 하나가 마이그레이션
# tar cvf - . | (cd /dir ; tar xvf -) 
위 명령어대로 마이그레이션을 진행하면 속성, 권한, 수정시간 , 파일의 변경없이 마이그레이션이 가능하다.

# cp  -rp . /test  X 

시험의 경우에는 다음과 같이 출제
tar 만 나오지는 않음
gzip / bzip2 / xz 셋중 하나와 같이 이용하는것으로 나오기 때문에
z/      j         /  J 이 옵션중 하나를 이용해야 한다.
그렇기 때문에 세 옵션을 암기.
매뉴얼 찾을수 있도록 훈련

https://meet.google.com/ogi-twpu-wic

[131] 연습문제
tar cf file.tar dir / file..
file file.ta

15분에 시작 

[133]
시스템 간 안전한 파일 전송
rcp -> scp 
ftp / vsftp > sftp
xinetd + rsync -> ssh + rsync

[root@serverb ~]# rpm -ql openssh-clients | grep bin
/usr/bin/scp
/usr/bin/sftp
/usr/bin/ssh
/usr/bin/ssh-add
/usr/bin/ssh-agent
/usr/bin/ssh-copy-id
/usr/bin/ssh-keyscan
openssh-clients에는 scp sftp ssh 가 한 세트로 포함
ssh에 대한 포트로 접근

sftp는 기존의 ftp명령과 동일
get mget
put mput
cd lcd 
pwd lpwd
작업 디렉토리를 맞추고 작업 + 파일 전송을 하거나 파일을 받거나.

단일 파일 또는 디렉토리를 업로드 / 다운로드 하는 용도로 사용

rsync : 디렉토리의 동기화 
rsync -avz dir dir
이전: rsync -avz -e ssh /path host:/path
rsync -avz –delete /path host:/path
141]

[(주의) 아래 사항의 차이점을 이해 해야 한다.
# rsync -avz --delete /backup/ /test101/  (# rsync -avz --delete -e ssh /backup/ /test101)
-> (ㄱ) /test101 디렉토리가 없으면 생성된다.
-> (ㄴ) /backup 디렉토리의 내용과 /test101 디렉토리의 내용은 동일하다.
타겟 디렉토리가 원본과 동일해야 하는 경우

# rsync -avz --delete /backup /test101
-> (ㄱ) /test101 디렉토리가 없으면 생성된다.
-> (ㄴ) /test101 디렉토리안에 backup 디렉토리가 생성된다.
타겟 디렉토리에 여러 동기화된 디렉토리를 생성하여 보관하는 경우
rsync에서 ssh로 사용할 때 상대방 서버에서 포트가 변경 되면 동일하게 포트를 지정

[145] 랩
시작 시간 : 11:05

RHCSA :EX200
등록 : 일주일 뒤에 등록해야지
슬롯 다 차있음
일단은 빈 날짜에 등록 먼저
등록 이후에 변경은 쉬움

최초 등록일로 부터 1년?
혹시나 1년이 지나면 
기본 : 티켓이 삭제

예외 : 사고 / 큰 행사
1년 거의 다 될쯤에 
시험등록 차사고 
사고 증명서.. 

깃허브 데이터는 일요일 삭제 
리포자체를 삭제한 뒤 새로 생성하기 때문에 복구 불가능
끝나고 집에 가셔서 파일을 미리 다운로드 해 두시기 바랍니다.
이후 요청이 있어도 제가 따로 백업해두지 않기 때문에 파일을 드릴수 없음

[152]
시스템 튜닝 :
sysctl 명령어를 직접 수정 값을 변경
[root@servera ~]# sysctl -a 
현재 파라미터 값을 확인 할 수 있음

현재는 튜닝 프로파일을 제공

정적 튜닝 구성
특정한 프로파일을 선택해서 해당 내용으로 유지 시키는것이 목적

동적 튜닝 구성
cpu / disk / network. 워크로드를 일정 기간마다 모니터링 하여
부하가 심한 리소스에 대해서 자동으로 프로파일을 변경 하도록 하는것이 목적
/etc/tuned/tuned-main.conf
dynamic_tuning = 1   : 1인 경우에는 동적 튜닝을 사용하기 위한 설정
update_interval = 10   : 모니터링의 시간 구간을 결정

서비스를 활성화 할때:
패키지 설치
패키지 구성 요소 확인
설정 파일 수정 사항.
서비스 온 , 활성화
확인 및 구성 요소 등록

[root@servera ~]# dnf info tuned
Last metadata expiration check: 0:00:10 ago on Thu 25 Jul 2024 10:11:15 PM EDT.
Installed Packages
Name         : tuned
Version      : 2.18.0
Release      : 1.el9
Architecture : noarch
Size         : 855 k
Source       : tuned-2.18.0-1.el9.src.rpm
Repository   : @System
Summary      : A dynamic adaptive system tuning daemon
URL          : http://www.tuned-project.org/
License      : GPLv2+
Description  : The tuned package contains a daemon that
             : tunes system settings dynamically. It does
             : so by monitoring the usage of several system
             : components periodically. Based on that
             : information components will then be put into
             : lower or higher power saving modes to adapt
             : to the current usage. Currently only
             : ethernet network and ATA harddisk devices
             : are implemented.

[root@servera ~]# dnf -y install tuned
# rpm -ql tuned
# rpm -qlc tuned
# rpm -qld tuned
[root@servera ~]# rpm -ql tuned | grep 'bin'
/usr/sbin/tuned 
/usr/sbin/tuned-adm   < 명령어

[root@servera ~]# tuned-adm list   
현재 이용가능한 프로파일들의 목록
Available profiles:
- accelerator-performance     - Throughput performance based tuning with disabled higher latency STOP states
- balanced                    - General non-specialized tuned profile
- desktop                     - Optimize for the desktop use-case
- hpc-compute                 - Optimize for HPC compute workloads
- intel-sst                   - Configure for Intel Speed Select Base Frequency
- latency-performance         - Optimize for deterministic performance at the cost of increased power consumption
- network-latency             - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance
- network-throughput          - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks
- optimize-serial-console     - Optimize for serial console use.
- powersave                   - Optimize for low power consumption
- throughput-performance      - Broadly applicable tuning that provides excellent performance across a variety of common server workloads
- virtual-guest               - Optimize for running inside a virtual guest
- virtual-host                - Optimize for running KVM guests
No current active profile.

[root@servera ~]# tuned-adm profile_info desktop
Profile name:
desktop

Profile summary:
Optimize for the desktop use-case

Profile description:

[root@servera ~]# tuned-adm profile_info virtual-guest
Profile name:
virtual-guest

Profile summary:
Optimize for running inside a virtual guest

Profile description:

[root@servera tuned]# ls
accelerator-performance  network-throughput
balanced                 optimize-serial-console
desktop                  powersave
functions                recommend.d
hpc-compute              throughput-performance
intel-sst                virtual-guest
latency-performance      virtual-host
network-latency


[root@servera tuned]# cd virtual-guest/
[root@servera virtual-guest]# ls
tuned.conf
[root@servera virtual-guest]# cat tuned.conf 
#
# tuned configuration
#

[main]
summary=Optimize for running inside a virtual guest
include=throughput-performance

[sysctl]
# If a workload mostly uses anonymous memory and it hits this limit, the entire
# working set is buffered for I/O, and any more write buffering would require
# swapping, so it's time to throttle writes until I/O can catch up.  Workloads
# that mostly use file mappings may be able to use even higher values.
#
# The generator of dirty data starts writeback at this percentage (system default
# is 20%)
vm.dirty_ratio = 30

# Filesystem I/O is usually much more efficient than swapping, so try to keep
# swapping low.  It's usually safe to go even lower than this on systems with
# server-grade storage.
vm.swappiness = 30

이 구성 파일은 제공된 프로파일 중 하나를 기준으로 사용한 다음 구성할 매개 변수를 추가하거나 수정할 수 있으므로 새 튜닝 프로파일을 생성할 때 매우 유용합니다. 튜닝 프로파일을 생성하거나 수정하려면 /usr/ lib/tuned 디렉터리에서 /etc/tuned 디렉터리로 튜닝 프로파일 파일을 복사한 다음 수정합니다.  /usr/lib/tuned 및 /etc/tuned 디렉터리에 동일한 이름의 프로파일 디렉터리가 있는 경우 항상 후자 가 우선합니다. 따라서 /usr/lib/tuned 시스템 디렉터리에 있는 파일을 직접 수정하지 마십시오. tuned.conf 파일의 나머지 섹션에서는 tuning 플러그인을 사용하여 시스템의 매개 변수를 수정합니다. 위 예제에서 [sysctl] 섹션은 sysctl 플러그인을 통해 vm.dirty_ratio 및 vm.swappiness 커널 매개 변수를 수정합니다. 

[root@servera virtual-guest]# tuned-adm 
active     off        recommend  
list       profile    verify     
[root@servera virtual-guest]# tuned-adm recommend 
virtual-guest
현재 시스템에 맞는 프로파일을 추천

[root@servera virtual-guest]# hostnamectl
 Static hostname: servera.lab.example.com
       Icon name: computer-vm
         Chassis: vm 
         가상머신으로 되어 있기 때문에 virtual-guest를 추천
      Machine ID: ace63d6701c2489ab9c0960c0f1afe1d
         Boot ID: 5d4e48b56e6646b0a80802b8466a5d70
  Virtualization: kvm
Operating System: Red Hat Enterprise Linux 9.0 (Plow)     
     CPE OS Name: cpe:/o:redhat:enterprise_linux:9::baseos
          Kernel: Linux 5.14.0-70.13.1.el9_0.x86_64
    Architecture: x86-64
 Hardware Vendor: Red Hat
  Hardware Model: OpenStack Compute

[root@servera virtual-guest]# !33
sysctl -a | grep vm.dirty_ratio
vm.dirty_ratio = 20
[root@servera virtual-guest]# !34
sysctl -a | grep vm.swappiness
vm.swappiness = 60
[root@servera virtual-guest]# tuned-adm 
active     off        recommend  
list       profile    verify     
[root@servera virtual-guest]# tuned-adm profile virtual-guest
[root@servera virtual-guest]# !33
sysctl -a | grep vm.dirty_ratio
vm.dirty_ratio = 30
[root@servera virtual-guest]# !34
sysctl -a | grep vm.swappiness
vm.swappiness = 30
[root@servera virtual-guest]# 
프로파일 적용후 변경 확인

[root@servera virtual-guest]# tuned-adm active 
Current active profile: virtual-guest
[root@servera virtual-guest]# tuned-adm off
[root@servera virtual-guest]# tuned-adm active 
No current active profile.

profile - 프로파일 적용
active - 적용된 프로파일 확인
off - 적용된 프로파일 해제 

시험에서는?
[root@servera virtual-guest]# tuned-adm recommend
virtual-guest
[root@servera virtual-guest]# tuned-adm profile virtual-guest
[root@servera virtual-guest]# tuned-adm active 
Current active profile: virtual-guest


[164]
nice
renice

프로세스를 실행 하면
nice / pr
따로 지정 하지 않고 실행하게 되면 0
nice값을 지정 한다는 것은 프로세스의 우선순위를 조정
[-] 우선순위를 앞으로
[+] 우선순위를 뒤로

# run.sh 
# nice –10 run.sh
# nice -n -10 run.sh

# run -10 run.sh 
# run -n 10 run.sh
대시바를 입력하고 nice값을 지정하기 때무넹 -10으로 지정하면 실제로는 +10
변경된 우선순위를 가지고 프로그램을 실행 한다.
그리고 나서 다시 변경 해야 하는 경우?

renice

nice : 프로그램 이름 자체로 입력
renice : PID 

# renice -n 19 555
변경되는 nice값은 기존의 값을 가지고 계산하지 않는다.
기존 값이 +15  > +19
기존의 값은 무시하고 새로운 값으로 업데이트 

백업
데이터 수집 등
스크립트 등에서 많이 사용
백업 -> +값으로 지정하여 조금 느리게
데이터 수집 -> -값으로 처리하여 다른것보다 빠르게

시험에서는 저장되어 있는 데이터를 가지고 확인
nioce renice 실시간으로 변경되고 처리되는 데이터들으 ㄴ크게 신경쓰지 않아도 됩니다.

[172] 랩

12:10
6장 + 11장 통합

[180]
SELINUX - Security Enhanced Linux
시험을 보시는 분들은 꼭 익혀놔야 하는 것!
리눅스 기본적으로 보안 사항
소유자 / 권한 레벨에서 보안을 지정
추가 속성
ACL

컨텍스트 * 

[root@servera ~]# cat /etc/selinux/config
selinux에 대한 부팅시 설정
SELINUX=부팅시 설정
enforcing : 강제 모드   정책을 위반한 경우에는 해당 프로세스를 차단 / audit파일에서 로그로 등록
permissive : 허용모드  정책을 위반한 경우에는 해당 프로세스를 진행/ audit파일에서 로그로 등록
[181]
disabled : 비활성화 정책을 위반한 경우에해당 프로세스를 진행/ audit파일에서 로그로 등록하지 않음 * RHEL9의 경우에는 SELINUX=disabled가 적용되지 않음.
다음과 같이 비활성화를 진행해야 한다.
비활성화
grubby --update-kernel ALL --args selinux=0
활성화
grubby --update-kernel ALL --remove-args selinux

부팅시가 아니라 온라인 상태에서의 확인?
[root@servera ~]# getenforce 
Enforcing
온라인 상태에서 임시 변환
[root@servera ~]# setenforce 
usage:  setenforce [ Enforcing | Permissive | 1 | 0 ]
1 Enforcing  
0 Permissive 

[root@servera audit]# cd 
[root@servera ~]# ls
[root@servera ~]# touch file1
[root@servera ~]# ls -l
total 0
-rw-r--r--. 1 root root 0 Jul 25 23:27 file1
[root@servera ~]# ls -lZ
total 0
-rw-r--r--. 1 root root unconfined_u:object_r:admin_home_t:s0 0 Jul 25 23:27 file1
[root@servera ~]# ls -ldZ /tmp
drwxrwxrwt. 5 root root system_u:object_r:tmp_t:s0 4096 Jul 25 23:20 /tmp
[root@servera ~]# mkdir /test
[root@servera ~]# ls -ldZ /test
drwxr-xr-x. 2 root root unconfined_u:object_r:default_t:s0 6 Jul 25 23:27 /test
[root@servera ~]# ls -ldZ /var
drwxr-xr-x. 20 root root system_u:object_r:var_t:s0 279 May 18  2022 /var
모든 파일과 디렉토리에는 컨텍스트가 이미 존재 하고 있다.
확인을 위해서는 다음 명령을 입력한다.
[root@servera ~]# semanage fcontext -l

[168]
semanage fcontext 명령
여 파일 및 디렉터리의 기본 컨텍스트를 결정하고, restorecon 명 령을 사용하여 SELinux 정책에서 정의한 컨텍스트를 파일 및 디렉터리에 적용하는 SELinux 정책 규칙을 관리

기본 컨텍스트를 결정 : 부모 디렉토리의 컨텍스트를 상속
모든 디렉토리에는 selinux 정책에 따른 파일, 디렉토리의 컨텍스트가 결정
한번 만들어진 컨텍스트가 영구적으로 유지되는것이 아니라 특정 명령어를 사용하여 변경도 가능

chcon         - 임시 변경용
restorecon  - 부모디렉토리를 따라 상속되는 컨텍스트로 재 적용

파일을 복사, 생성, 이동 할때 컨텍스트가 유지 되는 경우도 있고 또는 변경 되는 경우도 있다.
웹 서버 파일을 통하여 확인해보자.

[root@servera ~]# dnf -y install httpd
[root@servera ~]# ls -ld /var/www/html
drwxr-xr-x. 2 root root 6 Mar 21  2022 /var/www/html
[root@servera ~]# ls -ldZ /var/www/html
drwxr-xr-x. 2 root root system_u:object_r:httpd_sys_content_t:s0 6 Mar 21  2022 /var/www/html
[root@servera ~]# systemctl enable --now httpd
Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service → /usr/lib/systemd/system/httpd.service.
[root@servera ~]# echo 'hello' > /var/www/html
-bash: /var/www/html: Is a directory
[root@servera ~]# echo 'hello' > /var/www/html/index.html
[root@servera ~]# curl http://localhost
hello

[root@servera ~]# ls -lZ /var/www/html/index.html
-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 6 Jul 26 01:11 /var/www/html/index.html

생성의 경우에는 부모 디렉토리의 컨텍스트를 그대로 상속 받는다.

[root@servera ~]# semanage fcontext -l | grep '/var/www(/.*)?'
/var/www(/.*)?                                     all files          system_u:object_r:httpd_sys_content_t:s0 
/var/www 디렉토리 하위에 있는 모든 디렉토리 / 파일의 경우 기본 httpd_sys_content_t 가지게 된다.

다른곳에 있는 파일을 이동하여 확인해보자
[root@servera ~]# cd /var/www/html
[root@servera html]# rm -rf index.html 
[root@servera html]# cd 
[root@servera ~]# echo 'hello' > index.html[root@servera ~]# ls -lZ
total 4
-rw-r--r--. 1 root root unconfined_u:object_r:admin_home_t:s0 6 Jul 26 01:14 index.html

[root@servera ~]# mv index.html /var/www/html
[root@servera ~]# ls -lZ /var/www/html
total 4
-rw-r--r--. 1 root root unconfined_u:object_r:admin_home_t:s0 6 Jul 26 01:14 index.html
컨텍스트가 기존에 있는 것으로 유지 된 상태에서 파일을 이동

[root@servera ~]# curl http://localhost
….
에러가 난다. 실제 페이지가 정상적으로 표시 되지 않는다.
해결 방법 
1 해당 파일만 임시로 변경 테스트
2 부모 디렉토리의 컨텍스트로 재지정



SELinux 컨텍스트 변경
 파일의 SELinux 컨텍스트는 semanage fcontext, restorecon, chcon 명령으로 관리할 수 있습니다.
[root@servera ~]#  restorecon -v  /var/www
[root@servera ~]#  restorecon -v  /var/www/html
[root@servera ~]#  restorecon -Rv  /var/www
Relabeled /var/www/html/index.html from unconfined_u:object_r:admin_home_t:s0 to unconfined_u:object_r:httpd_sys_content_t:s0

[root@servera ~]# ls -lZ /var/www/html
total 4
-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 6 Jul 26 01:14 index.html
[root@servera ~]# ls -lZ /var/www/html/index.html 
-rw-r--r--. 1 root root unconfined_u:object_r:httpd_sys_content_t:s0 6 Jul 26 01:14 /var/www/html/index.html

[root@servera ~]# curl http://localhost
hello
selinux가 활성화 되어 있는 상태에서는 컨텍스트와 정책이 맞아야만 파일에 대한 접근이 완료

semanage fcontext -l | grep '/var/www(/.*)?'
/var/www(/.*)?
확장 정규 표현식은 [ (/.*)? ]
슬래시로 시작하고 그 뒤에 임의의 수의 문자가 있는 문자 집합
적용되는 디렉토리 내에 있는 ./ 으로 시작하는 모든 파일

[root@servera ~]# mkdir /virtual
[root@servera ~]#  touch /virtual/index.html
[root@servera ~]# ls -Zd /virtual/
unconfined_u:object_r:default_t:s0 /virtual/
[root@servera ~]# ls -Z /virtual/
unconfined_u:object_r:default_t:s0 index.html

만약에 이 디렉토리를 웹서버의 work dir로 생성을 한다? 
httpd_sys_content_t로 변경
[191]
자주 사용되는 옵션
semanage fcontext 명령

-a, --add 지정한 개체 유형의 기록 추가
 -d, --delete 지정한 개체 유형의 기록 삭제
 -l, --list 지정한 개체 유형의 기록 나열 
-t 컨텍스트
[root@servera ~]# # semanage fcontext -a -t httpd_sys_content_t '/virtual(/.*)?'
[root@servera ~]# restorecon -RFvv /virtual
Relabeled /virtual from unconfined_u:object_r:default_t:s0 to system_u:object_r:httpd_sys_content_t:s0
Relabeled /virtual/index.html from unconfined_u:object_r:default_t:s0 to system_u:object_r:httpd_sys_content_t:s0

[root@servera ~]#  semanage fcontext -l -C
SELinux fcontext                                   type               Context

/virtual(/.*)?                                     all files          system_u:object_r:httpd_sys_content_t:s0

7버전에서는 위 내용이 시험으로 출제

[196]
semanage boolean 
on / off 내용
서비스의 특정 기능 

[root@servera ~]# semanage boolean -l  | grep ftp
ftpd_anon_write                (off  ,  off)  Allow ftpd to anon write
[root@servera ~]# semanage boolean -l  | grep nfs
use_nfs_home_dirs              (off  ,  off)  Allow use to nfs home dirs

[root@servera ~]# semanage boolean -l | grep httpd_enable_homedirs
httpd_enable_homedirs          (off  ,  off)  Allow httpd to enable homedirs
[root@servera ~]# setsebool httpd_enable_homedirs on
[root@servera ~]# semanage boolean -l | grep httpd_enable_homedirs
httpd_enable_homedirs          (on   ,  off)  Allow httpd to enable homedirs
[root@servera ~]# setsebool -P httpd_enable_homedirs on[root@servera ~]# semanage boolean -l | grep httpd_enable_homedirs
httpd_enable_homedirs          (on   ,   on)  Allow httpd to enable homedirs

[root@servera ~]#  semanage boolean -l -C
SELinux boolean                State  Default Description

httpd_enable_homedirs          (on   ,   on)  Allow httpd to enable homedirs

시험을 보시는 분들은 다음 패키지를 기억
setroubleshoot-server

SELINUX에 대한 감사 파일
/var/log/audit
sealert -a /var/log/ audit/audit.log
[root@servera ~]# journalctl -f
[root@servera ~]# tail -f /var/log/messages

이전에는 6장에서 시험 문제를 출제
하지만 이제는 
시험문제를 여기서 출제 하지 않음.
시험 문제느 11장에서 나오기 때문에 

[340]
ebtables iptables nfnftables > firewall-cmd
 
firewalld
zone 


firewall-cmd을 사용하면 알아두어야 하는 옵션
--get-default-zone
--set-default-zone=ZONE


--add-interface=INTERFACE 
--add-service=SERVICE 
--add-port=PORT/PROTOCOL   *

--reload
 --permanent 방화벽의 경우 설정을 할때 두가지가 있음
runtime 설정 / 설정파일 내에 영구 설정permanent 옵션이 들어가야지만 영구설정으로 입력

permanent > realod 
두가지를 동시에 입력한다. –runtime-to-permanent
일정 시간만 사용한다 –timeout=XXX


SELinux 포트 레이블 지정 
네트워크

[355]
[root@servera ~]# 
[root@servera ~]# dnf -y remove httpd
먼저 사전 작업으로 웹 서버를 삭제 한뒤 실행 한다.
[student@workstation ~]$ lab start netsecurity-ports

servera에 패키지 설치 httpd
필요한 파일을 추가.
port : 80 > 82번으로 변경
현재 이 상태에서 82번 포트로 웹 서비스 하도록 하는게 목적

[student@workstation ~]$ ssh root@servera
[root@servera ~]# systemctl restart httpd
Job for httpd.service failed because the control process exited with error code.
See "systemctl status httpd.service" and "journalctl -xeu httpd.service" for details.

[root@servera ~]# systemctl status httpd.service
× httpd.service - The Apache HTTP Server
     Loaded: loaded (/usr/lib/systemd/system/httpd.service; disabled; vendor preset: disabled)
     Active: failed (Result: exit-code) since Fri 2024-07-26 02:12:30 EDT; 34s ago
       Docs: man:httpd.service(8)
    Process: 27816 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)
   Main PID: 27816 (code=exited, status=1/FAILURE)
     Status: "Reading configuration..."
        CPU: 30ms

Jul 26 02:12:30 servera.lab.example.com systemd[1]: Starting The Apache HTTP Server...
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: (13)Permission denied: AH00072: make_sock: could not bind to addres>
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: (13)Permission denied: AH00072: make_sock: could not bind to addres>
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: no listening sockets available, shutting down
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: AH00015: Unable to open logs
Jul 26 02:12:30 servera.lab.example.com systemd[1]: httpd.service: Main process exited, code=exited, status=1/FAILURE
Jul 26 02:12:30 servera.lab.example.com systemd[1]: httpd.service: Failed with result 'exit-code'.
Jul 26 02:12:30 servera.lab.example.com systemd[1]: Failed to start The Apache HTTP Server.

[root@servera ~]# ss -a | grep 82
nl    UNCONN 0      0                                          uevent:-541438252                     *            
nl    UNCONN 0      0                                          uevent:-1844827017                    *            
nl    UNCONN 0      0                                          uevent:-541438252                     *            
nl    UNCONN 0      0                                          uevent:-1844827017                    *            
u_str ESTAB  0      0                     /run/dbus/system_bus_socket 56882                         * 56881       
u_str ESTAB  0      0                                               * 56881                         * 56882       
u_dgr UNCONN 0      0                                               * 21082                         * 10210  

82번 자체를 사용하고 있어서 나는 에러는 아님
named 53 
named , chroot-named
정책에 의해서 나는 에러

[root@servera ~]# journalctl -xeu httpd.service
 Subject: A start job for unit httpd.service has begun execution
 Defined-By: systemd
 Support: https://access.redhat.com/support
 
 A start job for unit httpd.service has begun execution.
 
 The job identifier is 3227.
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: (13)Permission denied: AH00072: make_sock: could not bind to address [::]:82
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: (13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:82
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: no listening sockets available, shutting down
Jul 26 02:12:30 servera.lab.example.com httpd[27816]: AH00015: Unable to open logs
Jul 26 02:12:30 servera.lab.example.com systemd[1]: httpd.service: Main process exited, code=exited, status=1/FAILURE
 Subject: Unit process exited
 Defined-By: systemd
 Support: https://access.redhat.com/support
 
 An ExecStart= process belonging to unit httpd.service has exited.
 
 The process' exit code is 'exited' and its exit status is 1.
Jul 26 02:12:30 servera.lab.example.com systemd[1]: httpd.service: Failed with result 'exit-code'.
 Subject: Unit failed
 Defined-By: systemd
 Support: https://access.redhat.com/support
 
 The unit httpd.service has entered the 'failed' state with result 'exit-code'.
Jul 26 02:12:30 servera.lab.example.com systemd[1]: Failed to start The Apache HTTP Server.
 Subject: A start job for unit httpd.service has failed
 Defined-By: systemd
 Support: https://access.redhat.com/support
 
 A start job for unit httpd.service has finished with a failure.
 
 The job identifier is 3227 and the job result is failed.

실제 해결을 위해서 봐야 하는 파일 / 목록 / 내용

[root@servera ~]# systemctl restart httpd [term1]
[root@servera ~]# journalctl -f [term2]
Jul 26 02:19:18 servera.lab.example.com systemd[1]: Starting The Apache HTTP Server...
Jul 26 02:19:18 servera.lab.example.com httpd[27909]: (13)Permission denied: AH00072: make_sock: could not bind to address [::]:82
Jul 26 02:19:18 servera.lab.example.com httpd[27909]: (13)Permission denied: AH00072: make_sock: could not bind to address 0.0.0.0:82
Jul 26 02:19:18 servera.lab.example.com httpd[27909]: no listening sockets available, shutting down
Jul 26 02:19:18 servera.lab.example.com httpd[27909]: AH00015: Unable to open logs
Jul 26 02:19:18 servera.lab.example.com systemd[1]: Started dbus-:1.1-org.fedoraproject.Setroubleshootd@4.service.
Jul 26 02:19:18 servera.lab.example.com systemd[1]: httpd.service: Main process exited, code=exited, status=1/FAILURE
Jul 26 02:19:18 servera.lab.example.com systemd[1]: httpd.service: Failed with result 'exit-code'.
Jul 26 02:19:18 servera.lab.example.com systemd[1]: Failed to start The Apache HTTP Server.
Jul 26 02:19:19 servera.lab.example.com setroubleshoot[27910]: AnalyzeThread.run(): Cancel pending alarm
Jul 26 02:19:19 servera.lab.example.com systemd[1]: Started dbus-:1.1-org.fedoraproject.SetroubleshootPrivileged@3.service.
Jul 26 02:19:21 servera.lab.example.com setroubleshoot[27910]: SELinux is preventing /usr/sbin/httpd from name_bind access on the tcp_socket port 82. For complete SELinux messages run: sealert -l 750befb1-1702-452b-9ea4-2939e412a53c
Jul 26 02:19:21 servera.lab.example.com setroubleshoot[27910]: SELinux is preventing /usr/sbin/httpd from name_bind access on the tcp_socket port 82.
                                                               
                                                               *****  Plugin bind_ports (99.5 confidence) suggests   ************************
                                                               
                                                               If you want to allow /usr/sbin/httpd to bind to network port 82
                                                               Then you need to modify the port type.
                                                               Do
                                                               # semanage port -a -t PORT_TYPE -p tcp 82
                                                                   where PORT_TYPE is one of the following: http_cache_port_t, http_port_t, jboss_management_port_t, jboss_messaging_port_t, ntop_port_t, puppet_port_t.
                                                               
                                                               *****  Plugin catchall (1.49 confidence) suggests   **************************
                                                               
                                                               If you believe that httpd should be allowed name_bind access on the port 82 tcp_socket by default.
                                                               Then you should report this as a bug.
                                                               You can generate a local policy module to allow this access.
                                                               Do
                                                               allow this access for now by executing:
                                                               # ausearch -c 'httpd' --raw | audit2allow -M my-httpd
                                                               # semodule -X 300 -i my-httpd.pp
                                                               
Jul 26 02:19:21 servera.lab.example.com setroubleshoot[27910]: AnalyzeThread.run(): Set alarm timeout to 10
Jul 26 02:19:21 servera.lab.example.com setroubleshoot[27910]: AnalyzeThread.run(): Cancel pending alarm
Jul 26 02:19:23 servera.lab.example.com setroubleshoot[27910]: SELinux is preventing /usr/sbin/httpd from name_bind access on the tcp_socket port 82. For complete SELinux messages run: sealert -l 750befb1-1702-452b-9ea4-2939e412a53c
Jul 26 02:19:23 servera.lab.example.com setroubleshoot[27910]: SELinux is preventing /usr/sbin/httpd from name_bind access on the tcp_socket port 82.
                                                               
                                                               *****  Plugin bind_ports (99.5 confidence) suggests   ************************
                                                               
                                                               If you want to allow /usr/sbin/httpd to bind to network port 82
                                                               Then you need to modify the port type.
                                                               Do
                                                               # semanage port -a -t PORT_TYPE -p tcp 82
                                                                   where PORT_TYPE is one of the following: http_cache_port_t, http_port_t, jboss_management_port_t, jboss_messaging_port_t, ntop_port_t, puppet_port_t.
                                                               
                                                               *****  Plugin catchall (1.49 confidence) suggests   **************************
                                                               
                                                               If you believe that httpd should be allowed name_bind access on the port 82 tcp_socket by default.
                                                               Then you should report this as a bug.
                                                               You can generate a local policy module to allow this access.
                                                               Do
                                                               allow this access for now by executing:
                                                               # ausearch -c 'httpd' --raw | audit2allow -M my-httpd
                                                               # semodule -X 300 -i my-httpd.pp



[root@servera ~]# sealert -l 750befb1-1702-452b-9ea4-2939e412a53c
SELinux is preventing /usr/sbin/httpd from name_bind access on the tcp_socket port 82.

*****  Plugin bind_ports (99.5 confidence) suggests   ************************

If you want to allow /usr/sbin/httpd to bind to network port 82
Then you need to modify the port type.
Do
# semanage port -a -t PORT_TYPE -p tcp 82
    where PORT_TYPE is one of the following: http_cache_port_t, http_port_t, jboss_management_port_t, jboss_messaging_port_t, ntop_port_t, puppet_port_t.

*****  Plugin catchall (1.49 confidence) suggests   **************************

If you believe that httpd should be allowed name_bind access on the port 82 tcp_socket by default.
Then you should report this as a bug.
You can generate a local policy module to allow this access.
Do
allow this access for now by executing:
# ausearch -c 'httpd' --raw | audit2allow -M my-httpd
# semodule -X 300 -i my-httpd.pp


Additional Information:
Source Context                system_u:system_r:httpd_t:s0
Target Context                system_u:object_r:reserved_port_t:s0
Target Objects                port 82 [ tcp_socket ]
Source                        httpd
Source Path                   /usr/sbin/httpd
Port                          82
Host                          servera.lab.example.com
Source RPM Packages           httpd-2.4.51-7.el9_0.x86_64
Target RPM Packages           
SELinux Policy RPM            selinux-policy-targeted-34.1.29-1.el9_0.noarch
Local Policy RPM              selinux-policy-targeted-34.1.29-1.el9_0.noarch
Selinux Enabled               True
Policy Type                   targeted
Enforcing Mode                Enforcing
Host Name                     servera.lab.example.com
Platform                      Linux servera.lab.example.com
                              5.14.0-70.13.1.el9_0.x86_64 #1 SMP PREEMPT Thu Apr
                              14 12:42:38 EDT 2022 x86_64 x86_64
Alert Count                   6
First Seen                    2024-07-26 02:12:30 EDT
Last Seen                     2024-07-26 02:19:18 EDT
Local ID                      750befb1-1702-452b-9ea4-2939e412a53c

Raw Audit Messages
type=AVC msg=audit(1721974758.633:474): avc:  denied  { name_bind } for  pid=27909 comm="httpd" src=82 scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:reserved_port_t:s0 tclass=tcp_socket permissive=0


type=SYSCALL msg=audit(1721974758.633:474): arch=x86_64 syscall=bind success=no exit=EACCES a0=3 a1=55b87fc538b8 a2=10 a3=7ffddfb15b8c items=0 ppid=1 pid=27909 auid=4294967295 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=4294967295 comm=httpd exe=/usr/sbin/httpd subj=system_u:system_r:httpd_t:s0 key=(null)

Hash: httpd,httpd_t,reserved_port_t,tcp_socket,name_bind

[root@servera ~]# cat /var/log/messages 
Jul 26 02:19:23 servera setroubleshoot[27910]: SELinux is preventing /usr/sbin/httpd from name_bind access on the tcp_socket port 82. For complete SELinux messages run: sealert -l 750befb1-1702-452b-9ea4-2939e412a53c
Jul 26 02:19:23 servera setroubleshoot[27910]: SELinux is preventing /usr/sbin/httpd from name_bind access on the tcp_socket port 82.#012#012*****  Plugin bind_ports (99.5 confidence) suggests   ************************#012#012If you want to allow /usr/sbin/httpd to bind to network port 82#012Then you need to modify the port type.#012Do#012# semanage port -a -t PORT_TYPE -p tcp 82#012    where PORT_TYPE is one of the following: http_cache_port_t, http_port_t, jboss_management_port_t, jboss_messaging_port_t, ntop_port_t, puppet_port_t.#012#012*****  Plugin catchall (1.49 confidence) suggests   **************************#012#012If you believe that httpd should be allowed name_bind access on the port 82 tcp_socket by default.#012Then you should report this as a bug.#012You can generate a local policy module to allow this access.#012Do#012allow this access for now by executing:#012# ausearch -c 'httpd' --raw | audit2allow -M my-httpd#012# semodule -X 300 -i my-httpd.pp#012
Jul 26 02:19:31 servera systemd[1]: dbus-:1.1-org.fedoraproject.SetroubleshootPrivileged@3.service: Main process exited, code=killed, status=14/ALRM

[root@servera ~]# cd /var/log/audit/
[root@servera audit]# sealert -a  /var/log/audit/audit.log   < 시험에서는 이거! 

	




[root@servera ~]# semanage port -l | grep -w 80
http_port_t                    tcp      80, 81, 443, 488, 8008, 8009, 8443, 9000

[root@servera audit]# semanage port -a -t http_port_t  -p tcp 82   < 수정 + 추가를 했음
[root@servera audit]# semanage port -l | grep ^http_port_t
http_port_t                    tcp      82, 80, 81, 443, 488, 8008, 8009, 8443, 9000

[root@servera conf.d]# cd /etc/httpd
[root@servera httpd]# ls
conf  conf.d  conf.modules.d  logs  modules  run  state
[root@servera httpd]# cd conf
[root@servera conf]# ls
httpd.conf  magic
[root@servera conf]# cat httpd.conf  | grep 82
Listen 82


[root@servera conf]# systemctl restart httpd 
[root@servera conf]# systemctl enable --now httpd

[root@servera conf]# firewall-cmd --permanent --add-port=82/tcp
success
[root@servera conf]#  firewall-cmd --reload
success
[root@servera conf]# exit
logout
Connection to servera closed.
[student@workstation ~]$ curl http://servera
curl: (7) Failed to connect to servera port 80: Connection refused
[student@workstation ~]$ curl http://servera:82
Hello

[student@workstation ~]$  lab finish netsecurity-ports


[355] 연습문제 
45분

컨테이너 
podman ..

docker는 더 이상 사용하지 않음. docker가 미란티스라는 회사에 인수가 된 이후, 개발 방향을 변경.
EE CE 

docker는 도커 하위 계층인 containerd를 분리. 현재 쿠버네티스 및 CNCF 표준 런타임. 쿠버네티스는 CRI표준 기반의 CRI-O런타임 사용. containerd, cri-docker를 사용할 수 도 있음.

학습 시, 권장하는 방법은 docker가 아닌, podman기반으로 학습. 기존 도커에서 사용하던 명령어 및 컨테이너 이미지 파일은 현재 OCI사양에서 표준 이미지 및 명령어로 채택. 현재 레드햇 리눅스 및 다른 상용 배포판 그리고 커뮤니티 배포판은 docker가 아닌 podman으로 권장.

https://people.redhat.com/tmichett/do180/podman_basics.pdf
https://developers.redhat.com/cheat-sheets/podman-cheat-sheet

podman  컨테이너 및 컨테이너 이미지를 관리     <<< 
buildah 컨테이너를 생성 / 컨테이너 이미지를 빌드
skopeo  이미지를 검사. 복사. 삭제 이미지 서명 업로드 다운로드  이미지스트림

podman command

podman build 	컨테이너 파일을 사용하여 컨테이너 이미지를 빌드합니다. *
podman run 	  새 컨테이너에서 명령을 실행합니다. *
podman images 로컬 스토리지의 이미지를 나열합니다. *
podman ps 	  컨테이너에 대한 정보를 출력합니다. *
podman inspect컨테이너, 이미지, 볼륨, 네트워크 또는 포드의 구성을 표시합니다.
podman pull 	레지스트리에서 이미지를 다운로드합니다. *
podman cp 	  컨테이너와 로컬 파일 시스템 간에 파일 또는 폴더를 복사합니다.
podman exec 	실행 중인 컨테이너에서 명령을 실행합니다.
podman rm 	  하나 이상의 컨테이너를 제거합니다. *
podman rmi 	  로컬에 저장된 하나 이상의 이미지를 제거합니다. *
podman search 레지스트리에서 이미지를 검색합니다. *

이미지를 다운로드
1 빌드를 통해 이미지를 생성
2 컨테이너를 서비스로 변환

실제 컨테이너 학습은 이걸로 끝나는게 절대 아님
[root@node1 ~]# dnf info container-tools
Last metadata expiration check: 0:03:21 ago on Thu 15 Jun 2023 02:29:25 PM KST.
Available Packages
Name         : container-tools
Version      : 1
Release      : 12.el9
Architecture : noarch
Size         : 7.9 k
Source       : container-tools-1-12.el9.src.rpm
Repository   : appstream
Summary      : A meta-package witch container tools such as podman, buildah, skopeo, etc.
License      : MIT
Description  : Latest versions of podman, buildah, skopeo, runc, conmon, CRIU, Udica, etc as
             : well as dependencies such as container-selinux built and tested together, and
             : updated.

[root@node1 ~]# dnf -y install container-tools
Rootless 및 Rootful 컨테이너
Rootless podman은 root를 사용하지 않고 일반 계정으로 구성한다.
Rootful 하지만 특수한 경우에는 root 계정으로 구성하기도 한다. 

시험 / 일반적인 경우에는 사용자 계정을 따로 생성
[root@node1 ~]# adduser containeruser
[root@node1 ~]# echo 'redhat' | passwd --stdin containeruser

[root@servera ~]# su - containeruser 
[containeruser@servera ~]$ podman ps
WARN[0000] The cgroupv2 manager is set to systemd but there is no systemd user session available 
WARN[0000] For using systemd, you may need to login using an user session 
WARN[0000] Alternatively, you can enable lingering with: `loginctl enable-linger 1002` (possibly as root) 
WARN[0000] Falling back to --cgroup-manager=cgroupfs    
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
WARN[0000] Failed to add pause process to systemd sandbox cgroup: exec: "dbus-launch": executable file not found in $PATH 
컨테이너 계정으로 전환 하는 경우에는 su -  안됨
ssh 로 완전히 변경을 해야 한다.

[root@node1 ~]# ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Created directory '/root/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa
Your public key has been saved in /root/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:+48RdMxOhqHIyFGsLG3FbVjuad1akaXBjwTtIq4v27U root@node1.example.com
The key's randomart image is:
+---[RSA 3072]----+
|    .+.+. o+. .  |
|   . +=oo. =+=   |
|   oooo.o ooXo   |
|  . =  ..+.*o..  |
|   o   .S.o.+    |
|       ... +     |
|       .. +      |
|      o. o +     |
|      .+o E..    |
+----[SHA256]-----+

옵션을 사용하여 한줄에서 작업하기
[root@node1 ~]# ssh-keygen -f /root/.ssh/id_rsa -N '' -t rsa -b 4096

[root@node1 ~]# ssh-copy-id containeruser@localhost
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"
The authenticity of host 'localhost (::1)' can't be established.
ED25519 key fingerprint is SHA256:rsL1a8yqXPflsuROVvN8g3dhiyrbhQtHYQejQ416mTs.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
containeruser@localhost's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'containeruser@localhost'"
and check to make sure that only the key(s) you wanted were added.
[root@node1 ~]# ssh-copy-id -f containeruser@servera

[root@node1 ~]# ssh containeruser@localhost
containeruser 계정으로 컨테이너를 생성 / 빌드 / 서비스 구성


[containeruser@servera ~]$ mkdir -p /home/containeruser/.config/containers
[containeruser@servera ~]$ cd /home/containeruser/.config/containers
[containeruser@servera containers]$ vi registries.conf
[containeruser@servera containers]$ cat registries.conf 
unqualified-search-registries = ['registry.lab.example.com']
[[registry]]
location = "registry.lab.example.com"
insecure = true
blocked = false

[containeruser@servera containers]$ podman info
…
registries:
  registry.lab.example.com:
    Blocked: false
    Insecure: true
[containeruser@servera containers]$ 

[containeruser@servera ~]$ podman login  registry.lab.example.com
Username: admin
Password:  redhat
Login Succeeded!

[containeruser@servera ~]$ podman search  registry.lab.example.com
[containeruser@servera ~]$ podman search  registry.lab.example.com/
NAME                                        DESCRIPTION
registry.lab.example.com/rhel8/mariadb-103  
registry.lab.example.com/rhel8/mariadb-105  
registry.lab.example.com/rhel8/httpd-24     
registry.lab.example.com/library/nginx      
registry.lab.example.com/ubi7/ubi           
registry.lab.example.com/ubi8/ubi           
registry.lab.example.com/ubi9-beta/ubi      
registry.lab.example.com/ubi8/python-38     
registry.lab.example.com/ubi8/httpd-24      
registry.lab.example.com/rhel8/php-74  

registry.lab.example.com/ubi8/ubi   
—-----------------------------  —---  —--
                 1                       2      3
1 registry URL
2 NS
3 image name
imagename:tag

[containeruser@servera ~]$ skopeo inspect  docker://registry.lab.example.com/ubi8/python-38
{
    "Name": "registry.lab.example.com/ubi8/python-38",
    "Digest": "sha256:11b226f92a0036bcd330f6d5d3699e289481cf55a1466d81681024f90c72e4da",
    "RepoTags": [
        "latest"
    ],
    "Created": "2022-05-04T00:57:58.876112Z",
    "DockerVersion": "1.13.1",
    "Labels": {
        "architecture": "x86_64",
        "build-date": "2022-05-04T00:56:07.193475",
        "com.redhat.build-host": "cpt-1006.osbs.prod.upshift.rdu2.redhat.com",
        "com.redhat.component": "python-38-container",
        "com.redhat.license_terms": "https://www.redhat.com/en/about/red-hat-end-user-license-agreements#UBI",
        "description": "Python 3.8 available as container is a base platform for building and running various Python 3.8 applications and frameworks. Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python's elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on most platforms.",
        "distribution-scope": "public",
        "io.buildpacks.stack.id": "com.redhat.stacks.ubi8-python-38",
        "io.k8s.description": "Python 3.8 available as container is a base platform for building and running various Python 3.8 applications and frameworks. Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python's elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on most platforms.",
        "io.k8s.display-name": "Python 3.8",
        "io.openshift.expose-services": "8080:http",
        "io.openshift.s2i.scripts-url": "image:///usr/libexec/s2i",
        "io.openshift.tags": "builder,python,python38,python-38,rh-python38",
        "io.s2i.scripts-url": "image:///usr/libexec/s2i",
        "maintainer": "SoftwareCollections.org \u003csclorg@redhat.com\u003e",
        "name": "ubi8/python-38",
        "release": "96",
        "summary": "Platform for building and running Python 3.8 applications",
        "url": "https://access.redhat.com/containers/#/registry.access.redhat.com/ubi8/python-38/images/1-96",
        "usage": "s2i build https://github.com/sclorg/s2i-python-container.git --context-dir=3.8/test/setup-test-app/ ubi8/python-38 python-sample-app",
        "vcs-ref": "c5014a480dc3ca5c7c0a897ea5dc9b0abc878fb2",
        "vcs-type": "git",
        "vendor": "Red Hat, Inc.",
        "version": "1"
    },
    "Architecture": "amd64",
    "Os": "linux",
    "Layers": [
        "sha256:1a6ae3d35ced1c7654b3bf1a66b8a513d2ee7f497728e0c5c74841807c4b8e77",
        "sha256:6521843dd4764da8f455aa0734384e4e056cab523fba813770cda18edf0e32c3",
        "sha256:695b0761165a474d8d92953fa3e2aa9482ae1484c834279cf64fe5294d3ba147",
        "sha256:05534c4e53fa9bd2d41f08f2187596ad6193228dfa60026f6f6ef85ee987bf3e",
        "sha256:1786b78d0fea9cc457d793dc75e6d8d460e35aadcb78dacb6035b09468bb4cde"
    ],
    "Env": [
        "PATH=/opt/app-root/src/.local/bin/:/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",
        "container=oci",
        "SUMMARY=Platform for building and running Python 3.8 applications",
        "DESCRIPTION=Python 3.8 available as container is a base platform for building and running various Python 3.8 applications and frameworks. Python is an easy to learn, powerful programming language. It has efficient high-level data structures and a simple but effective approach to object-oriented programming. Python's elegant syntax and dynamic typing, together with its interpreted nature, make it an ideal language for scripting and rapid application development in many areas on most platforms.",
        "STI_SCRIPTS_URL=image:///usr/libexec/s2i",
        "STI_SCRIPTS_PATH=/usr/libexec/s2i",
        "APP_ROOT=/opt/app-root",
        "HOME=/opt/app-root/src",
        "PLATFORM=el8",
        "NODEJS_VER=14",
        "PYTHON_VERSION=3.8",
        "PYTHONUNBUFFERED=1",
        "PYTHONIOENCODING=UTF-8",
        "LC_ALL=en_US.UTF-8",
        "LANG=en_US.UTF-8",
        "CNB_STACK_ID=com.redhat.stacks.ubi8-python-38",
        "CNB_USER_ID=1001",
        "CNB_GROUP_ID=0",
        "PIP_NO_CACHE_DIR=off",
        "BASH_ENV=/opt/app-root/etc/scl_enable",
        "ENV=/opt/app-root/etc/scl_enable",
        "PROMPT_COMMAND=. /opt/app-root/etc/scl_enable"
    ]
}



[containeruser@servera ~]$ podman pull  registry.lab.example.com/ubi8/python-38
Trying to pull registry.lab.example.com/ubi8/python-38:latest...
Getting image source signatures
Copying blob 05534c4e53fa done  
Copying blob 1a6ae3d35ced done  
Copying blob 695b0761165a done  
Copying blob 6521843dd476 done  
Copying blob 1786b78d0fea done  
Copying config b335f517d3 done  
Writing manifest to image destination
Storing signatures
b335f517d3b508a101a197b926bc501f2139112269e8b7432ba304a21b81343f

[containeruser@servera ~]$ podman images
REPOSITORY                               TAG         IMAGE ID      CREATED      SIZE
registry.lab.example.com/ubi8/python-38  latest      b335f517d3b5  2 years ago  892 MB
[containeruser@servera ~]$ exit
logout
Connection to localhost closed.
[root@servera ~]# podman images
REPOSITORY  TAG         IMAGE ID    CREATED     SIZE

컨테이너는 실행시 프로세스가 할 일을 다하면 끝
[containeruser@servera ~]$ podman run -d --name python38 registry.lab.example.com/ubi8/python-38251b3ed60319051ea89fc7ce47bcd70d59967cf65ffbaa23be600c3f161f330b
[containeruser@servera ~]$ podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[containeruser@servera ~]$ podman ps -a
CONTAINER ID  IMAGE                                           COMMAND               CREATED        STATUS                    PORTS       NAMES
251b3ed60319  registry.lab.example.com/ubi8/python-38:latest  /bin/sh -c $STI_S...  8 seconds ago  Exited (0) 9 seconds ago              python38

[containeruser@servera ~]$ podman run -d --name python  --rm registry.lab.example.com/ubi8/python-38 
7ea85e2f1644480b72494b8998dc2d52aad429021386639dcd18ea0183bbf99b
[containeruser@servera ~]$ podman ps -a
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES


[containeruser@servera ~]$ podman run -d --name python  --rm registry.lab.example.com/ubi8/python-38  sleep infinity
3d1716f5130d1b2eba369f33a8ecfb55620ba89c8b091c98747f2caf64f51141
[containeruser@servera ~]$ podman ps
CONTAINER ID  IMAGE                                           COMMAND         CREATED        STATUS            PORTS       NAMES
3d1716f5130d  registry.lab.example.com/ubi8/python-38:latest  sleep infinity  2 seconds ago  Up 3 seconds ago              python

[containeruser@servera ~]$ podman stop python
WARN[0010] StopSignal SIGTERM failed to stop container python in 10 seconds, resorting to SIGKILL 
python
[containeruser@servera ~]$ podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[containeruser@servera ~]$ podman ps -a
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES

podman search registry.lab.example.com/
podman pull registry.lab.example.com/rhel8/httpd-24 

[containeruser@servera ~]$ podman run --name web --rm -d registry.lab.example.com/rhel8/httpd-24  
–name 컨테이너의 이름을 지정
–rm 컨테이너 정지, 완료시 삭제
-d 백그라운드 모드 실행

[containeruser@servera ~]$ podman exec -it web bash
exec 이미 실행된 컨테이너에 추가 명령을 내릴때 사용
-it 응답형 쉘을 실행 할때 터미널을 실행


podman cp 
내부 외부 파일 복사 가능 * tar 로 복사를 사용
컨테이너 내부 패키지에 tar가 있어야 함

$ mkdir ~/webfile
# echo ‘container web’ > ~/webfile/index.html

-v [hostdir]:[containerdir]
-p [hostport]:[containerdir]

.config/systemd/user
[containeruser@servera ~]$ podman stop web
추가 적인 옵션들을 사용

[containeruser@servera ~]$ mkdir ~/webfile
[containeruser@servera ~]$ echo ‘container web’ > ~/webfile/index.html
 podman run --name web --rm -d registry.lab.example.com/rhel8/httpd-24 
추가 옵션들을 사용해보자.
-v 볼륨 호스트의 파일을 내부 컨테이너에서 마운트 시키기 위해 사용
읽기는 아주 강함 하지만 쓰기가 작업되면서 부터는 느려지기 시작하기 때문에 데이터를 보관 하는 목적 + 성능을 위해서 볼륨 마운트 기능을 사용
-v [hostdir]:[containerdir]
-p 포트 컨테이너는 기본적으로 격리 상태 외부와 연결을 시켜줘야 한다.
-p [hostport]:[containeport-8080]

 podman run --name web --rm -d -v /home/containeruser/webfile:/var/www/html -p 8080:8080 registry.lab.example.com/rhel8/httpd-24 
[containeruser@servera ~]$  podman run --name web --rm -d -v /home/containeruser/webfile:/var/www/html -p 8080:8080 registry.lab.example.com/rhel8/httpd-24 
[containeruser@servera ~]$  podman run --name web --rm -d -v /home/containeruser/webfile:/var/www/html -p 8080:8080 registry.lab.example.com/rhel8/httpd-24 
[containeruser@servera ~]$ curl http://localhost:8080
정상적으로 나오지 않는다.
[containeruser@servera ~]$ ls
webfile
[containeruser@servera ~]$ cat webfile/index.html 
‘container web’
[containeruser@ser

이유: [containeruser@servera ~]$ ls -RlZ webfile/
webfile/:
total 4
-rw-r--r--. 1 containeruser containeruser unconfined_u:object_r:user_home_t:s0 20 Jul 26 04:13 index.html

container도 selinux의 영향을 받는다. 그런데 user_home_t로 설정되어 있기 때문에 접근이 불가능하다. 이를 교정해야 한다.

SELINUX를 위해서 Z옵션을 볼륨에 지원
[containeruser@servera ~]$  podman run --name web --rm -d -v /home/containeruser/webfile:/var/www/html:Z -p 8080:8080 registry.lab.example.com/rhel8/httpd-24 
[containeruser@servera ~]$ ls -RlZ webfile/webfile/:total 4
-rw-r--r--. 1 containeruser containeruser system_u:object_r:container_file_t:s0:c354,c986 20 Jul 26 04:13 index.html
[containeruser@servera ~]$ curl http://localhost:8080
‘container web’
정상!

단점: 호스트가 부팅되면 수동으로 할때마다 기동시켜줘야한다.
컨테이너를 서비스로 구성

.config/systemd/user

[containeruser@servera ~]$ mkdir -p ~/.config/systemd/user
cd esc+.
[containeruser@servera ~]$ cd ~/.config/systemd/user

--new systemd에서 컨테이너를 구동하다가 종료시 컨테이너를 삭제
--files 해당 옵션이 없으면 표준 출력으로 내보내기를 시도 있다면 파일로 현재 위치에 생성

[containeruser@servera user]$ podman generate systemd --name web --files --new 
/home/containeruser/.config/systemd/user/container-web.service
[containeruser@servera user]$ ls
container-web.service

파일을 등록 -> systemd

[containeruser@servera user]$ id
uid=1002(containeruser) gid=1002(containeruser) groups=1002(containeruser) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023

systemctl 명령 사용시 –user 붙여서 사용
[containeruser@servera ~]$ systemctl --user daemon-reload
^[[3~[containeruser@servera ~]$ podman stop -a
52e3bb8badaa3fd54268fca70e766f88288807b23a076d348acfc9217cfe1f66
[containeruser@servera ~]$ podman ps -a
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES

[containeruser@servera ~]$ systemctl --user enable --now container-web.service 
Created symlink /home/containeruser/.config/systemd/user/default.target.wants/container-web.service → /home/containeruser/.config/systemd/user/container-web.service.
[containeruser@servera ~]$ podman stop -a
57121f62d065260218b1534dad2464e10a58a363380ad6893a657930bd11abff
[containeruser@servera ~]$ podman ps 
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[containeruser@servera ~]$ podman ps 
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[containeruser@servera ~]$ podman ps ^C
[containeruser@servera ~]$ systemctl --user restart container-web.service 
[containeruser@servera ~]$ podman ps CONTAINER ID  IMAGE                                           COMMAND               CREATED       STATUS            PORTS                   NAMES
36d91f9b23b5  registry.lab.example.com/rhel8/httpd-24:latest  /usr/bin/run-http...  1 second ago  Up 2 seconds ago  0.0.0.0:8080->8080/tcp  web

systemd 관리하에 있기 때문에 해당 컨테이너의 경우에는 직접 podman 명령으로 컨트롤 하지말고 systemctl –user 명령으로 두어야 한다.

reboot 이후
# curl http://localhost:8080

[root@servera ~]# curl http://localhost:8080
curl: (7) Failed to connect to localhost port 8080: Connection refused
[root@servera ~]# ssh containeruser@localhost
Register this system with Red Hat Insights: insights-client --register
Create an account or view all your systems at https://red.ht/insights-dashboard
Last login: Fri Jul 26 03:34:02 2024 from ::1
[containeruser@servera ~]$ curl http://localhost:8080
‘container web’

사용자 레벨에서 서비스를 생성 하게 되면
해당 사용자의 프로세스가 하나라도 존재 해야 서비스가 실행

[containeruser@servera ~]$ loginctl enable-linger 
[containeruser@servera ~]$ loginctl show-user containeruser 
UID=1002
GID=1002
Name=containeruser
Timestamp=Fri 2024-07-26 04:35:00 EDT
TimestampMonotonic=34722949
RuntimePath=/run/user/1002
Service=user@1002.service
Slice=user-1002.slice
Display=3
State=active
Sessions=3
IdleHint=no
IdleSinceHint=1721982968835799
IdleSinceHintMonotonic=102846324
Linger=yes
Linger기능이 활성화 되어 있으면 프로세스가 하나라도 있어야 하는 제한이 없어짐
[containeruser@servera ~]$ exit
logout
Connection to localhost closed.
[root@servera ~]# reboot
Connection to servera closed by remote host.
Connection to servera closed.

[root@servera ~]# curl http://localhost:8080
‘container web’

컨테이너 빌드

[containeruser@servera ~]$  podman login registry.lab.example.com
Username: admin
Password: 
Login Succeeded!

[containeruser@servera ~]$ mkdir work
[containeruser@servera ~]$ cd work/

[containeruser@servera work]$ vi Containerfile
FROM registry.lab.example.com/ubi8/ubi
RUN dnf list ; dnf -y install httpd
CMD  ["/bin/bash", "-c", "sleep infinity"] 

[containeruser@servera work]$ podman build -t sleep:1.0 .
[containeruser@servera work]$ podman images
REPOSITORY                               TAG         IMAGE ID      CREATED        SIZE
localhost/sleep                          1.0         c30bce277aae  5 seconds ago  266 MB
registry.lab.example.com/ubi8/python-38  latest      b335f517d3b5  2 years ago    892 MB
registry.lab.example.com/rhel8/httpd-24  latest      630b2035aad8  2 years ago    470 MB
registry.lab.example.com/ubi8/ubi        latest      fca12da1dc30  2 years ago    235 MB

시험에서는 containerfile을 웹에서 제공
wget 명령을 이용하여 파일을 다운로드 받아야 한다! * 

순서:
Containerfile을 다운로드
이미지 빌드
이미지 run
서비스 파일 생성
서비스 파일 활성화
부팅시에도 계속 컨테이너가 돌 수 있도록 한다.


도커 
총 40시간
점심시간:  13:00- 14:00 입니다.
입실 퇴실 QR체크 잘 해주시기 바랍니다.

8시30? 8시 45분 오픈

도커 

파일 vm     centos
v-lab  rhv  RHEL 

windows / mac 
mac m1 m2 

링크

1. 파이어폭스 설치

2. 링크 
centos - 7 ver VM file

vmware workstation  win

vmware fusion mac 
https://con.dustbox.kr/ 접근
다운로드-> CA 인증서
pki-resource 다운로드
파이어폭스 설정

개인 정보 및 보안
인증서 -> 인증서 보기
가져오기 -> pki-resource  추가 
------------
가상머신 포털
로그인 -> 비밀번호 변경
가상머신 실행
가상머신 이름 선택
대시보드 진입
콘솔 > 콘솔 웹브라우저

root
centos
복사 , 붙여넣기
redhat < 

사용자 계정이 비활성화 또는 잠겨있기 때문에 로그인 할 수 없습니다. 시스템 관리자에게 문의하십시오. 

docker21 접근불가

docker18 
docker1
docker
vm 파일 다운로드? 
windows vmware workstation 
설치 

vcpu4
vmem 2G 
network nat 

10:10 
docker 
rhv -> 
docker# / docker
root / centos

vm
root / centos

rhv 의 경우에는 외부 접근 콘솔이 비활성화

rhv 사용하시는 분들은 스냅샷 

기본 소개. overview
설치, 컨테이너 구동
* 명령어 사용 및 옵션 
컨테이너 이미지
컨테이너 네트워크
컨테이너 볼륨
* dockerfile

컨테이너는 무엇이고 어디다 쓰느냐 * 

베어-메탈, 가상화, 컨테이너,

[개발환경, 인프라 ]동일한 환경을 구성

어느 환경에서건 사용에 있어 동일한 세팅값을 유지할수 있도록 하는 것이 포인트 

가상화보다 가볍게 사용하고 싶다.
+ 관리 체계를 좀더 심플하게 가져가고 싶다.
+ 플랫폼에 변화에 민감하게 반응하지 않는 형태를 유지하고 싶다.

질문: 도커는 인프라팀(HW/OS)에서 관리하는게 맞는지? 아니면 사용하는 application 이나 middleware (개발부서)에서 관리하는게 맞는지? 

[보통]의 경우에는 개발부서에서 진행하는 경우가 많다.
[일반]적인 경우에는 인프라팀& 개발부서 
= 인프라팀& 개발부서 에서 사용하는 사람이 관리

 경계선을 어느 선에서 그을것인가?

[middle/ app]
docker
OS
H/W

클러스터
인프라팀& 개발부서 
어떤 팀이, 접근을 더 많이 하느냐

openshift [OKD] 미들웨어 
kubernetes [K8S]  관리하기 위한 오케스트레이션 툴 
Container [docker] 프로세스화 관리체계의 일원화가 힘듬

docker 
ce
ee

OCI / CRI-O   runc podman 
RHEL7 2025 
RHEL8 2021.12.31 종료
RHEL9 


설치
docker 검색만 해도 가능

8 docker 검색해서는 불가능

프로그램
코드
+ 프로그램이 구동될 버전.
OS마다 필요한 라이브러리
설정

개발자 
프로그램 개발 
테스트 
실 서버 -> 빌드& 테스트

컨테이너 이미지만 정상적으로 만들어 진다면 
어느 서버에서건 구동이 가능하다!
불변성
변하지 않는다.
docker hub > 1.1 

volume  > 추가적으로 붙이는 이유

불변의 컨테이너 이미지 + 추가하는 데이터 

docker = podman 

명령어 

DIR [R/W]
- A file
- B file
- C file

DIR.iso [R/O]
- A file
- B file
- C file

빌드 기능을 통하여 내가 원하는 이미지를 생성가능하다.
공유 기능을 이용하여 생성한 이미지를 업로드 / 생성되어 있는 이미지를 다운로드 하여 사용
이미지를 실제 사용하기 위해서 구동하고, 삭제, 검색, commit.. 여러 가지 기능들을 docker 하위 명령어로 가지고 있다.

리눅스에서 docker 설치하기
7 설치를 위해 yum 자체에서도 구성이 가능
8->  docker 검색을 해도 안나옵니다.
# yum list
# yum list | grep docker 
# rpm -qa | grep docker
# systemctl status docker 
# systemctl enable --now docker
# yum -y install bash-completion
^d
root
centos
docker [tab] [tab]

컨테이너의 생애 주기
명령어.
옵션
2:00 에 시작을 하도록 하겠습니다.


root /centos 


docker 
설치 / docker? 

모놀리
하나의 애플리케이션 내에 여러 서비스를 모두 구성하여 사용하는 방식
장점 - 구성만 잘 된다면 배포가 쉽다.
단점 - 한곳의 버그, 장애가 나머지에 영향을 끼치기가 쉽다.
서비스의 사이즈가 커질수록 사용해야 하는 서버의 성능또한 올라가야 한다.
수정사항이 하나라도 잇는 경우 전체 재배포를 해야 하는 상황이 발생한다.

업데이트를 하기 위해 코드를 변경 > 빌드 >  테스트


모놀리 -> 단 하나의 서비스로 구성

머신마다 나눠보자.
VM - service1 웹
VM - service2  DB

전체 업데이트가 아니라 약간의 모듈화 구성


마이크로 서비스 아키텍처

나눌수있는 부분까지 다 나눠보자.

조각을 최대한 낸다.
개발의 단위가 최소화 된다. 

단점 - 
네트워크 

개발자도 네트워크에 어느정도 지식이 많아져야 하는 시대

MSA  기본인 시대

msa 개념

컨테이너의 생명 주기!

저장소 : 만들어진 이미지가 저장되어 있으며 또는 사용자가 생성한 이미지를 업로드 / 로그인 기능 
이미지 : 실제 파일 
컨테이너 : 프로세스 


저장소------이미지----------컨테이너

pull    : 저장소에서 이미지를 로컬로 가져오는것
push  : 로컬에서 이미지를 저장소로 내보내는것 
run     : 이미지를 컨테이너로 실행, 사용하려는 이미지가 없다면 다운로드 
search : 저장소에서 사용할 이미지를 검색
stop : 컨테이너의 정지
rm    : 생성된 컨테이너 삭제
rmi   : 이미지 삭제
create : 이미지 생성
commit : 이미지 고정 

도커가 설치되어 있으며 도커허브로 접근이 가능하도록 네트워크 설정도 완료
centos7 

컨테이너에서 사용되는 명령어. 옵션 . 옵션에 대한 기능.

# docker images : 도커에서 사용중인 이미지 목록을 출력
# docker history [이미지명] : 이미지의 변경 이력을 확인 가능
# docker login : 도커허브에 접근 할 수 있도록 ID/PASS를 입력받음
# docker login [SERVER]: 개인 저장소의 위치로 접근 하는 경우에는 [SERVER]까지 입력한다.
# docker pull : 저장소에서 이미지 가져오기
# docker push  : 저장소로 이미지 내보내기
# docker search : 저장소에 존재하는 이미지를 검색
# docker build -t [image:tag] : Dockerfile 기반으로 이미지를 생성
# docker build -t [image:tag] -f Dockerfile : 사용하려는 파일이름이 Dockerfile이 아닌 경우에 -f 옵션으로 파일명을 지정
docker  - Dockerfile
podman - Containerfile
# docker tag image:tag image:tag2: 이미지의 태그 변경시 사용
# docker image tag image:tag image1:tag : 이미지의 이름 변경 

실행
# docker run [image] : 이미지를 컨테이너로 실행
현재 상태에서 실행시 진행되는것:
1. 실행되자마자 컨테이너 종료상태
2. 실행시 이름을 지정하지 않았	기 때문에 자동으로 생성
3. 실행된 컨테이너에서 쉘을 실행한다. 하지만 이후 작업을 지정하지 않고서 run 명령을 내렸기에 바로 정지
4. 컨테이너가 종료되면 도커는 컨테이를 제거하지 않고 유지시킨다.

옵션
-it  : 컨테이너가 실행 후 실행된 컨테이너 내부에 쉘을 실행한다. 
-d : 컨테이너를 백그라운드로 실행한다. 
--rm : 컨테이너가 종료될 때 기존의 컨테이너를 유지하지 않고 삭제시킨다. 동일한 컨테이너를 여러번 실행시켜야 하는 경우 docker ps -a 에서 나오지 않도록 해야 하는 경우에 사용하면 좋다. 
--name : 컨테이너의 이름 변경, 좀 더 관리를 쉽게 하기 위해서 지정하는 것이 좋다.
image:tag 동일한 이미지가 존재하는 경우 특정 버전을 선택하여 사용하도록 tag를 지정
명령어 : 컨테이너를 실행후 바로 내부에서 사용자가 명시한 명령어를 실행한다.
-p : 컨테이너에서 동작되는 포트와 외부 호스트 포트를 포워딩 하기 위해 사용
-p 80:0880   : -p [내부]:[외부] 
-v : 볼륨 
컨테이너에 영구적인 데이터 보관을 위한 볼륨을 연결한다.
영구적인 데이터 보관 : 불변의 이미지에 저장을 해도 재시작을 하는 순간 삭제가 되기 때문에 영구저장을 위해서는 외부 호스트에서 추가 볼륨을 마운트 해 주어야 저장이 가능.
mysql-[/data]
# docker commit container newcontainer:tag : 실행중인 컨테이너를 이미지화 하는 것
# docker create image:tag : run과 비슷하지만 직접실행하지 않고 컨테이너 생성만 한다.
# docker start [CONTAINER ID]: 생성된 컨테이너를 시작한다.
# docker stop [CONTAINER ID]: 실행중인 컨테이너를 중지한다. -15 
# docker kill [CONTAINER ID]: 실행중인 컨테이너를 강제종료한다 -9
# docker rm -f  [CONTAINER ID]: 실행중인 컨테이너를 강제 제거, 실행중인 자료에 대해서 보장하지 않습니다.
# docker stats  [CONTAINER ID]: 실행중인 컨테이너에 대하여 상태를 확인한다.
# docker inspect [CONTAINER ID]: 실행중인 컨테이너의 정보를 확인
# docker ps  : 현재 동작중인 컨테이너를 출력
# docker ps -a : 모든 상태의 컨테이너를 출력
# docker attach [CONTAINER ID]:  백그라운드 모드로 동작중인 컨테이너에 연결
# docker exec [CONTAINER ID] [COMMAD]: 컨테이너 내부에 명령어를 실행
# docker top [CONTAINER NAME]: 컨테이너 상태 확인
# docker logs tail  [CONTAINER ID] : 컨테이너 내부에서 출력되는 표준 출력, 오류를 화면서 표시를 해준다.
# docker pause [CONTAINER ID] : 컨테이너를 일시정지한다.
# docker unpause [CONTAINER ID] : 일시정지되는 컨테이너를 다시 시작
# docker port  [CONTAINER ID] : 컨테이너에서 사용중인 포트를 확인
# docker diff [CONTAINER ID] : 원본 이미지 형태와 현재 실행중인 컨테이너의 파일에 대한 차이점을 확인하기 위해 사용
# docker cp CONTAINER:SRC_PATH DEST_PATH|-
# docker cp SRC_PATH|- CONTAINER:DEST_PATH
컨테이너 내부 / 외부에서 파일을 복사 해야 하는 경우 사용
# docker rmi [CONTAINER image]: 이미지를 호스트에서 삭제하는 경우 사용 

서비스가 되기 위한 과정.
테스트
run 명령어를 테스트
docker hub 에서 이미지를 다운로드 받는다.
docker.io/library/centos 
실행, 각종 옵션 테스트
-----------
run 
ps
images
create
rm
commit 
-------------
중간에 한번은 pull 	파일이 정상 다운로드 안되는 경우가 생깁니다.

웹서버
회사 

개념 
실습
파일 
https://hub.docker.com/ -
도커  이미지 라이프 사이클

컨테이너 상에서 프로세스 
라이프 사이클
볼륨,
네트워크..


run command : 컨테이너를 실행> 명령어 실행 > 컨테이너 중지
docker run 해서 linux 컨네이너 실행하면 바로 종료되는
exec = 현재 가동중인 컨테이너

docker exec    <<< 


run
exec

컨테이너가 실행 중인 상태냐 / 컨테이너를 실행하면서 진행하는 상태냐 

어제 이어서 docker 기본 명령어를 통해 컨테이너 조작을 끝내보자.

조립 해볼 단계

# docker version -> 
클라이언트, 엔진[서버]

도커는 클라이언트 / 서버 구조로 나누어져 있으며
명령어를 docker로 사용하는 것은 클라이언트 작업

클라이언트가 엔진으로 명령어를 전달하여 
엔진은 명령을 해석하고 실행하여 결과를 다시 클라이언트로 출력
[클라이언트]-------[엔진(docker daemon )]

# docker system info

Union File System (UFS)
여러개의 파일 시스템을 하나의 파일시스템에 마운트 하는 기능 
A dir
B dir

[  THIN R/W Layer ] R/W container layer
---------------
job2
-------------
job1
---------------
base image
---------------  image layer

snapshot 

[vmdk]---[ 1111111111     ]
   R/O          R/W

Layer =     붙어있는 그림 
이중에 하나라도 떨어지면? 
Module =  떨어져 있는 그림 

# docker image inspect [image name]
이미지의 계층 구조를 확인할때 사용
docker pull redhat/ubi8

컨테이너가 구동시 UFS 은 어떻게 동작할까.

총 3계층

>  merged      실제 사용자가 보는 모습
>  upper dir   container layer
>  low dir       imagelayer

AUFS
debian 계열에서 사용

Device mapper
범용 스토리지 드라이버
프로덕션 사용시 성능 저하

overlay
빠르다.!
메모리 사용에 효율적
container 변경이 많은 경우 성능저하 -> 변경이 거의 없는 상태에서 사용하는 베스트
# docker rm $(docker ps -a -q)
# docker rmi $(docker images -q)

+ 
--restart=always 
현재 상태에 상관없이 다시 시작하도록 한다.
항상 띄워서 사용해야 하는 컨테이너의 경우에는 --restart=always  옵션을 사용하여 데몬 재시작, 중단이후에도 docker 데몬을 재시작하는 경우 따로 손댈 필요없이 가동 할수있다.
현재 상태에 상관없이 데몬 재시작시 컨테이너도 가동한다.
데몬 재시작 >
STOP -> START 
가상화 - > 

run - 컨테이너를 가동 각종 옵션을 붙여서 
컨테이너를 실행할때 접근, 접근하는 경우가 많지는 않다.
exec - 
컨테이너 내부로 접근해야 하는 경우에 사용 
80/tcp -> 0.0.0.0:8080

호스트에서 8080 포트로 접근시 컨테이너의 80 포트로 바인딩

save    이미지
export 컨테이너

쿠버네티스 ansible  자동화 : 금요일 예정

기본 명령어 [기본 조작]

이미지 생성

-------------------
[볼륨 네트워트]
-------------
컴포즈

docker  >  podman 

이 추세에 따라 podman에 대한 정보도 어느정도 가지고 있어야 한다.

OCI : 
오픈소스 기반의 컨테이너의 표준사항을 정의

docker -> podman ?
docker 가 EE /CE를 더이상 지원하지 않겠다.
리눅스 파운데이션:
컨테이너의 방향성 고민

OCI 탄생 배경 : docker 때문에 오히려 개발에 대한 가속화

docker 2010 
dotcloud 라는 회사로 시작 2011 런칭 
PaaS

신기술 없는 신기술
클라우드

LXC chroot | 2013 
2017> 
OCI 표준사양을 정의 
RHEL 9 2022.5 
RHEL 8 2019 

도커는 하나의 제품에서 
컨테이너 런타임, 
컨테이너 이미지 관리
컨테이너 관리
레지스트리 이미지 서버, 

현재 도커에서 사용하는 이미지 OCI 
이미지 제작시 사용되는 명령어, 관리에 사용하는 명령,
podman = docker 95% 

OCI?
기본적으로 컨테이너 관리 기능 제공
도커의 기능을 분리 하여 독립적인 소프트웨어로 제공

마이크로 서비스 아키텍처를 지향하면서
모놀리식 아키텍처를 가지고 있는 서비스

CRI-O  기본적인 컨테이너 환경을 제공하는 것이 목적
* K8S openshift에서 사용되는 런타임
도커보다 가볍게 사용가능
PODMAN docker의 클라이언트
Buildah docker의 build 기능을 따로 빼서 소프트웨어화 함
skopeo : 컨테이너 관리 용도


> docker
kernel
container
image
docker engine
docker cli * 
image regstry

>podman
kernel
container
image
podman *
skopeo *
Buildah * 
image regstry

docker > CRI-O > runc 

런타임의 역할
컨테이너의 기본적인 생성, 조회를 할 수 있는 환경을 조성
컨테이너를 생성 런타임은 OCI를 통해 runc 기반으로 컨테이너를 생성

구별해둔 이유:
기능과 관리범위

두개 모두다 컨테이너 생성, 관리가 목적이지만 CRI는 빠르고 가볍고 단순하게 컨테이너를 생성하는 것이 목적

고급언어 *
저급 언어 * 

libpod docker containerd CRI-O  : high-runtime 

>> runc : low-runtime
일반적으로 컨테이너가 구성 및 환경을 제공해주는 소프트웨어 
계층을 여러 단위로 나누어 지원

runc 자원 : 순수하게 컨테이너 런타임 구성
리눅스 소프트웨어 기반으로 동작 기본적으로는 커널기능을 기반으로 동작
커널에서는 다음과 같은 기능 :


namespace
시스템에서 동작하는 프로세서를 시스템과 분리하는것을 목적으로 하며 
이 동작이 일어나면 프로세서를 격리
주요목적: 격리 
커널버전에 따라서 조금씩 범위가 차이가 있지만 4.8 이상부터는 큰 차이는 없다. 

Cgroup
제한, 격리 
모든 자원들이 서로 경쟁하지 않고 각각의 영역에서 격리, 추적이 가능하기 때문에 시스템을 좀더 안정적으로 사용 가능 
RHEL7 이상에서 C-GROUP v2 

chroot
현재 디렉토리를 최상위 디렉토리처럼 변경


PODMAN  - CRI 를 통해서 동작하지 않고 podman CLI 를 통해서 관리 가능

podman - runc - host[ container ]


                        podman        CRI-O
네트워크        O                   X
스토리지       X                    X  
이미지            O                  ㅁ
API                O                   ㅁ

1. 기존에 있는 중지된 컨테이너 삭제/ 이미지 삭제[선택]
2. nginx 컨테이너를 가동 
기본 웹페이지 출력
> 이름 변경
> 포트 변경
> 항상 실행
> 백그라운드 모드에서 작업
확인

------------------- 25분 -------------

commit 
export 

4:10 

빌드 
파일 이름 : Dockerfile 

도커는 Docker Instruction 이라는 명령어 구조를 가지고 있음.
순차적으로 처리 
최종 사용자가 원하는 프로그램을 실행하는 구조로 되어 있다.

FROM 빌드에 사용할 이미지 지정
LABEL description="string" 이미지에 대한 설명
MAINTAINER 이니셜<mail> 작성자 표시

WORKDIR 빌드에 사요할 작업 디렉토리를 변경
COPY : 파일이나 디렉토리를 복사하는 경우
ADD : 원격에서 파일을 내려받기, 복사하는 경우 사용
RUN : 특정 명령어를 일시적으로 실행
EXPOSE : 컨테이너 실행시 특정 포트를 호스트에 노출

실행
ENTRYPOINT : 컨테이너 실행시 특정 프로그램을 실행
CMD : 컨테이너 실행시 특정 프로그램을 실행, RUN과 다르게 계층 생성을 하지 않는다. : 명령어, 옵션, 인자를 구분하지 못하기 때문에 "" 내에 작성하고 토큰으로 구별되는 경우에는 , 를 작성한다.
EX : CMD ["CMD","arg"]

차이점 : 컨테이너 시작시 실행 명령에 대한 지정 여부
ENTRYPOINT로 정의 하게 되면 컨테이너가 실행될 때 반드시 지정한 명령으로 수행된다.
CMD는 인자값으로 변경하여 실행이 가능
컨테이너가 가동될때 변경되면 안되는 명령 : ENTRYPOINT
인자값은 CMD로 해주는 것이 좋다.
대부분 list 형태로 해주는 것이 좋다.

Docker file에 다음 명령어를 입력하여 build 해봅시다.  date +%H:%M
vi :
# vi Dockerfile
입력키: i a o 
삭제: dd x 
치환: r 
저장: wq w 
Dockfile 을 이용해서 
nginx , httpd 
웹서버 한번 실행, 외부 노출 

centos 
httpd 
볼륨.
변수 : ENV 
네트워크 관련

FROM centos:centos7
LABEL description="apache webserver test"
MAINTAINER test<test@example.com>
RUN yum -y install httpd 
RUN yum clean all
host의 index 파일을 Dockerfile로 복사해서 빌드 하기. 
ENTRYPOINT ["/usr/sbin/httpd","-D","FOREGROUND"]

디렉토리 : /var/lib/docker/overlay2/
1. yum install > 용량 확인  -> commit? 
2. yum install > yum clean all > 용량 확인 commit ?

size 
초기 overlay2 - 212M
컨테이너  단순  가동 - 424M 
yumlist update -      737M
httpd install       -      803M   591M 
yum clean all    268 

용량의차이
base image으로 사용한 상태: 
달라지면?

파일 복사
볼륨
네트워크

컨테이너를 실행 - 파일 복사

build시 COPY의 단점 :
자주 변경되는 데이터이거나 한번이라도 수정이 가해지게 된다면
re-build를 해야만 한다.
아니면 컨테이너를 실행 이후에 파일을 overwrite해줘야 하는 불편함이 존재.

-v 볼륨을 사용하여 
호스트와/ 컨테이너간의 데이터를 공유하면서 실행한다면 좀더 수월하게 변경을 할 수 있다.

-v
hostVOL:containerVOL
hostVOL:containerVOL:RO
engineVOl 
읽기만 필요한 경우에는 굳이 RW 속성이 아니라 RO로 속성을 부여하여 호스트에 임의의 파일이 생성되는 것을 방지.
컨테이너 간에 파일 공유를 쉽게 할 수 있다.
디렉토리를 작성할때 굳이 없는 디렉토리를 생성하지 않아도 된다. 
만약에 생성을 해야 한다면 컨테이너 내부에 없는 디렉토리는 항상 생성을 해주거나 또는 dockerfile 작성시에 신경을 써야 하니까
engineVOl 
그냥 사용하기
-v /test 
-v로 지정되는 디렉토리는 컨테이너 내부에서 사용하는 디렉토리만 지정이 된다.
이름을 지정해서 사용하기 

컨테이너로 구성하는 경우
데이터를 컨테이너 내에 쌓을것인가? - 
볼륨을 어떻게 구성 할 것인가?

연습:
con1|con2 ------con3
con1
데이터 생성...  date  추가로 한줄씩 생성
con2
데이터 생성 df  새로고침 
con3
데이터 수집
같은 공유 디렉토리를 사용/ 

ssh  설치
실제 호스트가 아니라  컨테이너로 접근
RO  확인 용도로 사용

컨테이너끼리 데이터 공유

변수 -e 
mysql X 
wordpress 

도커 네트워크 :
도커 데몬이 올라오면 docker0가 생성
가상 bridge로 구성
NAT , iptables 통해서 진행
* 컨테이너의 게이트웨이를 지원
기본적으로는 DOcker0를 바라본다.
L2

-p
컨테이너의 포트를 외부로 노출시켜 외부 연결을 진행
서비스를 외부에 하기 위함 
= 기본은 막혀있다.

호스트포트 :컨테이너포트   8080:80 
-p 8080:80
-p 80 < 컨테이너포트
호스트포트는 랜덤으로 연결
-P  
build 시 입력되는 expose 포트와 호스트를 랜덤으로 연결

네트워크 ->
도커, podman , K8S 
네트워크 정보는 미리 구성해두신 상태에서 작업

한개 open ---- link > 네트워크 정보?

도커는 컨테이너에 IP 를 순차적으로 할당.
내부 IP는  컨테이너를 재시작 할때 변경될 가능성이 있다.
컨테이너에서 사용되는 내부 IP는 외부랑 굳이 연결될 필요가 없다.이는 	iptables에서 처리
컨테이너를 생성 하는 경우에 veth 장치가 각 컨테이너에 생성된다.
veth 사용자가 직접 제어할 필요는 없다. - 엔진이 생성
veth 장치는 NIC와 연결
docker0 는 veth에 [바인딩]되어  호스트의 NIC로 이어주는 역할

컨테이너는 브릿지를 통하여 외부와 네트워크 

도커설치-
도커 엔진 시작
CLI  명령ㅇ
컨테이너 이미지.
컨테이너 이미지 빌드.
기본 조작
볼륨
네트워크

hub!  login : 업로드 다운로드 / 검색 다운로드 

wordpress 
1. 수동
2. 빌드
3. 컴포즈 

disk overlay

-------------------------
도커 
명령어 - 이해도를 좀 올리기 위한 텍스트 
이해도가 높다. 나는 명령어가 쉽다. 나는 어렵지 않다. PASS

라이프사이클 -  
내일 :애플리케이션 설계시에 컨테이너 이용법

컨테이너 - 항만 ->
규격화 : protocol  > OCI 
service - business 

물건을 싣는 배 -> 가마니 -> 적재 -> 운송
긴 거리를 항해 -> 썩을수도 있고, 상할수도 있고, 
컨테이너 -> 규격

배 -> 정해진 대로 생산
컨테이너 -> 국제규격 ->
보관이 용이 -> 냉동 컨테이너 , 냉장 컨테이너, 일반 컨테이너.
운송에 용이..

컴퓨터 ->
APP+H/W -> APP+OS+virtaul+HW -> APP+Container+virtaul+HW 

도커 = [컨테이너]
실제적으로 도커 = 도커 엔진
나머지 기능까지 합한다면 도커 오픈소스 프로젝트
리눅스 컨테이너에 좀더 많은 기능을 추가.
어플리케이션을 좀 더 쉽게 사용 가능하도록 만들었다.
GO : 
가상머신과 달리 오버헤드가 거의 없다. : 99.8%

도커 컨테이너->
chroot / namespace / cgroup [v2]
-> 격리가 가능한 프로세스로 환경 구성

애플리케이션에 대한 라이브러리, 데이터로 구성이 되어 있기 때문에 용량이 작다.

가상머신 - 무거운 서비스 
컨테이너 - app
지금은 아니다. !

리눅스 추천 | 
docker -> podman 

docker CLI----------->docker Engine
클라이언트에서 명령을 요청하면 docker.sock 을 통해 API 명령 호출

도커를 버려야 하나요? 아닙니다. 계속 사용하셔도 무방.
RHEL8 
podman > 어떻게든 답을 찾을것이다. 
docker - RHEL9 

이미지 -> 실제 컨테이너로 구동 되기전에 저장된 파일
[저장소이름]/[이미지이름]:[버전]
가상머신에서 사용되는 ISO 랑 비슷~
계층별 구조화 이미지
컨테이너 생성시 불변의 이미지로 R/O 
docker pull, docker run 명령을 통하여 이미지 내려받기 가능
docker login , reg 변경을 통하여 이미지 내려받기 

이미지[파일] -> 컨테이너 = [프로세스]
mem file1 
exec               sync

DISK file1

도커 이미지에서 컨테이너로 실행
격리된 파일 시스템, 시스템자원, 네트워크...등등등 격리된 공간을 생성
목적에 맞게 컨테이너를 생성하여 사용
이미지는 읽기 전용 [불변] 컨테이너 레이어가 있으며 추가 정보는 컨테이너 레이어에 저장 -> commit을 하지 않는 이상 저장되는 정보가 아니다.!
컨테이너 내에서의 설치/ 삭제는 다른 컨테이너와 호스트에 영향이 없다.
nodebb -> node.js
프로그램의 버전에 관련한 스트레스가 적으며 
의존성 문제, 충돌문제에서 벗어날수 있다.

pull
run
create
del

docker run conname : 아무 반응이 없다. : 실행후 종료
run에 있는 옵션중 자주 사용하는 옵션들은 꼭 잘 이해를 해 두기 바랍니다.
--rm
--name
-id
-it
--restart=always 
--network 
-v
exit / ^d = 빠져나오는 순간 중지
^P+Q

컨테이너 애플리케이션 MSA 
서비스에 대한 컨테이너화 /  격리화를 통해 관리 포인트를 최대한 잘게 쪼개자.
하나의 컨테이너에는 하나의 서비스

작업중에 포함
작업후에 포함

제한 
CPU
MEM
disk 
network
자원에 대한 제한
첫시간 진행 -: limit  
자원을 공유=
제한이 없다면 하나의 컨테이너가 마음껏 자원을 소모..
제한
1 2 3

관리를 사람이 직접 해야 하다보니까 
최대한 관리 포인트를 줄이는 것도 하나의 방법.!

단일 컨테이너 만들기!

다중 컨테이너 만들기
yaml * 
이미지 빌드 
설계..

도커파일의 사용이유:
이미지를 다운 -> 작업  두번 세번/ 작업을 것을 한번만 할 수 있도로 ㄱ해주는것
애플리케이션 빌드 자동화
도커 허브에서 신뢰할수잇는 이미지 다운로드 
용량에 대한 고민을 좀 하고
어떤식으로 쌓아야 

캐시 
캐시를 이용하지 않을때 
속도 차이

컨테이너로 애플리케이션 설계시의 고민
code X

30분 
network / vol / build [option]
--cache 

도커 파일 /
컴포즈 /
컨테이너 기반 -> 컨테이너로 올라갈때 까지 


도커 허브 가입 완료

로그인

이미지는 있는상태 centos -> 
[리포지토리네임]/[이미지네임]:[태그]


선수적으로 해야하는 것 :
도커허브 로그인 -> create repo 
1. 로그인 하는 방법
# docker login
2. 이미지 이름 변경
이유: 보통 내부에서 생성하거나 따로 빌드해서 만드는 이미지의 경우 이름이 단순, 또는 기업 포맷에 맞추어 생성을 하는 경우가 대부분
[root@docker ~]# docker images
REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
web1         1.0       0eeaf1a57a16   23 hours ago   437MB
> 현재 가지고 있는 이미지 파일
[root@docker ~]# docker push web1:1.0 
> docker hub로 업로드시도[태그를 변경하지 않고 시도]
The push refers to repository [docker.io/library/web1]
> 로그인을 했지만 계정으로 업로드가 아닌 오피셜 zone으로 업로드 
0fa551173627: Preparing 
4f31be0e8f87: Preparing 
5ce287204658: Preparing 
174f56854903: Preparing 
denied: requested access to the resource is denied
> 리소스 권한 없음, 업로드는 실패.
이미지는 바로 업로드가 불가능 하다. 도커허브상에서 이미지 파일의 이름은 고유 해야 한다.
> 리포를 생성하면 다음과 같이 가이드 텍스트가 출력
[root@docker ~]# docker tag web1:1.0 docker0099/web1:1.0
[root@docker ~]# docker images
REPOSITORY        TAG       IMAGE ID       CREATED        SIZE
docker0099/web1   1.0       0eeaf1a57a16   23 hours ago   437MB
web1              1.0       0eeaf1a57a16   23 hours ago   437MB

docker tag local-image:tagname new-repo:tagname
docker push new-repo:tagname

docker push studylab/docker0099:tagname

[root@docker ~]# docker push studylab/webserver:latest
The push refers to repository [docker.io/studylab/webserver]
0fa551173627: Pushed 
4f31be0e8f87: Pushed 
5ce287204658: Pushed 
174f56854903: Pushed 
latest: digest: sha256:34baf040f83605509119c157d7ab864d10f330209c1a6989e11a252fbdf8be7b size: 1159

도커 허브 
초기에는 도커허브에 이미지 pull 이 무제한
익명 유저(docker login 안함)	IP 기반으로 6시간동안 100번 request 제한
로그인 유저(docker login 함)	계정 기반으로 6시간동안 200번 request 가능
지불 계정 유저(docker login 한 Paid 유저)	제한 없음
지불 유저의 경우도 IP 기반 제한 (opens new window)이 있다고 합니다. 공개 저장소를 대량으로 사용하는 경우 주의가 필요합니다.

완전한 오프라인형태의 망에서 컨테이너를 사용해야 한다?
방법은 무엇이 있을까?
save / import >
온라인 망에서 pull download -> save / tar -> 내부망 서버로 오프라인 복사
import 
기존에 사용한 명령어.
내부에 구축하는 repo 

save/ export /import 요 명령어가 별로 쓸모없어 보이지만 자주 사용될수도 있다.

저장소 공간 :
registry : 컨테이너 이미지를 저장하는 저장소
docker hub 
private hub

이미지 종류 :
1 official images
2 verrified publisher
3 etc..
----------------------
리소스 제한 

CPU
MEM
DISK

리소스 제한 
엔지니어 
--------
베어메탈 - 리소스 관리를 하기 쉬웠다.

GUI 설치 , 설계 
32G

6명
공유된 멀티유저 시스템
한명이 리소스를 과하게 사용하게 되면 나머지는 손해

가상화 관리 -
리소스 관리, 리소스 제한 


기본적으로는 컨테이너는 호스트 하드웨어 리소스의 사용 제한을 받지 않고 사용 가능 

컨테이너 -> 소규모 프로그램

워드프레스 , x5

CPU
MEM
DISK

docker run --help

# docker run --memory [ContainerName]
컨테이너가 사용가능한 최대 메모리 용량을 지정
EX : docker run --rm -id --memory 1g [ContainerName]
# docker run --memory-swap [ContainerName]
컨테이너가 사용가능한 스왑 메모리에 대한 설정
EX : docker run --rm -id --memory 1g --memory-swap 800m [ContainerName]
메모리 옵션을 사용할때는 mem, swap을 같이 지정해주는 것이 좋다.
만약 --memory만 지정을 한다: 스왑이 메모리의 두배로 지정
# docker run --memory-reservation [ContainerName]
소프트 제한 값을 설정한다.
지정된 메모리 값보다 작게 설정 한다.
--memory : 절대 넘어갈수 없는 용량
--memory-reservation : 보장값
EX : docker run --rm -id --memory 1g --memory-reservation 400m 
전체 1g 의 메모리 용량을 가지고 스왑용량은 	2g로 설정된다. 
그리고 최소 보장용량을 400m를 가지는 컨테이너를 생성 할 수 있다.
# docker run --oom-kill-disable [ContainerName]
OOM killer가 프로세스를 kill 하지 못하도록 보호
중요한 컨테이너의 경우 물리적 메모리가 부족한 상황에서 프로세스 종료를 할 수 없도록 보호하는 용로도 옵션을 사용 
-------------------------
CPU
# docker run --cpus="value" [ContainerName]
컨테이가 사용할 cpu core수를 지정
EX : docker run --cpus="2" [ContainerName]
EX : docker run --cpus="1" [ContainerName]
EX : docker run --cpus="1.5" [ContainerName]
# docker run --cpuset-cpus [ContainerName]
컨네이너에 부여할 CPU 코어를 지정
cpu index번호는 0부터 출발하기 때문에 0부터 지정, 
lscpu의 CPU(s): 1, On-line CPU(s) list:   0 정보를 확인하여 CPU core 개수를 확인
EX : docker run --cpuset-cpus=0-1 [ContainerName]
# docker run --cpu-shares [ContainerName]
컨테이너가 사용중인 	CPU의 비중을 부여
기본값은 1024 
EX : docker run --cpu-shares 512 [ContainerName]

block IO
# docker run --blkio-weight [ContainerName]
블록디바이스 IO의 쿼터[qouta]를 설정하여 사용
기본값 500 [100-1000]
EX docker run --blkio-weight 300 [ContainerName]
# docker run --blkio-weight-device /dev/sdb [ContainerName]

# docker run --device-read-bps /dev/sdb:10m [ContainerName]
장치의 읽기 속도 제한
# docker run --device-write-bps [ContainerName]
장치의 쓰기 속도 제한
# docker run --device-read-iops /dev/sdb:10 [ContainerName]
장치의 읽기 iops 속도 제한
# docker run --device-write-iops [ContainerName]
장치의 쓰기 iops 속도 제한

# docker stats web
실행중인 컨테이너의 통계를 확인을 할 수 있다.
# docker events -f continer=weba
실행중인 컨테이너의 이벤트 정보를 출력

컨테이너 기반 애플리케이션 구성시 이것만은 지켜보자.
회사 / 개인 / 쿠버네티스 / 

> 간단하고 쉽게 유지 해야 한다.
단순한것이 제일 힘들다. 
> 동일한 작업을 반복하는 것만큼 미련한 짓은 없다.
시간을 두배로 소모시키는 짓
자동화  
> 프로그램에 필요없는[쓸데없는] 기능 추가하지 않기
간단하고 쉽게 유지해야 한다에 어긋남
> 관심사에 대한 분리
모듈하나는 서비스 하나만 제공한다.
컨테이너, 웹 mysql php node dns ...
단일로 하나만 하자.

각종 기반이 되는 인프라마다 특성은 있지만 결국
상단의 컨테이너[애플리케이션]->
관리되는 컨테이너 내에서 실행하기 우해 마이크로 서비스 개념
장애포인트가 명확하게 보인다.
운영이 원활하게 되도록 실행에 대한 보장성, 신뢰성 

컨테이너 접근 

단일 서비스 원칙
모니터링/ 상태보고/ 추적
라이프사이클 원칙
이미지 불변 + 데이터 확장
프로세스 
격리/ 독립

시간적인 문제/

이미지 최소화: 불필요한 모든 파일을 제거 하는 것

도커 -> 단점
켜고 끌때 
내가 너무 힘들게 작업을 해야 한다.?

wordpress 
mysql

이미지 다운로드
이미지 실행
환경변수
포트
볼륨

도커 컴포즈

컨테이너 실행시 docker run으로 실행
올릴때마다 하나씩 너무 힘들다는 생각을 해본적은 없는가
귀찮다.
컴포즈 ->
컨테이너가 하나가 아니라 두개 이상 -> 여러개 동시에 실행되는 인프라 구성이 필요한 경우
- 깔끔하게 실행
- 한방에 실행

명령어가 굉장히 직관적으로 
쉽다.

일단은 따로 구성하기
수동 작업으로 한번 진행.
수동 작업 좀 손쉽게 해보는거
그 다음에
compose 
-> 워드프레스 수동 설치 하기
docker run -d --restart=always --name wordpressdb -e MYSQL_ROOT_PASSWORD=pw -e MYSQL_DATABASE=wordpress -v /mysql_data:/var/lib/mysql mysql:5.7 

docker run -d --restart=always -e WORDPRESS_DB_PASSWORD=pw -e WORDPRESS_DB_USER=root --name wordpress -v /wp_data:/var/www/html/ --link wordpressdb:mysql -p 8080:80 wordpress

쉘 스크립트 형태로 생성해두고.
해당 명령어를 쉘 로 실행하는 것도 방법.!

컴 포 즈 
yaml 특성
vi

한방에 구축하기

도커-컴포즈 설치
curl -L "https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose

[root@docker ~]# docker-compose version
docker-compose version 1.24.0, build 0aa59064
docker-py version: 3.7.2
CPython version: 3.6.8
OpenSSL version: OpenSSL 1.1.0j  20 Nov 2018

컴포즈를 위한 디렉토리를 하나 생성해두자.
실제작업 디렉토리의 경우에 무슨작업인지 정확히 판별 가능한 상태
컴포즈 root 디렉토리에 README 파일을 하나 두고 각 디렉토리의 역할을 설명하는 것도 하나의 방법

yaml 문법
YAML은 사람이 읽을 수 있고 이해하기 쉽다,.
key=value  
배열
띄어쓰기를 정말 잘해야 합니다.

studylab/webserver:latest 

# docker-compose up -d
# docker-conpose down

실행된 모습을 보자.
코드
[root@docker sample]# cat docker-compose.yml 
version: '3'
services:
  web:  
    image: studylab/webserver:latest
    ports:
      - 8080:80
    volumes:
      - /test:/var/www/html
이름이 작성되는 방식
[프로젝트디렉토리]_[서비스이름]_[번호]

http://mrleeidea.com/2019/06/21/vim-yaml-editing-%EC%89%BD%EA%B2%8C-%ED%95%98%EA%B8%B0/

compose syntax 
version : '3'
1 네트워크 지원 X 
   모든 컨테이너가 브릿지 모드에 배치, 모든 컨테이너가 연결 가능
   컨테이너간에 link를 사용하였지만 2 3 버전으로 가면서 추천하지 않는다.
2 yaml 마이너 버전까지 작성해야 함
    서비스를 services 아래에 다중 컨테이너로 구성
    volume에 name volume 사용 가능
    모든 컨테이너가 서비스 이름과 동일한 호스트 이름으로 검색이 가능 
    * 의존성에 대한 검증 및 의존성 확인으로 순차적인 컨테이너 생성
    * depends_on 
     단점 : 실행순서만 제어 / 컨테이너가 기동되고 내부 프로세스가 완료 처리가 되지 않더라도 실행이 되었다고 판단되면 다음 작업을 진행해버리기 때문에 문제가 생김 
     [A]--20초---[B]--30초-----[C]
시간을 벌기위해 쉘 스크립트 형태로 구현된 compose를 사용하기도 한다.
docker-compose -f /root/compose/sample/docker-compose.yml up -d
3 도커 스웜을 지원함

service
image로 사용하는 방법은 docker hub에서 정확히 이미지명을 입력
build 로 사용하는 방법:
services:
  web:
    build: 이미지 대신 빌드로 이미지를 생성
      context: 도커파일이 존재하는 디렉토리 /root/content
      dockerfile: 디렉토리에 있는 도커파일의 이름 webserver_config

오류:
restart : 
- always : 컨테이너를 수동으로 내리기 전까지는 항상 재시작
- on-failer : 컨테이너가 오류가 있을 때 재시작

volume
디렉토리 마운트 형태
docker volume 형태
- [호스트 디렉토리]:[컨테이너디렉토리]
- [볼륨명]:[컨테이너디렉토리]
    > 다음과 같이 입력한다면 volume을 하위에 따로 지정을 해주어야 정상 가동

환경변수
enviroment: 
 - WORDPRESS_DB_PASSWORD=pw
 - MYSQL_DATABASE=wordpress
env_file:
 - [file path]
key value 형태로 입력해주면 된다.

port
-p 옵션으로 지정하는 것과 다른게 없다.
[호스트]:[컨테이너포트]

network
네트워크를 따로 적용 시키는 경우에는 
service에서 각 컨테이너에도 적용을 시켜주어야 한다.
network:
  network1:
    driver: bridge
 
컨테이너:
web:
   networks:
		  -  network1
     

생성-  적용



쿠버네티스 구성
vmware file
압축 풀기 -> vmware import 
Rocky8  VM 생성
Rocky8  VM으로 clone 작업
3대
master
node1
node2
clone > full / link
full clone 선택 

master :
root/redhat <  
team01/team01

K8S 구축 
클론만 된 상태에서 CPU MEM  성능 조절을 먼저 하고
네트워크 	NAT로 변경

머신을 모두 종료된 상태에서 네트워크 카드를 추가 
vm -> setting > add > network adapter > finish
추가한 NIC의 형식을 host-only로 변경
OK


각 머신마다 NIC는 두장이 장착되어 있다.
1- NAT  192.168.10.200
2- host-only 
master

putty 
# ifconfig
장치 :
ens160
ens224

ifconfig로 확인한 뒤 putty 로 원격 접근 후 터미널에서 작업

host-only 장치 설정
nmcli con add con-name ens224 ifname ens224 type ethernet ipv4.addresses 192.168.100.100/24 autoconnect yes ipv4.method manual
nmcli con up ens224
NAT 설정
nmcli con mod ens160 ipv4.addresses 192.168.10.200/24 ipv4.gateway 192.168.10.2 ipv4.dns 8.8.8.8 autoconnect yes ipv4.method manual
reboot
재접속 -> 200
ifconfig
ping www.google.com
yum list
 hostnamectl set-hostname master.example.com
node1 작업
 hostnamectl set-hostname  node1.example.com
nmcli con add con-name ens224 ifname ens224 type ethernet ipv4.addresses 192.168.110.100/24 autoconnect yes ipv4.method manual
nmcli con up ens224
nmcli con mod ens160 ipv4.addresses 192.168.10.210/24 ipv4.gateway 192.168.10.2 ipv4.dns 8.8.8.8 autoconnect yes ipv4.method manual
reboot 

node2
hostnamectl set-hostname  node2.example.com
nmcli con add con-name ens224 ifname ens224 type ethernet ipv4.addresses 192.168.120.100/24 autoconnect yes ipv4.method manual
nmcli con up ens224
nmcli con mod ens160 ipv4.addresses 192.168.10.220/24 ipv4.gateway 192.168.10.2 ipv4.dns 8.8.8.8 autoconnect yes ipv4.method manual
reboot 

master
hostnamectl set-hostname master.example.com
NIC1 192.168.10.200 / 192.168.10.2 / 8.8.8.8
NIC2 192.168.100.100/24
node1
hostnamectl set-hostname node1.example.com
NIC1 192.168.10.210 / 192.168.10.2 / 8.8.8.8/
NIC2 192.168.110.100/24
node2
hostnamectl set-hostname node2.example.com
NIC1 192.168.10.220 / 192.168.10.2 / 8.8.8.8
NIC2 192.168.120.100/24


node1
nmcli con mod ens224 ipv4.addresses 192.168.110.100/24 
node2
nmcli con mod ens224 ipv4.addresses 192.168.120.100/24 

[root@master ~]# yum -y install epel-release.noarch
[root@master ~]# yum list
[root@master ~]# yum -y install bash-completion git ansible

master 에서 작업
# cd 
# git clone https://github.com/tangt64/duststack-k8s-auto/
# cd library/

nat
host-only

추천 사항 : 꼭 지킬 필요는 없다.
CPU intel > AMD 를 추천 core
메모리 32
disk  256
네트워크는 둘다 구성
CPU에서 가상화 기능 지원

K8S 설명


docker = 컨테이너
쿠버네티스 - 오케스트레이션 툴

클러스터화 된 풀의 중앙제어
마스터서버에서 모든 나머지 서버에 대한 중앙 관리

상태 관리 - 사용자[관리자/ 개발자] 즉각적인 변화

스케쥴링
배포
할때마다 여유로운 서버를 확인- 배포 

컨테이너의 버전에 대한 롤백/ 롤아웃

도커 -- 
100개의 노드 
1000개의 컨테이너
300 개 v1.1 

master
node
kubectl
etcd
pod

[사용	자]----[K8S]------[POD]

1 con run


상태 체크 - 차이점 발견 - 조치

루프 

명령어는 도커처럼 사용하다보면 익혀집니다.
개념

master
api server
etcd
controller

node 

계층구조 파일 시스템
snat dnat 커널 모듈

# lsmod 

tc : 트래픽 쉐이핑 

kebeadmin init --apiserver-adver=192.168.100.100


node로 설정되는 서버에 docker를 설치
selinux 종료
방화벽 종료

swap 비활성화
kernerl 
nat, ufs
ssh-keygen 
docker.service
파일의 마지막에 다음과 같이 입력
--exec-pot native.cgroupdriver=systemd
쿠버네티스 설치를 위한 레포지토리 구성

쿠버네티스 초기화
노드 가입

kubectl get nodes


docker는 더 이상 사용하지 않음. docker가 미란티스라는 회사에 인수가 된 이후, 개발 방향을 변경.

EE CE 

docker는 도커 하위 계층인 containerd를 분리. 현재 쿠버네티스 및 CNCF 표준 런타임. 쿠버네티스는 CRI표준 기반의 CRI-O런타임 사용. containerd, cri-docker를 사용할 수 도 있음.

학습 시, 권장하는 방법은 docker가 아닌, podman기반으로 학습. 기존 도커에서 사용하던 명령어 및 컨테이너 이미지 파일은 현재 OCI사양에서 표준 이미지 및 명령어로 채택. 현재 레드햇 리눅스 및 다른 상용 배포판 그리고 커뮤니티 배포판은 docker가 아닌 podman으로 권장.


https://people.redhat.com/tmichett/do180/podman_basics.pdf
https://developers.redhat.com/cheat-sheets/podman-cheat-sheet

podman command

podman build 	컨테이너 파일을 사용하여 컨테이너 이미지를 빌드합니다. *
podman run 	  새 컨테이너에서 명령을 실행합니다. *
podman images 로컬 스토리지의 이미지를 나열합니다. *
podman ps 	  컨테이너에 대한 정보를 출력합니다. *
podman inspect컨테이너, 이미지, 볼륨, 네트워크 또는 포드의 구성을 표시합니다.
podman pull 	레지스트리에서 이미지를 다운로드합니다. *
podman cp 	  컨테이너와 로컬 파일 시스템 간에 파일 또는 폴더를 복사합니다.
podman exec 	실행 중인 컨테이너에서 명령을 실행합니다.
podman rm 	  하나 이상의 컨테이너를 제거합니다. *
podman rmi 	  로컬에 저장된 하나 이상의 이미지를 제거합니다. *
podman search 레지스트리에서 이미지를 검색합니다. *

이미지를 다운로드
빌드를 통해 이미지를 생성
컨테이너를 서비스로 변환

podman  컨테이너 및 컨테이너 이미지를 관리
buildah 컨테이너를 생성 
skopeo  이미지를 검사. 복사. 삭제 이미지 서명

[root@node1 ~]# dnf info container-tools
Last metadata expiration check: 0:03:21 ago on Thu 15 Jun 2023 02:29:25 PM KST.
Available Packages
Name         : container-tools
Version      : 1
Release      : 12.el9
Architecture : noarch
Size         : 7.9 k
Source       : container-tools-1-12.el9.src.rpm
Repository   : appstream
Summary      : A meta-package witch container tools such as podman, buildah, skopeo, etc.
License      : MIT
Description  : Latest versions of podman, buildah, skopeo, runc, conmon, CRIU, Udica, etc as
             : well as dependencies such as container-selinux built and tested together, and
             : updated.

[root@node1 ~]# dnf -y install container-tools



[root@node1 ~]# adduser containeruser
[root@node1 ~]# echo 'redhat' | passwd --stdin containeruser


[root@node1 ~]# ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Created directory '/root/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa
Your public key has been saved in /root/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:+48RdMxOhqHIyFGsLG3FbVjuad1akaXBjwTtIq4v27U root@node1.example.com
The key's randomart image is:
+---[RSA 3072]----+
|    .+.+. o+. .  |
|   . +=oo. =+=   |
|   oooo.o ooXo   |
|  . =  ..+.*o..  |
|   o   .S.o.+    |
|       ... +     |
|       .. +      |
|      o. o +     |
|      .+o E..    |
+----[SHA256]-----+

옵션을 사용하여 한줄에서 작업하기

[root@node1 ~]# ssh-keygen -f /root/.ssh/id_rsa -N '' -t rsa -b 4096

[root@node1 ~]# ssh-copy-id containeruser@localhost
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/.ssh/id_rsa.pub"
The authenticity of host 'localhost (::1)' can't be established.
ED25519 key fingerprint is SHA256:rsL1a8yqXPflsuROVvN8g3dhiyrbhQtHYQejQ416mTs.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed
/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys
containeruser@localhost's password: 

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'containeruser@localhost'"
and check to make sure that only the key(s) you wanted were added.
[root@node1 ~]# ssh containeruser@localhost
[containeruser@node1 ~]$

containeruser 계정으로 컨테이너를 생성 / 빌드 / 서비스 구성



[containeruser@node1 ~]$ podman search centos
podman search centos | grep quay.io
quay.io/centos/centos                                        The official CentOS base containers.

[containeruser@node1 ~]$ podman pull quay.io/centos/centos
Trying to pull quay.io/centos/centos:latest...
Getting image source signatures
Copying blob 7a0437f04f83 done  
Copying config 300e315adb done  
Writing manifest to image destination
Storing signatures
300e315adb2f96afe5f0b2780b87f28ae95231fe3bdd1e16b9ba606307728f55

[containeruser@node1 ~]$ skopeo inspect docker://quay.io/centos/centos
"RepoTags": [
        "CentOS-7",
        ...
        "stream8"
[containeruser@node1 ~]$ podman images
REPOSITORY             TAG         IMAGE ID      CREATED      SIZE
quay.io/centos/centos  latest      300e315adb2f  2 years ago  217 MB
quay.io/centos/centos  centos5.11  b424fba01172  6 years ago  298 MB
[containeruser@node1 ~]$ podman rm b424fba01172
Error: "b424fba01172" does not refer to a container: no such container
[containeruser@node1 ~]$ podman rmi b424fba01172
Untagged: quay.io/centos/centos:centos5.11
Deleted: b424fba01172292ee3b8b5dad6c6618b066e9196ecfaa8cf30baf11dfe530ff9

[containeruser@node1 ~]$ podman create 300e315adb2f
e5aad5a03658378c858ce7c05534ecec74a1a8ddde99b71c01a6991eddb4796d
[containeruser@node1 ~]$ podman ps -a
CONTAINER ID  IMAGE                         COMMAND     CREATED         STATUS      PORTS       NAMES
e5aad5a03658  quay.io/centos/centos:latest  /bin/bash   10 seconds ago  Created                 pedantic_mccarthy
[containeruser@node1 ~]$ podman run 300e315adb2f
[containeruser@node1 ~]$ podman ps 
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[containeruser@node1 ~]$ podman ps  -a
CONTAINER ID  IMAGE                         COMMAND     CREATED         STATUS                     PORTS       NAMES
e5aad5a03658  quay.io/centos/centos:latest  /bin/bash   44 seconds ago  Created                                pedantic_mccarthy
4dd365020f1c  quay.io/centos/centos:latest  /bin/bash   9 seconds ago   Exited (0) 10 seconds ago              angry_khorana

[containeruser@node1 ~]$ podman ps -a
CONTAINER ID  IMAGE                         COMMAND     CREATED             STATUS                         PORTS       NAMES
e5aad5a03658  quay.io/centos/centos:latest  /bin/bash   2 minutes ago       Created                                    pedantic_mccarthy
4dd365020f1c  quay.io/centos/centos:latest  /bin/bash   About a minute ago  Exited (0) About a minute ago              angry_khorana
46e665a94be3  quay.io/centos/centos:latest  bash        59 seconds ago      Exited (0) 11 seconds ago                  great_williams
[containeruser@node1 ~]$ podman rm --all
46e665a94be32ce4bd60c92d86bac06f26050c4bbd56751c8c47a4fd1a84569d
4dd365020f1c496313c4461854dc8b1e8a6c562f9909383cbbac0e40cfa1ce6b
e5aad5a03658378c858ce7c05534ecec74a1a8ddde99b71c01a6991eddb4796d
[containeruser@node1 ~]$ podman ps -a
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES


[containeruser@node1 ~]$ podman pull quay.io/centos7/httpd-24-centos7
[containeruser@node1 ~]$ podman images
REPOSITORY                        TAG         IMAGE ID      CREATED       SIZE
quay.io/centos7/httpd-24-centos7  latest      e3164ac8fc99  28 hours ago  356 MB
quay.io/centos/centos             latest      300e315adb2f  2 years ago   217 MB

[containeruser@node1 ~]$ podman run quay.io/centos7/httpd-24-centos7
=> sourcing 10-set-mpm.sh ...
...
[Thu Jun 15 06:05:39.774738 2023] [core:notice] [pid 1] AH00094: Command line: 'httpd -D FOREGROUND'


[containeruser@node1 ~]$ podman run -d quay.io/centos7/httpd-24-centos7
770d7895c0451853cde2231b79129e4e4b5f48e54b2752fd9621a8a0c7cf267b
[containeruser@node1 ~]$ podman ps
CONTAINER ID  IMAGE                    f               COMMAND               CREATED        STATUS        PORTS       NAMES
770d7895c045  quay.io/centos7/httpd-24-centos7:latest  /usr/bin/run-http...  2 seconds ago  Up 2 seconds              pensive_carver

[containeruser@node1 ~]$ podman run -d  --name web quay.io/centos7/httpd-24-centos7
ec6f074fc0e45fe4cacbf18bd8be298ea9d64d15f177918dc1c2c709795ba5e4
[containeruser@node1 ~]$ podman top web
USER        PID         PPID        %CPU        ELAPSED       TTY         TIME        COMMANDdefault     1           0           0.000       9.40006456s   ?           0s          httpd -D FOREGROUND 
default     52          1           0.000       9.400531166s  ?           0s          /usr/bin/cat 
default     53          1           0.000       9.400583888s  ?           0s          /usr/bin/cat 
default     54          1           0.000       9.400627186s  ?           0s          /usr/bin/cat 
default     55          1           0.000       9.400726694s  ?           0s          /usr/bin/cat 
default     56          1           0.000       9.400770134s  ?           0s          httpd -D FOREGROUND 
default     60          1           0.000       9.400805991s  ?           0s          httpd -D FOREGROUND 
default     67          1           0.000       9.400842371s  ?           0s          httpd -D FOREGROUND 
default     74          1           0.000       9.400889628s  ?           0s          httpd -D FOREGROUND 
default     86          1           0.000       9.400939334s  ?           0s          httpd -D FOREGROUND

[containeruser@node1 ~]$ podman ps
CONTAINER ID  IMAGE                                    COMMAND               CREATED         STATUS         PORTS       NAMES
ec6f074fc0e4  quay.io/centos7/httpd-24-centos7:latest  /usr/bin/run-http...  27 seconds ago  Up 27 seconds              web
[containeruser@node1 ~]$ podman stop web
web
[containeruser@node1 ~]$ podman rm web
web

[containeruser@node1 ~]$ podman run -d  --name web -p 8080:8080 quay.io/centos7/httpd-24-centos7
4683b0cccea3f7454ffc70ce5401bfdc17ee74268a42e189aa407dae13924b4d
[containeruser@node1 ~]$ curl http://localhost
curl: (7) Failed to connect to localhost port 80: Connection refused
[containeruser@node1 ~]$ curl http://localhost:8080
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

[containeruser@node1 ~]$ podman stop web ; podman rm web

컨테이너내에 데이터를 쌓게되면 이 데이터는 영구저장이 되지 않음
영구적인 데이터를 위해서는 외부 디렉토리를 마운트
[containeruser@node1 ~]$  podman run -d --name web -p 8080:8080 -v /home/containeruser/web/:/var/www/html/:Z --rm  quay.io/centos7/httpd-24-centos7

[containeruser@node1 ~]$ curl http://localhost:8080
container web server index page
[containeruser@node1 ~]$ ls -ldZ web/
drwxr-xr-x. 2 containeruser containeruser system_u:object_r:container_file_t:s0:c409,c960 24 Jun 15 15:17 web/

빌드 코드
FROM quay.io/centos/centos:centos7
RUN yum install httpd -y
CMD /usr/sbin/httpd -D FOREGROUND

[containeruser@node1 ~]$ mkdir podman
[containeruser@node1 ~]$ cd podman/
[containeruser@node1 podman]$ ls
[containeruser@node1 podman]$ vi Containerfile

[containeruser@node1 podman]$ podman build -t web-container .

[containeruser@node1 podman]$ podman images
REPOSITORY                        TAG         IMAGE ID      CREATED        SIZE
localhost/web-container           latest      55ddb1b02234  9 seconds ago  454 MB

[containeruser@node1 ~]$ podman run -d --name web -p 8080:8080 -v /home/containeruser/web/:/var/www/html/:Z --rm localhost/web-container
91fdad3ec9656dfcbb2c52932bf4070d40756a451373379e1a4b8588eb05bd58
[containeruser@node1 ~]$ curl http://localhost:8080
curl: (56) Recv failure: Connection reset by peer
httpd가 기본 설치되어 있는 상태이기 때문에

[containeruser@node1 ~]$ podman ps
CONTAINER ID  IMAGE                           COMMAND               CREATED             STATUS             PORTS                   NAMES
91fdad3ec965  localhost/web-container:latest  /bin/sh -c /usr/s...  About a minute ago  Up About a minute  0.0.0.0:8080->8080/tcp  web
[containeruser@node1 ~]$ podman exec -it web bash
IncludeOptional conf.d/*.conf
[root@91fdad3ec965 conf]# cat httpd.conf | grep Listen
# Listen: Allows you to bind Apache to specific IP addresses and/or
# Change this to Listen on specific IP addresses as shown below to 
#Listen 12.34.56.78:80
Listen 80
[root@91fdad3ec965 conf]# exit
exit

80번 포트로 연결을 하면 된다.
[containeruser@node1 ~]$ podman run -d --name web -p 8080:80 -v /home/containeruser/web/:/var/www/html/:Z --rm localhost/web-container
[containeruser@node1 ~]$ curl http://localhost:8080
container web server index page
호스트 8080 : 내부 컨테이너 80 연결은 정상


파일을 편집하여 Listen 80 > 8080 으로 변경
[containeruser@node1 podman]$ vi Containerfile
FROM quay.io/centos/centos:centos7
RUN yum install httpd -y
RUN sed -i s/Listen\ 80/Listen\ 8080/ /etc/httpd/conf/httpd.conf
CMD /usr/sbin/httpd -D FOREGROUND

[containeruser@node1 podman]$ podman build -t web-container .
[containeruser@node1 podman]$ podman run -d --name web -p 8080:8080 -v /home/containeruser/web/:/var/www/html/:Z --rm localhost/web-container
a1108d6dc094441b1848920de7fcd6c70e30e2c52400043d33798fbc36c4a740
[containeruser@node1 podman]$ curl http://localhost:8080
container web server index page


[containeruser@node1 ~]$ mkdir systemd
[containeruser@node1 ~]$ podman generate systemd --new --files --name web
[containeruser@node1 ~]$ mkdir -p ~/.config/systemd/user
[containeruser@node1 systemd]$ mv container-web.service ~/.config/systemd/user

[containeruser@node1 systemd]$ id
uid=1001(containeruser) gid=1001(containeruser) groups=1001(containeruser) context=unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023
[containeruser@node1 systemd]$ systemctl daemon-reload --user
[containeruser@node1 systemd]$ systemctl enable --now container-web --user
Created symlink /home/containeruser/.config/systemd/user/default.target.wants/container-web.service → /home/containeruser/.config/systemd/user/container-web.service.

[containeruser@node1 systemd]$ systemctl enable --now container-web --user
Created symlink /home/containeruser/.config/systemd/user/default.target.wants/container-web.service → /home/containeruser/.config/systemd/user/container-web.service.
[containeruser@node1 systemd]$ systemctl status container-web --user
● container-web.service - Podman container-web.service
     Loaded: loaded (/home/containeruser/.config/systemd/user/container-web.ser>     Active: active (running) since Thu 2023-06-15 15:39:35 KST; 23s ago
       Docs: man:podman-generate-systemd(1)
   Main PID: 36013 (conmon)
      Tasks: 17 (limit: 23272)
     Memory: 15.3M
        CPU: 104ms
     CGroup: /user.slice/user-1001.slice/user@1001.service/app.slice/container->             ├─35996 /usr/bin/slirp4netns --disable-host-loopback --mtu=65520 ->             ├─35998 rootlessport
             ├─36004 rootlessport-child
             └─36013 /usr/bin/conmon --api-version 1 -c cc192ea83bb70db79ec233b>

[root@node1 ~]# reboot
[root@node1 ~]# curl http://localhost:8080
curl: (7) Failed to connect to localhost port 8080: Connection refused

[root@node1 ~]# loginctl enable-linger containeruser
[root@node1 ~]# loginctl show-
show-seat     show-session  show-user     
[root@node1 ~]# loginctl show-user containeruser
...
Linger=yes

Linger = 사용자 세션이 종료가 된 이후에도 / 로그인을 하지 않아도 유지 되도록 해당 기능 활성화 

[root@node1 ~]# reboot

Session was closed
[root@node1 ~]# curl http://localhost:8080
container web server index page


교육 과정 설문조사 링크를 표시합니다.
rol 과정으로 들어오시면 교육 과정 설문 조사

Lab Extension End Time 
Mon Sep 09 2024 18:00:00 GMT+0900
+45일 추가
























































